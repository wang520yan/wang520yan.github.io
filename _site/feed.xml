<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yan Blog</title>
    <description>代码编织梦想，程序创造未来</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 05 Aug 2020 00:43:08 +0800</pubDate>
    <lastBuildDate>Wed, 05 Aug 2020 00:43:08 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>一曲长恨一段情</title>
        <description>&lt;p&gt;&lt;img src=&quot;/img/image_20200603002.png&quot; /&gt;
一曲长恨有幽情，江州司马最伤心。&lt;/p&gt;

&lt;p&gt;1100多年前的一个夜晚，&lt;/p&gt;

&lt;p&gt;唐宣宗李忱做了个梦。&lt;/p&gt;

&lt;p&gt;他梦见来到黄帝的天宫，&lt;/p&gt;

&lt;p&gt;黄帝的身旁，站立着一个玉面童子，&lt;/p&gt;

&lt;p&gt;童子唱着《长恨歌》，&lt;/p&gt;

&lt;p&gt;衣袂飘飘，声脆如铃。&lt;/p&gt;

&lt;p&gt;宣宗大为惊奇，问黄帝：&lt;/p&gt;

&lt;p&gt;“这童儿是谁？&lt;/p&gt;

&lt;p&gt;小小年纪竟能背出《长恨歌》。”&lt;/p&gt;

&lt;p&gt;黄帝笑曰：&lt;/p&gt;

&lt;p&gt;“这小童的前世，&lt;/p&gt;

&lt;p&gt;就是长恨歌主白居易啊。&lt;/p&gt;

&lt;p&gt;他只因尘缘未了，&lt;/p&gt;

&lt;p&gt;才到大唐走了一遭。”&lt;/p&gt;

&lt;p&gt;宣宗还要再问，&lt;/p&gt;

&lt;p&gt;却见黄帝和童子腾云驾鹤而去，&lt;/p&gt;

&lt;p&gt;瞬间无影无踪。&lt;/p&gt;

&lt;p&gt;醒来后，宣宗闻内侍来报，&lt;/p&gt;

&lt;p&gt;白侍郎已于几日前在洛阳病逝。&lt;/p&gt;

&lt;p&gt;宣宗潸然泪下，挥毫落纸：&lt;/p&gt;

&lt;p&gt;“童子解吟《长恨曲》，&lt;/p&gt;

&lt;p&gt;胡儿能唱《琵琶篇》。&lt;/p&gt;

&lt;p&gt;文章已满行人耳，&lt;/p&gt;

&lt;p&gt;一度思卿一怆然。”&lt;/p&gt;

&lt;p&gt;白居易一曲《长恨歌》，&lt;/p&gt;

&lt;p&gt;足以压倒全唐的叙事诗。&lt;/p&gt;

&lt;p&gt;但世人多以为，&lt;/p&gt;

&lt;p&gt;曲里写的是李杨的爱情，&lt;/p&gt;

&lt;p&gt;却少有人知道，&lt;/p&gt;

&lt;p&gt;他是在借李杨之事，&lt;/p&gt;

&lt;p&gt;述说自己抱憾一生的相思。&lt;/p&gt;

&lt;p&gt;“在天愿作比翼鸟，在地愿为连理枝。”&lt;/p&gt;

&lt;p&gt;这句话，他最开始是说给他的湘灵听的…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/image_202006030001.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;少年成名&quot;&gt;少年成名&lt;/h3&gt;

&lt;p&gt;离离原上草，一岁一枯荣。&lt;/p&gt;

&lt;p&gt;野火烧不尽，春风吹又生。&lt;/p&gt;

&lt;p&gt;这是白居易16岁时写的，&lt;/p&gt;

&lt;p&gt;这首诗惊艳了盛唐，惊呆了顾况。&lt;/p&gt;

&lt;p&gt;顾况何许人也？&lt;/p&gt;

&lt;p&gt;唐人将其视作李杜的衣钵传人，&lt;/p&gt;

&lt;p&gt;是天下学子争相拜谒的文坛巨擘。&lt;/p&gt;

&lt;p&gt;少年白居易，也曾带着自己的诗去拜谒顾况。&lt;/p&gt;

&lt;p&gt;顾况初看到白居易的名字，调侃道：&lt;/p&gt;

&lt;p&gt;“长安米贵，白居可不容易。”&lt;/p&gt;

&lt;p&gt;但看到“离离原上草”，&lt;/p&gt;

&lt;p&gt;他就惊叹：有才如此，居天下不难！&lt;/p&gt;

&lt;p&gt;白居易的诗名，很快就传遍了京城，&lt;/p&gt;

&lt;p&gt;据说，他曾因为这首诗，&lt;/p&gt;

&lt;p&gt;在酷夏的长安，&lt;/p&gt;

&lt;p&gt;免费得到了一筐只有贵族才用得起的冰消暑。&lt;/p&gt;

&lt;p&gt;但就在天下人把这个少年捧上天的时候，&lt;/p&gt;

&lt;p&gt;他却选择离开京师这个名利场，&lt;/p&gt;

&lt;p&gt;回到故乡符离，&lt;/p&gt;

&lt;p&gt;因为家乡，有一个女孩让她魂牵梦萦。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/image_20200603005.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;初遇湘灵&quot;&gt;初遇湘灵&lt;/h3&gt;

&lt;p&gt;母亲陈氏，15岁嫁给了自己41岁的舅舅，&lt;/p&gt;

&lt;p&gt;老夫少妻本来就有隔膜，&lt;/p&gt;

&lt;p&gt;白父又常年在外做官，聚少离多，&lt;/p&gt;

&lt;p&gt;压抑的婚姻让陈氏扭曲变形，&lt;/p&gt;

&lt;p&gt;她把所有希望都寄托了早慧的白居易上，&lt;/p&gt;

&lt;p&gt;严厉监控着儿子的一举一动。&lt;/p&gt;

&lt;p&gt;书斋苦闷，&lt;/p&gt;

&lt;p&gt;又有严母在侧，&lt;/p&gt;

&lt;p&gt;白居易终日郁郁寡欢。&lt;/p&gt;

&lt;p&gt;12岁那年，为了躲避政敌报复，&lt;/p&gt;

&lt;p&gt;白家举家搬到了符离。&lt;/p&gt;

&lt;p&gt;路程遥远颠簸，&lt;/p&gt;

&lt;p&gt;白家幼子染疾去世，举家陷入悲痛。&lt;/p&gt;

&lt;p&gt;而就是在那个时候，&lt;/p&gt;

&lt;p&gt;邻家姑娘湘灵，走进了白居易的世界。&lt;/p&gt;

&lt;p&gt;两个少年，在中秋灯会上一见倾心，&lt;/p&gt;

&lt;p&gt;“娉婷十五胜天仙，白日姮娥旱地莲”。&lt;/p&gt;

&lt;p&gt;她是农家之女，&lt;/p&gt;

&lt;p&gt;身材姣好，清新如莲。&lt;/p&gt;

&lt;p&gt;因为贫贱，她侥幸逃过名教束缚，&lt;/p&gt;

&lt;p&gt;保存了一派天真烂漫。&lt;/p&gt;

&lt;p&gt;白居易善谱曲，&lt;/p&gt;

&lt;p&gt;而她的歌喉又让人着迷。&lt;/p&gt;

&lt;p&gt;一个谱曲，一个唱和，&lt;/p&gt;

&lt;p&gt;才子佳人很快就坠入爱河。&lt;/p&gt;

&lt;p&gt;少年连梦里都是湘灵，&lt;/p&gt;

&lt;p&gt;“还有少年春气味，&lt;/p&gt;

&lt;p&gt;时时暂到梦中来。”&lt;/p&gt;

&lt;p&gt;不见湘灵，就害上相思病。&lt;/p&gt;

&lt;p&gt;“抱枕无言语，空房独悄然。&lt;/p&gt;

&lt;p&gt;谁知尽日卧，非病亦非眠。”&lt;/p&gt;

&lt;p&gt;儿子突然变了个人，&lt;/p&gt;

&lt;p&gt;母亲肯定是最快察觉的。&lt;/p&gt;

&lt;p&gt;婚姻从来都是人生最重要的砝码，&lt;/p&gt;

&lt;p&gt;但从来只有大家闺秀下嫁风流才子，&lt;/p&gt;

&lt;p&gt;哪有名门之后娶村妇为妻的道理！&lt;/p&gt;

&lt;p&gt;她相当恼火：&lt;/p&gt;

&lt;p&gt;“我白家是贵胄之后，&lt;/p&gt;

&lt;p&gt;三代为官，书香门第，&lt;/p&gt;

&lt;p&gt;岂是你一个农妇可想入非非的！”&lt;/p&gt;

&lt;p&gt;为了把这段感情掐死在摇篮里，&lt;/p&gt;

&lt;p&gt;陈氏就把白居易送到南方的叔叔家去，&lt;/p&gt;

&lt;p&gt;再后来，白居易就去了京城游历。&lt;/p&gt;

&lt;p&gt;所以，此番从京城回乡，&lt;/p&gt;

&lt;p&gt;已经时隔三年了，&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/image_20200603004.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;偷尝禁果&quot;&gt;偷尝禁果&lt;/h3&gt;

&lt;p&gt;阔别几年后回到家乡，&lt;/p&gt;

&lt;p&gt;白居易直奔湘灵家去。&lt;/p&gt;

&lt;p&gt;但他却看见木门紧闭，&lt;/p&gt;

&lt;p&gt;荒草丛生，湘灵一家早不见踪影。&lt;/p&gt;

&lt;p&gt;他失魂落魄地回家，&lt;/p&gt;

&lt;p&gt;问起弟弟白行简：&lt;/p&gt;

&lt;p&gt;“哦！你走后，他们家也搬走了，&lt;/p&gt;

&lt;p&gt;不知道去哪，&lt;/p&gt;

&lt;p&gt;走之前，湘灵还抓了一把土放在锦囊里，&lt;/p&gt;

&lt;p&gt;估计要永别家乡，&lt;/p&gt;

&lt;p&gt;不会再回来了吧。”&lt;/p&gt;

&lt;p&gt;白居易怅然若失，&lt;/p&gt;

&lt;p&gt;但也只好作罢。&lt;/p&gt;

&lt;p&gt;不多久，他就结识了家乡名士“符离五子”，&lt;/p&gt;

&lt;p&gt;终日和他们游玩唱和，&lt;/p&gt;

&lt;p&gt;谈诗论文，不亦乐乎。&lt;/p&gt;

&lt;p&gt;但母亲又看不惯，&lt;/p&gt;

&lt;p&gt;又动了让他走的心思，&lt;/p&gt;

&lt;p&gt;“符离五子不过乡野之人，&lt;/p&gt;

&lt;p&gt;你每天和他们混在一起，&lt;/p&gt;

&lt;p&gt;对你的仕途有什么帮助！&lt;/p&gt;

&lt;p&gt;看来我也要学一学孟母三迁了。”&lt;/p&gt;

&lt;p&gt;这一次，白居易据理力争，&lt;/p&gt;

&lt;p&gt;并向母亲保证，&lt;/p&gt;

&lt;p&gt;以后不游玩，只切磋学问，才勉强留下。&lt;/p&gt;

&lt;p&gt;七八年后的一个春日，&lt;/p&gt;

&lt;p&gt;白居易手捧书卷，漫步湖边，&lt;/p&gt;

&lt;p&gt;他嘴里念念有词，&lt;/p&gt;

&lt;p&gt;但眼光却被远处的一辆车马吸引住了。&lt;/p&gt;

&lt;p&gt;车里的人，是湘灵！&lt;/p&gt;

&lt;p&gt;他们又回来了！&lt;/p&gt;

&lt;p&gt;“我们一家去了越中给人种地，&lt;/p&gt;

&lt;p&gt;那里赋税轻。&lt;/p&gt;

&lt;p&gt;但爹爹病了，想回家，&lt;/p&gt;

&lt;p&gt;所以我们就回来了，&lt;/p&gt;

&lt;p&gt;这次回来了就不走了。”&lt;/p&gt;

&lt;p&gt;白居易激动地抓着湘灵的手，语无伦次：&lt;/p&gt;

&lt;p&gt;“我也回来了，&lt;/p&gt;

&lt;p&gt;我也不走了。”&lt;/p&gt;

&lt;p&gt;这次见面，他们的感情急剧升温，&lt;/p&gt;

&lt;p&gt;白居易继续以与符离五子切磋为由，&lt;/p&gt;

&lt;p&gt;半夜无人时，就溜进湘灵家，&lt;/p&gt;

&lt;p&gt;他们偷尝了禁果，享尽云雨之欢。&lt;/p&gt;

&lt;p&gt;“花非花，雾非雾，夜半来，天明去。&lt;/p&gt;

&lt;p&gt;来如春梦几多时，&lt;/p&gt;

&lt;p&gt;去如朝云无觅处。”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/image_20200603005.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;有一晚，符离五子来到白家找白居易，&lt;/p&gt;

&lt;p&gt;无意间跟白母说起，&lt;/p&gt;

&lt;p&gt;许久不见居易了，想念得紧。&lt;/p&gt;

&lt;p&gt;白母眉头一皱，&lt;/p&gt;

&lt;p&gt;他不是每天晚上都找你们去了吗？&lt;/p&gt;

&lt;p&gt;怎么会许久不见？&lt;/p&gt;

&lt;p&gt;不知是不是心理作用，&lt;/p&gt;

&lt;p&gt;白母又隐约听到了湘灵的歌声，&lt;/p&gt;

&lt;p&gt;她疑惑，难道七八年过去了，&lt;/p&gt;

&lt;p&gt;儿子还没忘了那个村妇？&lt;/p&gt;

&lt;p&gt;她抓来弟弟白行简一问，&lt;/p&gt;

&lt;p&gt;一切都明白了。&lt;/p&gt;

&lt;p&gt;白居易清晨回家，&lt;/p&gt;

&lt;p&gt;堂上老母脸色铁青，&lt;/p&gt;

&lt;p&gt;“搬！去襄阳！&lt;/p&gt;

&lt;p&gt;全家都到你父亲那里去！即刻走！”&lt;/p&gt;

&lt;p&gt;白居易苦苦跪求母亲成全，&lt;/p&gt;

&lt;p&gt;但母亲不仅不为所动，&lt;/p&gt;

&lt;p&gt;甚至连给他们告别的机会都不留，&lt;/p&gt;

&lt;p&gt;“深笼夜锁独栖鸟，&lt;/p&gt;

&lt;p&gt;利剑春断连理枝。”&lt;/p&gt;

&lt;p&gt;一夜之间，白家消失得杳无踪迹。&lt;/p&gt;

&lt;p&gt;一路上，白居易强忍着悲痛，&lt;/p&gt;

&lt;p&gt;写了一首《潜别离》，&lt;/p&gt;

&lt;p&gt;“不得哭，潜别离；&lt;/p&gt;

&lt;p&gt;不得语，暗相思；”&lt;/p&gt;

&lt;p&gt;我不敢哭，不敢说，&lt;/p&gt;

&lt;p&gt;只能在心中和你暗暗告别，暗暗相思。&lt;/p&gt;

&lt;p&gt;“河水虽浊有清日，&lt;/p&gt;

&lt;p&gt;乌头虽黑有白时。”&lt;/p&gt;

&lt;p&gt;我母亲会有回心转意的时候的，&lt;/p&gt;

&lt;p&gt;湘灵你要等我。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/image_20200603006.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;天各一方&quot;&gt;天各一方&lt;/h3&gt;

&lt;p&gt;到了襄阳后半年，&lt;/p&gt;

&lt;p&gt;白父就因病辞世了。&lt;/p&gt;

&lt;p&gt;按风俗，白居易要扶老父灵柩回符离。&lt;/p&gt;

&lt;p&gt;一路颠簸坎坷，&lt;/p&gt;

&lt;p&gt;终于回到家乡，&lt;/p&gt;

&lt;p&gt;但湘灵一家又不知所终了。&lt;/p&gt;

&lt;p&gt;他走之后，流言四起，&lt;/p&gt;

&lt;p&gt;湘灵一家在符离也待不住了，只好远走。&lt;/p&gt;

&lt;p&gt;白居易失望至极。&lt;/p&gt;

&lt;p&gt;但自父亲去世后，失去靠山的白家，&lt;/p&gt;

&lt;p&gt;已经要靠着举债度日。&lt;/p&gt;

&lt;p&gt;为了白家，也为了湘灵，&lt;/p&gt;

&lt;p&gt;白居易只能忍住悲痛，发奋科考。&lt;/p&gt;

&lt;p&gt;唐朝的进士难度极高，&lt;/p&gt;

&lt;p&gt;白居易参加的那年，&lt;/p&gt;

&lt;p&gt;3000多人只取17个，&lt;/p&gt;

&lt;p&gt;但他当然位列其中，&lt;/p&gt;

&lt;p&gt;还成了最年轻的一个。&lt;/p&gt;

&lt;p&gt;“慈恩塔下题名处，&lt;/p&gt;

&lt;p&gt;十七人中最少年。”&lt;/p&gt;

&lt;p&gt;白居易意气风发，自比李杜，&lt;/p&gt;

&lt;p&gt;立志成为唐兴两百多年来第一人。&lt;/p&gt;

&lt;p&gt;他骑着高头大马，&lt;/p&gt;

&lt;p&gt;吹吹打打回到家乡，&lt;/p&gt;

&lt;p&gt;白府大红灯笼，&lt;/p&gt;

&lt;p&gt;人流如织，喜气洋洋。&lt;/p&gt;

&lt;p&gt;但邻家却孤灯一盏，&lt;/p&gt;

&lt;p&gt;隐约还传来几声啜泣。&lt;/p&gt;

&lt;p&gt;是湘灵回来了，&lt;/p&gt;

&lt;p&gt;但这次她选择避而不见。&lt;/p&gt;

&lt;p&gt;新科进士和乡野村妇，&lt;/p&gt;

&lt;p&gt;他们之间的距离，&lt;/p&gt;

&lt;p&gt;已经比年少时更远了。&lt;/p&gt;

&lt;p&gt;但白居易有心寻她，&lt;/p&gt;

&lt;p&gt;两人自然很快就又碰面了，&lt;/p&gt;

&lt;p&gt;再见面，湘灵已经二十五岁了，&lt;/p&gt;

&lt;p&gt;像她这个年纪的女孩，&lt;/p&gt;

&lt;p&gt;很多早就是几个孩子的母亲了，&lt;/p&gt;

&lt;p&gt;只有她还在傻傻地等着白居易来娶她。&lt;/p&gt;

&lt;p&gt;白居易感动心疼，&lt;/p&gt;

&lt;p&gt;他拉上湘灵，跪求母亲成全。&lt;/p&gt;

&lt;p&gt;“当年你是布衣学子，&lt;/p&gt;

&lt;p&gt;她尚且配不上。&lt;/p&gt;

&lt;p&gt;如今你是天子门生，&lt;/p&gt;

&lt;p&gt;她就更是连给你提鞋都不配了！”&lt;/p&gt;

&lt;p&gt;抗争了十几年依旧是这样的结果，&lt;/p&gt;

&lt;p&gt;湘灵终于绝望了。&lt;/p&gt;

&lt;p&gt;没过多久，白居易就要回京赴吏部的选拔了。&lt;/p&gt;

&lt;p&gt;临别的渡口，&lt;/p&gt;

&lt;p&gt;湘灵送了白居易一双她亲手做的千层底。&lt;/p&gt;

&lt;p&gt;鞋成双，人孤单。&lt;/p&gt;

&lt;p&gt;鞋尚能双行双止，&lt;/p&gt;

&lt;p&gt;人却要天各一方。&lt;/p&gt;

&lt;p&gt;白居易肝肠寸断，&lt;/p&gt;

&lt;p&gt;“南浦凄凄别，西风袅袅秋。&lt;/p&gt;

&lt;p&gt;一看肠一断，好去莫回头！”&lt;/p&gt;

&lt;p&gt;岁月让所有的我爱你，&lt;/p&gt;

&lt;p&gt;都变成了对不起。&lt;/p&gt;

&lt;p&gt;天公于他们，&lt;/p&gt;

&lt;p&gt;何曾有一点善意。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/image_20200603007.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;误我百年&quot;&gt;误我百年&lt;/h3&gt;

&lt;p&gt;到京之后，&lt;/p&gt;

&lt;p&gt;白居易遇到了人生中最重要的一个朋友，&lt;/p&gt;

&lt;p&gt;就是那个写下了&lt;/p&gt;

&lt;p&gt;“曾经沧海难为水，除却巫山不是云”的元稹。&lt;/p&gt;

&lt;p&gt;初遇元稹，他正在写《莺莺传》。&lt;/p&gt;

&lt;p&gt;官家千金崔莺莺，&lt;/p&gt;

&lt;p&gt;为处理老父后事而暂居普救寺，&lt;/p&gt;

&lt;p&gt;在这里，她遇到了张生。&lt;/p&gt;

&lt;p&gt;张生爱慕崔莺莺的才貌，&lt;/p&gt;

&lt;p&gt;以诗传情，以琴撩拨，&lt;/p&gt;

&lt;p&gt;莺莺春心大动，&lt;/p&gt;

&lt;p&gt;在婢女红娘的牵线下，&lt;/p&gt;

&lt;p&gt;和张生私定终生，&lt;/p&gt;

&lt;p&gt;张生也是“夜半来，天明去”，&lt;/p&gt;

&lt;p&gt;俨然恩爱夫妻。&lt;/p&gt;

&lt;p&gt;岂料最后他认定莺莺是影响他前程的祸水，&lt;/p&gt;

&lt;p&gt;决绝地抛弃了莺莺。&lt;/p&gt;

&lt;p&gt;一年多后，张生路过莺莺家，&lt;/p&gt;

&lt;p&gt;此时莺莺已另嫁，&lt;/p&gt;

&lt;p&gt;张生却还想以莺莺兄长的身份掩人耳目，&lt;/p&gt;

&lt;p&gt;请求一见，被莺莺断然拒绝。&lt;/p&gt;

&lt;p&gt;后来，这个故事还被改编为《西厢记》。&lt;/p&gt;

&lt;p&gt;故事写得令人愤恨，&lt;/p&gt;

&lt;p&gt;但却是以元稹和表妹的初恋为原型。&lt;/p&gt;

&lt;p&gt;元稹就是始乱终弃的张生，&lt;/p&gt;

&lt;p&gt;他在故事里，把张生的这种行为称为善于补过。&lt;/p&gt;

&lt;p&gt;因为他抛弃表妹之后，&lt;/p&gt;

&lt;p&gt;就攀上了丞相之女韦丛，&lt;/p&gt;

&lt;p&gt;从此青云直上。&lt;/p&gt;

&lt;p&gt;元稹得意地把他的作品拿给白居易看，&lt;/p&gt;

&lt;p&gt;白居易唏嘘感慨，&lt;/p&gt;

&lt;p&gt;他为莺莺怜惜，也想起了他的湘灵，深感负疚。&lt;/p&gt;

&lt;p&gt;他又借他人之事说自己的遗恨：&lt;/p&gt;

&lt;p&gt;“为君一日恩，&lt;/p&gt;

&lt;p&gt;误妾百年身。&lt;/p&gt;

&lt;p&gt;寄言痴小人家女，&lt;/p&gt;

&lt;p&gt;慎勿将身轻许人。”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/image_20200603009.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;放浪形骸&quot;&gt;放浪形骸&lt;/h3&gt;

&lt;p&gt;湘灵好像从此在白居易的人生里消失了。&lt;/p&gt;

&lt;p&gt;心如死灰的白居易开始疯狂狎妓。&lt;/p&gt;

&lt;p&gt;在唐代，以他的官制，&lt;/p&gt;

&lt;p&gt;只能蓄养三个家伎。&lt;/p&gt;

&lt;p&gt;但白居易则一人占了三十多个青楼女子，&lt;/p&gt;

&lt;p&gt;他把天下所有女人都当玩物，&lt;/p&gt;

&lt;p&gt;三年一换，老了就遣散，&lt;/p&gt;

&lt;p&gt;“十载春啼变莺舌，&lt;/p&gt;

&lt;p&gt;三嫌老丑换蛾眉”&lt;/p&gt;

&lt;p&gt;他放浪形骸，任凭天下人戳他脊梁骨，&lt;/p&gt;

&lt;p&gt;他不遗余力地糟践自己。&lt;/p&gt;

&lt;p&gt;年近四十，身边依旧变换着各种女人，&lt;/p&gt;

&lt;p&gt;他拒绝了所有说媒，坚决不娶妻。&lt;/p&gt;

&lt;p&gt;但再凶恶的母亲，&lt;/p&gt;

&lt;p&gt;也有体衰年老的一天，&lt;/p&gt;

&lt;p&gt;她找来族人相劝，生死相逼，&lt;/p&gt;

&lt;p&gt;寡母楚楚可怜，湘灵杳无踪迹，&lt;/p&gt;

&lt;p&gt;他心里早就放不下别人，&lt;/p&gt;

&lt;p&gt;娶与不娶，又有什么差别呢。&lt;/p&gt;

&lt;p&gt;37岁，白居易终于在百般无奈中迎娶了杨氏，&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/image_20200603010.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;同床异梦&quot;&gt;同床异梦&lt;/h3&gt;

&lt;p&gt;白居易讨厌杨氏，&lt;/p&gt;

&lt;p&gt;他对杨氏最有情的一句诗，&lt;/p&gt;

&lt;p&gt;是新婚时的一句说教：“君虽不读书”。&lt;/p&gt;

&lt;p&gt;他忍不住拿湘灵和杨氏对比，&lt;/p&gt;

&lt;p&gt;湘灵是农家女，&lt;/p&gt;

&lt;p&gt;但她“娉婷十五胜天仙”，&lt;/p&gt;

&lt;p&gt;杨氏是名门之后，&lt;/p&gt;

&lt;p&gt;但还“不如农妇识时节”。&lt;/p&gt;

&lt;p&gt;他因功授封，杨氏沾光，&lt;/p&gt;

&lt;p&gt;他觉得她不配：&lt;/p&gt;

&lt;p&gt;“吾转官阶常有愧，君加邑号有何功？”&lt;/p&gt;

&lt;p&gt;“倚得身名便慵堕，日高犹睡绿窗中！”&lt;/p&gt;

&lt;p&gt;白居易流传下来的诗，&lt;/p&gt;

&lt;p&gt;提及妻子的，除了说教，就是训斥。&lt;/p&gt;

&lt;p&gt;被贬江州后，失意让他更无法抵御思念。&lt;/p&gt;

&lt;p&gt;有一次，他在晾晒衣物时，&lt;/p&gt;

&lt;p&gt;翻到了那双藏在箱底的千层底。&lt;/p&gt;

&lt;p&gt;心中又再度翻江倒海，&lt;/p&gt;

&lt;p&gt;“人只履犹双，何曾得相似？”&lt;/p&gt;

&lt;p&gt;“欲忘忘未得，欲去去无由。”&lt;/p&gt;

&lt;p&gt;所有的情随事迁，都禁不起一点挑逗，&lt;/p&gt;

&lt;p&gt;这一生，终究只剩长恨了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/image_20200603011.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;自此永别&quot;&gt;自此永别&lt;/h3&gt;

&lt;p&gt;白居易53岁的时候，&lt;/p&gt;

&lt;p&gt;曾再回故居找湘灵。&lt;/p&gt;

&lt;p&gt;再重逢，湘灵依旧未嫁，&lt;/p&gt;

&lt;p&gt;虽是徐娘半老，但风韵依旧。&lt;/p&gt;

&lt;p&gt;白居易走后，湘灵舍身佛寺。&lt;/p&gt;

&lt;p&gt;未曾想到老了，&lt;/p&gt;

&lt;p&gt;他们之间还隔着凡俗这堵墙。&lt;/p&gt;

&lt;p&gt;白居易又愧又恨，想说的话，开不了口。&lt;/p&gt;

&lt;p&gt;十多年后，年过花甲的白居易再次鼓起勇气，又回来找她，&lt;/p&gt;

&lt;p&gt;这回他想，无论如何，&lt;/p&gt;

&lt;p&gt;都要和她在一起。&lt;/p&gt;

&lt;p&gt;但湘灵连见都不见，&lt;/p&gt;

&lt;p&gt;因为她不想他看到自己年老色衰的样子。&lt;/p&gt;

&lt;p&gt;她回了白居易一封信：&lt;/p&gt;

&lt;p&gt;“有赖父母的理解支持，&lt;/p&gt;

&lt;p&gt;我才能坚守终身非你不嫁的承诺。&lt;/p&gt;

&lt;p&gt;父母去世后，兄弟们也很理解我，&lt;/p&gt;

&lt;p&gt;我得以继续留在佛寺中。&lt;/p&gt;

&lt;p&gt;三十年来，我诵经念佛已经习惯了。&lt;/p&gt;

&lt;p&gt;红尘之事，也都不记得了，&lt;/p&gt;

&lt;p&gt;你，也不要再来找我了。”&lt;/p&gt;

&lt;p&gt;白居易老皱的手掐着书信，五脏俱裂。&lt;/p&gt;

&lt;p&gt;他写了一首诗给湘灵：&lt;/p&gt;

&lt;p&gt;“别来老大苦修道，炼得离心成死灰。&lt;/p&gt;

&lt;p&gt;平生忆念消磨尽，昨夜因何入梦来？”&lt;/p&gt;

&lt;p&gt;如果你已经忘了我，&lt;/p&gt;

&lt;p&gt;为什么昨晚还到我的梦里来？&lt;/p&gt;

&lt;p&gt;湘灵苦笑，她只是让弟弟替她传最后一句话，&lt;/p&gt;

&lt;p&gt;“既然在梦里见过了，&lt;/p&gt;

&lt;p&gt;那就当我们已经话别了，&lt;/p&gt;

&lt;p&gt;天上人间再见吧。”&lt;/p&gt;

&lt;p&gt;白居易心碎离开。&lt;/p&gt;

&lt;p&gt;符离中秋初相遇，&lt;/p&gt;

&lt;p&gt;一见君郎误终身。&lt;/p&gt;

&lt;p&gt;就让你永远记住我十五岁的样子吧。&lt;/p&gt;

&lt;p&gt;本以为时间可以修复所有遗恨，&lt;/p&gt;

&lt;p&gt;本以为五蕴皆空的佛法可以超度我的灵魂，&lt;/p&gt;

&lt;p&gt;却想不到，即使吃斋念佛三十年，&lt;/p&gt;

&lt;p&gt;面对你，&lt;/p&gt;

&lt;p&gt;到头来我还是连“色即是空”都参不透。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/image_20200603003.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;长恨歌主&quot;&gt;长恨歌主&lt;/h3&gt;

&lt;p&gt;世人只知道苏轼旷达，&lt;/p&gt;

&lt;p&gt;却不知道那是他从白居易处学来的闲淡消散。&lt;/p&gt;

&lt;p&gt;他曾是朝堂之上的硬骨头。&lt;/p&gt;

&lt;p&gt;锋芒所向，令权豪色变。&lt;/p&gt;

&lt;p&gt;誓言成空后，&lt;/p&gt;

&lt;p&gt;他渐渐参破穷达。&lt;/p&gt;

&lt;p&gt;晚年的诗，格调不高，境界不大，&lt;/p&gt;

&lt;p&gt;闲适淡泊中总带着一丝丝哀愁。&lt;/p&gt;

&lt;p&gt;他不再把自己比作李杜，&lt;/p&gt;

&lt;p&gt;而自称是唐代的陶渊明。&lt;/p&gt;

&lt;p&gt;“亭上独吟罢，眼前无事时。&lt;/p&gt;

&lt;p&gt;数峰太白雪，一卷陶潜诗。”&lt;/p&gt;

&lt;p&gt;他厌倦了争斗，遣散了所有家伎，&lt;/p&gt;

&lt;p&gt;最美好的生活于他不过是：&lt;/p&gt;

&lt;p&gt;“窗前有竹玩，门外有酒沽。&lt;/p&gt;

&lt;p&gt;何以待君子，数竿对一壶。”&lt;/p&gt;

&lt;p&gt;他一辈子都忘不了湘灵，&lt;/p&gt;

&lt;p&gt;他把湘灵写进长恨歌，&lt;/p&gt;

&lt;p&gt;她的美丝毫不逊于贵妃，&lt;/p&gt;

&lt;p&gt;“回眸一笑百媚生，六宫粉黛无颜色。”&lt;/p&gt;

&lt;p&gt;但在爱情里，他和玄宗一样无力，&lt;/p&gt;

&lt;p&gt;“天长地久有时尽，此恨绵绵无绝期。”&lt;/p&gt;

&lt;p&gt;他苦笑，这世上最无奈的誓言也许是，&lt;/p&gt;

&lt;p&gt;“但教心似金钿坚，天上人间会相见。”&lt;/p&gt;

&lt;p&gt;后来，《长恨歌》传到了日本，&lt;/p&gt;

&lt;p&gt;上至天皇，下至黎庶，&lt;/p&gt;

&lt;p&gt;全国无不顶礼膜拜，&lt;/p&gt;

&lt;p&gt;他被日本人奉为文殊菩萨，&lt;/p&gt;

&lt;p&gt;影响力远超李杜。&lt;/p&gt;

&lt;p&gt;再后来，人们称他为长恨歌主，&lt;/p&gt;

&lt;p&gt;一曲长恨有幽情，&lt;/p&gt;

&lt;p&gt;江州司马最伤心。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/image_20200603013.png&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Jun 2020 08:10:00 +0800</pubDate>
        <link>http://localhost:4000/2020/06/03/%E4%B8%80%E6%9B%B2%E9%95%BF%E6%81%A8%E4%B8%80%E6%AE%B5%E6%83%85/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/06/03/%E4%B8%80%E6%9B%B2%E9%95%BF%E6%81%A8%E4%B8%80%E6%AE%B5%E6%83%85/</guid>
        
        <category>爱情</category>
        
        
      </item>
    
      <item>
        <title>ML6--sklearn库的学习</title>
        <description>&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;  自2007年发布以来，scikit-learn已经成为Python重要的机器学习库了，scikit-learn简称sklearn，支持包括分类，回归，降维和聚类四大机器学习算法。还包括了特征提取，数据处理和模型评估者三大模块。&lt;/p&gt;

&lt;p&gt;  sklearn是Scipy的扩展，建立在Numpy和matplolib库的基础上。利用这几大模块的优势，可以大大的提高机器学习的效率。&lt;/p&gt;

&lt;p&gt;  sklearn拥有着完善的文档，上手容易，具有着丰富的API，在学术界颇受欢迎。sklearn已经封装了大量的机器学习算法，包括LIBSVM和LIBINEAR。同时sklearn内置了大量数据集，节省了获取和整理数据集的时间。&lt;/p&gt;

&lt;p&gt;  官网：https://scikit-learn.org/stable/&lt;/p&gt;
&lt;h1 id=&quot;官方文档解读&quot;&gt;官方文档解读&lt;/h1&gt;
&lt;h2 id=&quot;sklearn官方文档的内容&quot;&gt;sklearn官方文档的内容&lt;/h2&gt;
&lt;p&gt;  定义：针对经验E和一系列的任务T和一定表现的衡量P，如果随着经验E的积累，针对定义好的任务T可以提高表现P，就说明机器具有学习能力。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518001.png&quot; /&gt; &lt;/div&gt;
&lt;h2 id=&quot;sklearn官方文档结构&quot;&gt;sklearn官方文档结构&lt;/h2&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518002.png&quot; /&gt; &lt;/div&gt;

&lt;p&gt;  由图中，可以看到库的算法主要有四类：分类，回归，聚类，降维。其中：&lt;/p&gt;

&lt;p&gt;  常用的回归：线性、决策树、SVM、KNN ；集成回归：随机森林、Adaboost、GradientBoosting、Bagging、ExtraTrees&lt;/p&gt;

&lt;p&gt;  常用的分类：线性、决策树、SVM、KNN，朴素贝叶斯；集成分类：随机森林、Adaboost、GradientBoosting、Bagging、ExtraTrees&lt;/p&gt;

&lt;p&gt;  常用聚类：k均值（K-means）、层次聚类（Hierarchical clustering）、DBSCAN&lt;/p&gt;

&lt;p&gt;  常用降维：LinearDiscriminantAnalysis、PCA&lt;/p&gt;

&lt;p&gt;  这个流程图代表：蓝色圆圈是判断条件，绿色方框是可以选择的算法，我们可以根据自己的数据特征和任务目标去找一条自己的操作路线。&lt;/p&gt;

&lt;p&gt;  sklearn中包含众多数据预处理和特征工程相关的模块，虽然刚接触sklearn时，大家都会为其中包含的各种算法的广度深度所震惊，但其实sklearn六大板块中有两块都是关于数据预处理和特征工程的，两个板块互相交互，为建模之前的全部工程打下基础。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518003.png&quot; /&gt; &lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;模块preprocessing：几乎包含数据预处理的所有内容&lt;/li&gt;
  &lt;li&gt;模块Impute：填补缺失值专用&lt;/li&gt;
  &lt;li&gt;模块feature_selection：包含特征选择的各种方法的实践&lt;/li&gt;
  &lt;li&gt;模块decomposition：包含降维算法
    &lt;h1 id=&quot;sklearn的快速使用&quot;&gt;sklearn的快速使用&lt;/h1&gt;
    &lt;p&gt;  传统的机器学习任务从开始到建模的一般流程就是：&lt;strong&gt;获取数据——&amp;gt;数据预处理——&amp;gt;训练模型——&amp;gt;模型评估——&amp;gt;预测，分类&lt;/strong&gt;。本次我们将根据传统机器学习的流程，看看在每一步流程中都有哪些常用的函数以及他们的用法是怎么样的。那么首先先看一个简单的例子：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  鸢尾花识别是一个经典的机器学习分类问题，它的数据样本中包括了4个特征变量，1个类别变量，样本总数为150。&lt;/p&gt;

&lt;p&gt;  它的目标是为了根据花萼长度（sepal length）、花萼宽度（sepal width）、花瓣长度（petal length）、花瓣宽度（petal width）这四个特征来识别出鸢尾花属于山鸢尾（iris-setosa）、变色鸢尾（iris-versicolor）和维吉尼亚鸢尾（iris-virginica）中的哪一种。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 引入数据集，sklearn包含众多数据集
from sklearn import datasets
# 将数据分为测试集和训练集
from sklearn.model_selection import train_test_split
# 利用邻近点方式训练数据
from sklearn.neighbors import KNeighborsClassifier
 
# 引入数据,本次导入鸢尾花数据，iris数据包含4个特征变量
iris = datasets.load_iris()
# 特征变量
iris_X = iris.data
# print(iris_X)
print('特征变量的长度',len(iris_X))
# 目标值
iris_y = iris.target
print('鸢尾花的目标值',iris_y)
# 利用train_test_split进行训练集和测试机进行分开，test_size占30%
X_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=0.3)
# 我们看到训练数据的特征值分为3类
# print(y_train)
'''
[1 1 0 2 0 0 0 2 2 2 1 0 2 0 2 1 0 1 0 2 0 1 0 0 2 1 2 0 0 1 0 0 1 0 0 0 0
 2 2 2 1 1 1 2 0 2 0 1 1 1 1 2 2 1 2 2 2 0 2 2 2 0 1 0 1 0 0 1 2 2 2 1 1 1
 2 0 0 1 0 2 1 2 0 1 2 2 2 1 2 1 0 0 1 0 0 1 1 1 0 2 1 1 0 2 2]
 '''
# 训练数据
# 引入训练方法
knn = KNeighborsClassifier()
# 进行填充测试数据进行训练
knn.fit(X_train,y_train)
 
params = knn.get_params()
print(params)
'''
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski',
 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5,
 'p': 2, 'weights': 'uniform'}
 
'''
 
score = knn.score(X_test,y_test)
print(&quot;预测得分为：%s&quot;%score)
'''
预测得分为：0.9555555555555556
[1 2 1 1 2 2 1 0 0 0 0 1 2 0 1 0 2 0 0 0 2 2 0 2 2 2 2 1 2 2 2 1 2 2 1 2 0
 2 1 2 1 1 0 2 1]
[1 2 1 1 2 2 1 0 0 0 0 1 2 0 1 0 2 0 0 0 1 2 0 2 2 2 2 1 1 2 2 1 2 2 1 2 0
 2 1 2 1 1 0 2 1]
'''
 
# 预测数据，预测特征值
print(knn.predict(X_test))
'''
[0 2 2 2 2 0 0 0 0 2 2 0 2 0 2 1 2 0 2 1 0 2 1 0 1 2 2 0 2 1 0 2 1 1 2 0 2
 1 2 0 2 1 0 1 2]
'''
# 打印真实特征值
print(y_test)
'''
[1 2 2 2 2 1 1 1 1 2 1 1 1 1 2 1 1 0 2 1 1 1 0 2 0 2 0 0 2 0 2 0 2 0 2 2 0
 2 2 0 1 0 2 0 0]
 
'''
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  下面，我们开始一步步介绍&lt;/p&gt;
&lt;h3 id=&quot;1-获取数据&quot;&gt;1. 获取数据&lt;/h3&gt;
&lt;h4 id=&quot;11-导入sklearn数据集&quot;&gt;1.1 导入sklearn数据集&lt;/h4&gt;
&lt;p&gt;  sklearn中包含了大量的优质的数据集，在我们学习机器学习的过程中，我们可以使用这些数据集实现出不同的模型，从而提高我们的动手实践能力，同时这个过程也可以加深对理论知识的理解和把握。除了引入数据之外，我们还可以通过load_sample_images()来引入图片。&lt;/p&gt;

&lt;p&gt;  首先，要使用sklearn中的数据集，必须导入datasets模块。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn import datasets
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  下面两个图中包含了大部分sklearn中的数据集，调用方式也图中给出。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518004.png&quot; /&gt; &lt;/div&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518005.png&quot; /&gt; &lt;/div&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518006.png&quot; /&gt; &lt;/div&gt;

&lt;p&gt;  这里我们使用iris的数据来举个例子，表示导出数据集：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;iris = datasets.load_iris() # 导入数据集
X = iris.data # 获得其特征向量
y = iris.target # 获得样本label
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;2数据预处理&quot;&gt;2，数据预处理&lt;/h3&gt;
&lt;p&gt;  数据预处理阶段是机器学习中不可缺少的一环，它会使得数据更加有效的被模型或者评估器识别。下面我们来看一下sklearn中有哪些平时我们常用的函数：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn import preprocessing
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  为了使得训练数据的标准化规则与测试数据的标准化规则同步，preprocessing中提供了很多的Scaler：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;StandardScaler&lt;/li&gt;
  &lt;li&gt;MaxAbsScaler&lt;/li&gt;
  &lt;li&gt;MinMaxScaler&lt;/li&gt;
  &lt;li&gt;RobustScaler&lt;/li&gt;
  &lt;li&gt;Normalizer
等其他预处理操作
  对应的有直接的函数使用：scale()，maxabs_scale()，minmax_scale()，robust_scale()，normaizer（）
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;sklearn.preprocessing.scale(X)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;h4 id=&quot;21-数据标准化&quot;&gt;2.1 数据标准化&lt;/h4&gt;
    &lt;p&gt;  标准化：在机器学习中，我们可能要处理不同种类的资料，例如，音讯和图片上的像素值，这些资料可能是高纬度的，资料标准化后会使得每个特征中的数值平均变为0（将每个特征的值都减掉原始资料中该特征的平均），标准差变为1，这个方法被广泛的使用在许多机器学习算法中（例如：支持向量机，逻辑回归和类神经网络）。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  StandardScaler计算训练集的平均值和标准差，以便测试数据及使用相同的变换。&lt;/p&gt;

&lt;p&gt;  变换后各维特征有0均值，单位方差，也叫z-score规范化（零均值规范化），计算方式是将特征值减去均值，除以标准差。&lt;/p&gt;

&lt;p&gt;  &lt;strong&gt;fit&lt;/strong&gt; :用于计算训练数据的均值和方差，后面就会用均值和方差来转换训练数据&lt;/p&gt;

&lt;p&gt;  &lt;strong&gt;fit_transform&lt;/strong&gt; :不仅计算训练数据的均值和方差，还会基于计算出来的均值和方差来转换训练数据，从而把数据转化成标准的正态分布。&lt;/p&gt;

&lt;p&gt;  &lt;strong&gt;transform&lt;/strong&gt; :很显然，它只是进行转换，只是把训练数据转换成标准的正态分布。（一般会把train和test集放在一起做标准化，或者在train集上做标准化后，用同样的标准化器去标准化test集，此时可以使用scaler)。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;data = [[0, 0], [0, 0], [1, 1], [1, 1]]
# 1. 基于mean和std的标准化
scaler = preprocessing.StandardScaler().fit(train_data)
scaler.transform(train_data)
scaler.transform(test_data)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  一般来说先使用fit：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;scaler = preocessing.StandardScaler().fit(X)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  这一步可以计算得到scaler，scaler里面存的有计算出来的均值和方差。
  再使用transform&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;scaler.transform(X)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  这一步再用scaler中的均值和方差来转换X，使X标准化。&lt;/p&gt;

&lt;p&gt;  最后，在预测的时候，也要对数据做同样的标准化处理，即也要用上面的scaler中的均值和方差来对预测时候的特征进行标准化。&lt;/p&gt;

&lt;p&gt;  注意：测试数据和预测数据的标准化的方式要和训练数据标准化的方式一样，必须使用同一个scaler来进行transform&lt;/p&gt;

&lt;h4 id=&quot;22-最小-最大规范化&quot;&gt;2.2 最小-最大规范化&lt;/h4&gt;
&lt;p&gt;  最小最大规范化对原始数据进行线性变换，变换到[0,1]区间（也可以是其他固定最小最大值的区间）。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 2. 将每个特征值归一化到一个固定范围
scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(train_data)
scaler.transform(train_data)
scaler.transform(test_data)
#feature_range: 定义归一化范围，注用（）括起来
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;23-正则化normalize&quot;&gt;2.3 正则化（normalize）&lt;/h4&gt;
&lt;p&gt;  当你想要计算两个样本的相似度时必不可少的一个操作，就是正则化。其思想是：首先求出样本的p范数，然后该样本的所有元素都要除以该范数，这样最终使得每个样本的范数都是1。规范化（Normalization）是将不同变化范围的值映射到相同的固定范围，常见的是[0,1]，也成为归一化。&lt;/p&gt;

&lt;p&gt;  如下例子，将每个样本变换成unit norm。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; X = [[ 1., -1.,  2.],
...      [ 2.,  0.,  0.],
...      [ 0.,  1., -1.]]
&amp;gt;&amp;gt;&amp;gt; X_normalized = preprocessing.normalize(X, norm='l2')
 
&amp;gt;&amp;gt;&amp;gt; X_normalized                                     
array([[ 0.40..., -0.40...,  0.81...],
       [ 1.  ...,  0.  ...,  0.  ...],
       [ 0.  ...,  0.70..., -0.70...]])
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  我们可以发现对于每一个样本都有0.4^2+0.4^2+0.81^2=1。这就是L2 norm，变换后每个样本的各维特征的平方和为1.类似的，L1 norm则是变换后每个样本的各维特征的绝对值之和为1.还有max norm，则是将每个样本的各维特征除以该样本各维特征的最大值，&lt;/p&gt;

&lt;p&gt;  在度量样本之间相似性时，如果使用的是二次型kernel，则需要做Normalization。&lt;/p&gt;
&lt;h3 id=&quot;3-数据集拆分&quot;&gt;3. 数据集拆分&lt;/h3&gt;
&lt;p&gt;  在得到训练数据集时，通常我们经常会把训练数据进一步拆分成训练集和验证集，这样有助于我们模型参数的选取。&lt;/p&gt;

&lt;p&gt;  train_test_split是交叉验证中常用的函数，功能是从样本中随机的按比例选取train data和testdata，形式为：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;X_train,X_test, y_train, y_test =
 
cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  注意：train_test_split 不再 cross_validation中，已经移到 model_selection 中。&lt;/p&gt;

&lt;p&gt;  参数解释&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;train_data：所要划分的样本特征集&lt;/li&gt;
  &lt;li&gt;train_target：所要划分的样本结果&lt;/li&gt;
  &lt;li&gt;test_size：样本占比，如果是整数的话就是样本的数量&lt;/li&gt;
  &lt;li&gt;random_state：是随机数的种子。&lt;/li&gt;
  &lt;li&gt;随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则：&lt;br /&gt;
   1. 种子不同，产生不同的随机数&lt;/p&gt;

&lt;p&gt;   2. 种子相同，即使实例不同也产生相同的随机数&lt;/p&gt;

&lt;p&gt;参数说明&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518007.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;示例&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 作用：将数据集划分为 训练集和测试集
# 格式：train_test_split(*arrays, **options)
from sklearn.mode_selection import train_test_split
 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
&quot;&quot;&quot;
参数
---
arrays：样本数组，包含特征向量和标签
 
test_size：
　　float-获得多大比重的测试样本 （默认：0.25）
　　int - 获得多少个测试样本
 
train_size: 同test_size
 
random_state:
　　int - 随机种子（种子固定，实验可复现）
　　
shuffle - 是否在分割之前对数据进行洗牌（默认True）
 
返回
---
分割后的列表，长度=2*len(arrays),
　　(train-test split)
&quot;&quot;&quot;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  拆分参数遇到的问题及其解决方法
  导入模块&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.cross_validation import cross_val_score
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  则会报错，代码如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;    from sklearn.cross_validation import cross_val_score
ModuleNotFoundError: No module named 'sklearn.cross_validation'
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  解决方法：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.model_selection import cross_val_score
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;4-定义模型&quot;&gt;4. 定义模型&lt;/h3&gt;
&lt;p&gt;  在这一步我们首先要分析自己数据的类型，明白自己要用什么模型来做，然后我们就可以在sklearn中定义模型了，sklearn为所有模型提供了非常相似的接口，这样使得我们可以更加快速的熟悉所有模型的用法，在这之前，我们先来看看模型的常用属性和功能。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 拟合模型
model.fit(X_train, y_train)
# 模型预测
model.predict(X_test)
 
# 获得这个模型的参数
model.get_params()
# 为模型进行打分
model.score(data_X, data_y) # 线性回归：R square； 分类问题： acc
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;41-线性回归&quot;&gt;4.1 线性回归&lt;/h4&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518008.png&quot; /&gt; &lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;定义线性回归模型&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;copy_X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;err&quot;&gt;参数&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fit_intercept&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：是否计算截距。&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;模型没有截距&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;当&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_intercept&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;设置为&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;时，该参数将被忽略。&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;如果为真，&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;则回归前的回归系数&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;将通过减去平均值并除以&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;范数而归一化。&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：指定线程数&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;42-逻辑回归lr&quot;&gt;4.2 逻辑回归LR&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;定义逻辑回归模型&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;penalty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dual&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.0001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fit_intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intercept_scaling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;liblinear&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multi_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ovr&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;warm_start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;err&quot;&gt;参数&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;penalty&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：使用指定正则化项（默认：&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;）&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dual&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;取&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;（默认）&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：正则化强度的反，值越小正则化强度越大&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;指定线程数&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：随机数生成器&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fit_intercept&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;是否需要常量&lt;/span&gt;  

&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;43-朴素贝叶斯算法nbnaive-bayes&quot;&gt;4.3 朴素贝叶斯算法NB（Naive Bayes）&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;naive_bayes&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;naive_bayes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GaussianNB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;高斯贝叶斯&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;naive_bayes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MultinomialNB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit_prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;naive_bayes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BernoulliNB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binarize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit_prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;err&quot;&gt;文本分类问题常用&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MultinomialNB&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;参数&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：平滑参数&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fit_prior&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：是否要学习类的先验概率；&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;使用统一的先验概率&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;class_prior&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;是否指定类的先验概率；若指定则不能根据参数调整&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;binarize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;二值化的阈值，若为&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;，则假设输入由二进制向量组成&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;44-决策树dt&quot;&gt;4.4 决策树DT&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gini&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;min_samples_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_samples_leaf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_weight_fraction_leaf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_leaf_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;min_impurity_decrease&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_impurity_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
     &lt;span class=&quot;n&quot;&gt;class_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;presort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;参数&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;：特征选择准则&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gini&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：树的最大深度，&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;尽量下分&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;min_samples_split&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：分裂内部节点，所需要的最小样本树&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;min_samples_leaf&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：叶子节点所需要的最小样本数&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;寻找最优分割点时的最大特征数&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_leaf_nodes&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：优先增长到最大叶子节点数&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;min_impurity_decrease&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：如果这种分离导致杂质的减少大于或等于这个值，则节点将被拆分。&lt;/span&gt;   
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;45-支持向量机svm&quot;&gt;4.5 支持向量机SVM&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svm&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SVC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rbf&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;参数&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：误差项的惩罚参数&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;核相关系数。浮点数，&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;If&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_features&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;will&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;be&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;used&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;instead&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;46-k近邻算法knn&quot;&gt;4.6 k近邻算法KNN&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neighbors&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;定义&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kNN&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;分类模型&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neighbors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;KNeighborsClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_neighbors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;分类&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neighbors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;KNeighborsRegressor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_neighbors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;回归&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;参数&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_neighbors&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;使用邻居的数目&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_jobs&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;：并行任务数&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;47-多层感知器神经网络&quot;&gt;4.7 多层感知器（神经网络）&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;neural_network&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLPClassifier&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;定义多层感知机分类算法&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLPClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;solver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'adam'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.0001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&quot;参数
---
    hidden_layer_sizes: 元祖
    activation：激活函数
    solver ：优化算法{‘lbfgs’, ‘sgd’, ‘adam’}
    alpha：L2惩罚(正则化项)参数。
&quot;&quot;&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;5-模型评估与选择&quot;&gt;5. 模型评估与选择&lt;/h3&gt;
&lt;p&gt;  评价指标针对不同的机器学习任务有不同的指标，同一任务也有不同侧重点的评价指标。以下方法，sklearn中都在sklearn.metrics类下，务必记住那些指标适合分类，那些适合回归。&lt;/p&gt;

&lt;p&gt;  机器学习常用的评估指标请参考博文：Python机器学习笔记：常用评估指标的前世今生&lt;/p&gt;

&lt;h4 id=&quot;51-交叉验证&quot;&gt;5.1 交叉验证&lt;/h4&gt;
&lt;p&gt;交叉验证cross_val_score的scoring参数&lt;br /&gt;
  分类：accuracy(准确率)、f1、f1_micro、f1_macro（这两个用于多分类的f1_score）、precision(精确度)、recall(召回率)、roc_auc&lt;br /&gt;
  回归：neg_mean_squared_error（MSE、均方误差）、r2&lt;br /&gt;
  聚类：adjusted_rand_score、completeness_score等&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.model_selection import cross_val_score
cross_val_score(model, X, y=None, scoring=None, cv=None, n_jobs=1)
&quot;&quot;&quot;参数
---
    model：拟合数据的模型
    cv ： k-fold
    scoring: 打分参数-‘accuracy’、‘f1’、‘precision’、‘recall’ 、‘roc_auc’、'neg_log_loss'等等
&quot;&quot;&quot;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;交叉验证的学习&lt;br /&gt;
  1，导入k折交叉验证模块&lt;br /&gt;
  注意cross_val_score 是根据模型进行计算，计算交叉验证的结果，可以简单的认为cross_val_score中调用了KFold 进行数据集划分。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.model_selection import cross_val_score
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  2，交叉验证的思想&lt;br /&gt;
  把某种意义下将原始数据（dataset）进行分组，一部分作为训练集（train set），另一部分作为验证集（validation set or test set），首先用训练集对分类器进行训练，再利用验证集来测试训练得到的模型（model），以此来作为评价分类器的性能指标。&lt;/p&gt;

&lt;p&gt;  3，为什么使用交叉验证法&lt;br /&gt;
  交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程序熵减少过拟合。
  交叉验证还可以从有限的数据中获取尽可能多的有效信息
  4，model_selection.KFold 和 model_selection.cross_val_score的区别
  我们直接看官网：KFold：https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold&lt;/p&gt;

&lt;p&gt;  cross_val_score：https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518009.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;  KFold 就是对数据集划分为 训练集/测试集，然后将训练数据集划分为K折，每个折进行一次验证，而剩下的K-1 折进行训练，依次循环，直到用完所有的折。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518010.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;  而 cross_val_score 就是通过交叉验证评估得分。&lt;/p&gt;

&lt;p&gt;  下面看看K折交叉验证函数KFold函数：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;KFold（n_split, shuffle, random_state）
 
　　参数：n_split:要划分的折数
 
　　　　　shuffle: 每次都进行shuffle，测试集中折数的总和就是训练集的个数
 
　　　　　random_state:随机状态
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  其使用如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.model_selection import KFold
from sklearn.datasets import load_iris
from sklearn.linear_model import LinearRegression
import numpy as np
import pandas as pd
 
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4])
kf = KFold(n_splits=2)
# get_n_splits: Returns the number of splitting iterations in the cross-validator
print(kf.get_n_splits(X))  # 2
 
KF = KFold(n_splits=5)
X, Y = load_iris().data, load_iris().target
alg = LinearRegression()
# 这里想强行使用DataFrame的数据格式，因为以后大家读取数据使用都是csv格式
# 所以必不可免要用 iloc
X, Y = pd.DataFrame(X), pd.DataFrame(Y)
# split()：Generate indices to split data into training and test set.
for train_index, test_index in KF.split(X):
    print(&quot;TRAIN:&quot;, train_index, &quot;TEST:&quot;, test_index)
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]
    alg.fit(X_train, y_train)
    # 后面就自由发挥即可
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;5主要有哪些方法&quot;&gt;5，主要有哪些方法&lt;/h3&gt;
&lt;h4 id=&quot;1留出法holdout-cross-validation&quot;&gt;1，留出法（holdout cross validation）&lt;/h4&gt;

&lt;p&gt;  在机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：训练集，验证集和测试集。&lt;/p&gt;

&lt;p&gt;  训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518011.png&quot; /&gt; &lt;/div&gt;

&lt;p&gt;  这个方法操作简单，只需要随机将原始数据分为三组即可。&lt;/p&gt;

&lt;p&gt;  不过如果只做一次分割，它对训练集，验证集和测试机的样本比例，还有分割后数据的分布是否和原始数据集的分布相同等因素比较敏感，不同的划分会得到不同的最优模型，，而且分成三个集合后，用于训练的数据更少了。于是又了2.k折交叉验证（k-fold cross validation）.&lt;/p&gt;

&lt;p&gt;  下面例子，一共有150条数据：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; from sklearn.model_selection import train_test_split
&amp;gt;&amp;gt;&amp;gt; from sklearn import datasets
&amp;gt;&amp;gt;&amp;gt; from sklearn import svm
 
&amp;gt;&amp;gt;&amp;gt; iris = datasets.load_iris()
&amp;gt;&amp;gt;&amp;gt; iris.data.shape, iris.target.shape
((150, 4), (150,))
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  用train_test_split来随机划分数据集，其中40%用于测试集，有60条数据，60%为训练集，有90条数据：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; X_train, X_test, y_train, y_test = train_test_split(
...     iris.data, iris.target, test_size=0.4, random_state=0)
 
&amp;gt;&amp;gt;&amp;gt; X_train.shape, y_train.shape
((90, 4), (90,))
&amp;gt;&amp;gt;&amp;gt; X_test.shape, y_test.shape
((60, 4), (60,))
　　用train来训练，用test来评价模型的分数。

&amp;gt;&amp;gt;&amp;gt; clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
&amp;gt;&amp;gt;&amp;gt; clf.score(X_test, y_test)                          
0.96...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;2-k-折交叉验证k-fold-cross-validation&quot;&gt;2. k 折交叉验证（k-fold cross validation）&lt;/h4&gt;

&lt;p&gt;  K折交叉验证通过对k个不同分组训练的结果进行平均来减少方差，因此模型的性能对数据的划分就不那么敏感。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第一步，不重复抽样将原始数据随机分为 k 份。&lt;/li&gt;
  &lt;li&gt;第二步，每一次挑选其中 1 份作为测试集，剩余 k-1 份作为训练集用于模型训练。&lt;/li&gt;
  &lt;li&gt;第三步，重复第二步 k 次，这样每个子集都有一次机会作为测试集，其余机会作为训练集。&lt;/li&gt;
  &lt;li&gt;在每个训练集上训练后得到一个模型，&lt;/li&gt;
  &lt;li&gt;用这个模型在相应的测试集上测试，计算并保存模型的评估指标，
第四步，计算 k 组测试结果的平均值作为模型精度的估计，并作为当前 k 折交叉验证下模型的性能指标。
  K一般取10，数据量小的是，k可以设大一点，这样训练集占整体比例就比较大，不过同时训练的模型个数也增多。数据量大的时候，k可以设置小一点。当k=m的时候，即样本总数，出现了留一法。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  举例，这里直接调用了cross_val_score，这里用了5折交叉验证&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.model_selection import cross_val_score
&amp;gt;&amp;gt;&amp;gt; clf = svm.SVC(kernel='linear', C=1)
&amp;gt;&amp;gt;&amp;gt; scores = cross_val_score(clf, iris.data, iris.target, cv=5)
&amp;gt;&amp;gt;&amp;gt; scores                                             
array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])
　　得到最后平均分数为0.98，以及它的95%置信区间：

&amp;gt;&amp;gt;&amp;gt; print(&quot;Accuracy: %0.2f (+/- %0.2f)&quot; % (scores.mean(), scores.std() * 2))
Accuracy: 0.98 (+/- 0.03)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  我们可以直接看一下K-Fold是怎么样划分数据的：X有四个数据，把它分成2折，结构中最后一个集合是测试集，前面的是训练集，每一行为1折：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; import numpy as np
&amp;gt;&amp;gt;&amp;gt; from sklearn.model_selection import KFold
 
&amp;gt;&amp;gt;&amp;gt; X = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;]
&amp;gt;&amp;gt;&amp;gt; kf = KFold(n_splits=2)
&amp;gt;&amp;gt;&amp;gt; for train, test in kf.split(X):
...     print(&quot;%s %s&quot; % (train, test))
[2 3] [0 1]
[0 1] [2 3]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  同样的数据X，我们来看LeaveOneOut后是什么样子，那就是把它分成4折，结果中最后一个集合是测试集，只有一个元素，前面的是训练集，每一行为1折：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.model_selection import LeaveOneOut
 
&amp;gt;&amp;gt;&amp;gt; X = [1, 2, 3, 4]
&amp;gt;&amp;gt;&amp;gt; loo = LeaveOneOut()
&amp;gt;&amp;gt;&amp;gt; for train, test in loo.split(X):
...     print(&quot;%s %s&quot; % (train, test))
[1 2 3] [0]
[0 2 3] [1]
[0 1 3] [2]
[0 1 2] [3]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3留一法leave-one-out-cross-validation&quot;&gt;3，留一法（Leave one out cross validation）&lt;/h4&gt;

&lt;p&gt;  每次的测试集都只有一个样本，要进行m次训练和预测，这个方法用于训练的数据只比整体数据集少一个样本，因此最接近原始样本的分布。但是训练复杂度增加了，因为模型的数量与原始数据样本数量相同。一般在数据缺少时使用。&lt;br /&gt;
  此外： &lt;br /&gt;
  多次 k 折交叉验证再求均值，例如：10 次 10 折交叉验证，以求更精确一点。&lt;br /&gt;
  划分时有多种方法，例如对非平衡数据可以用分层采样，就是在每一份子集中都保持和原始数据集相同的类别比例。&lt;br /&gt;
  模型训练过程的所有步骤，包括模型选择，特征选择等都是在单个折叠 fold 中独立执行的。&lt;/p&gt;
&lt;h4 id=&quot;4bootstrapping&quot;&gt;4，Bootstrapping&lt;/h4&gt;

&lt;p&gt;  通过自助采样法，即在含有 m 个样本的数据集中，每次随机挑选一个样本，再放回到数据集中，再随机挑选一个样本，这样有放回地进行抽样 m 次，组成了新的数据集作为训练集。&lt;br /&gt;
  这里会有重复多次的样本，也会有一次都没有出现的样本，原数据集中大概有 36.8% 的样本不会出现在新组数据集中。&lt;br /&gt;
  优点是训练集的样本总数和原数据集一样都是 m，并且仍有约 1/3 的数据不被训练而可以作为测试集。 &lt;br /&gt;
  缺点是这样产生的训练集的数据分布和原数据集的不一样了，会引入估计偏差。&lt;/p&gt;

&lt;h4 id=&quot;52-检验曲线&quot;&gt;5.2 检验曲线&lt;/h4&gt;
&lt;p&gt;  使用检验曲线，我们可以更加方便的改变模型参数，获取模型表现。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.model_selection import validation_curve
train_score, test_score = validation_curve(model, X, y, param_name, param_range, cv=None, scoring=None, n_jobs=1)
&quot;&quot;&quot;参数
---
    model:用于fit和predict的对象
    X, y: 训练集的特征和标签
    param_name：将被改变的参数的名字
    param_range： 参数的改变范围
    cv：k-fold
    
返回值
---
   train_score: 训练集得分（array）
    test_score: 验证集得分（array）
&quot;&quot;&quot;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;　　&lt;/p&gt;
&lt;h4 id=&quot;53-分类模型&quot;&gt;5.3 分类模型&lt;/h4&gt;
&lt;p&gt;  accuracy_score（准确率得分）是模型分类正确的数据除以样本总数 【模型的score方法算的也是准确率】&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;accuracy_score(y_test,y_pre)
# 或者 model.score(x_test,y_test)，大多模型都是有score方法的
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;  classification_report中的各项得分的avg/total 是每一分类占总数的比例加权算出来的&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;print(classification_report(y_test,y_log_pre))
 
             precision    recall  f1-score   support
 
          0       0.87      0.94      0.90       105
          1       0.91      0.79      0.85        73
 
avg / total       0.88      0.88      0.88       178
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;  confusion_matrix（混淆矩阵），用来评估分类的准确性&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import confusion_matrix
&amp;gt;&amp;gt;&amp;gt; y_true = [2, 0, 2, 2, 0, 1]
&amp;gt;&amp;gt;&amp;gt; y_pred = [0, 0, 2, 2, 0, 2]
&amp;gt;&amp;gt;&amp;gt; confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])

&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  precision_score(精确度)、recall_score(召回率)、f1_score（后者由前两个推导出的）
  这三个不仅适合二分类，也适合多分类。只需要指出参数average=‘micro’/‘macro’/’weighted’&lt;/p&gt;

&lt;p&gt;  macro：计算二分类metrics的均值，为每个类给出相同权重的分值。
当小类很重要时会出问题，因为该macro-averging方法是对性能的平均。
另一方面，该方法假设所有分类都是一样重要的，因此macro-averaging
方法会对小类的性能影响很大&lt;/p&gt;

&lt;p&gt;  micro： 给出了每个样本类以及它对整个metrics的贡献的pair（sample-
weight），而非对整个类的metrics求和，它会每个类的metrics上的权重及
因子进行求和，来计算整个份额。Micro-averaging方法在多标签（multilabel）
问题中设置，包含多分类，此时，大类将被忽略&lt;/p&gt;

&lt;p&gt;  weighted: 对于不均衡数量的类来说，计算二分类metrics的平均，
通过在每个类的score上进行加权实现
roc_curve（ROC曲线，用于二分类）&lt;/p&gt;

&lt;h3 id=&quot;6-保存模型&quot;&gt;6 保存模型&lt;/h3&gt;
&lt;p&gt;  最后，我们可以将我们训练好的model保存到本地，或者放到线上供用户使用，那么如何保存训练好的model呢？主要有下面两种方式：&lt;/p&gt;

&lt;h4 id=&quot;61-保存为pickle文件&quot;&gt;6.1 保存为pickle文件&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;
 
&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;保存模型&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'model.pickle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'wb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;读取模型&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'model.pickle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'rb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;62-sklearn自带方法joblib&quot;&gt;6.2 sklearn自带方法joblib&lt;/h4&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;externals&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;joblib&lt;/span&gt;
 
&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;保存模型&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;joblib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'model.pickle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;载入模型&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;joblib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'model.pickle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;7模型评分&quot;&gt;7，模型评分&lt;/h3&gt;
&lt;p&gt;  1，模型的score方法：最简单的模型评估方法就是调用模型自己的方法：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 预测
y_predict = knnClf.predict(x_test)
print(&quot;score on the testdata:&quot;,knnClf.score(x_test,y_test))
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  2，sklearn的指标函数：库提供的一些计算方法，常用的有classification_report方法&lt;/p&gt;

&lt;p&gt;  3，sklearn也支持自己开发评价方法。&lt;/p&gt;

&lt;h3 id=&quot;8几种交叉验证cross-validation方式的比较&quot;&gt;8，几种交叉验证（cross validation）方式的比较&lt;/h3&gt;
&lt;p&gt;  模型评价的目的：通过模型评价，我们知道当前训练模型的好坏，泛化能力如何？从而知道是否可以应用在解决问题上，如果不行，那又是那些出了问题？&lt;br /&gt;
&lt;strong&gt;train_test_split&lt;/strong&gt;&lt;br /&gt;
  在分类问题中，我们通常通过对训练集进行triain_test_split，划分出train 和test两部分，其中train用来训练模型，test用来评估模型，模型通过fit方法从train数据集中学习，然后调用score方法在test集上进行评估，打分；从分数上我们知道模型当前的训练水平如何。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import  matplotlib.pyplot as plt
 
cancer = load_breast_cancer()
X_train,X_test,y_train,y_test = train_test_split(cancer.data,cancer.target,random_state=0)
 
logreg = LogisticRegression().fit(X_train,y_train)
print(&quot;Test set score:{:.2f}&quot;.format(logreg.score(X_test,y_test)))   
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  结果：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Test set score:0.96   
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  然而这这方式只进行了一次划分，数据结果具有偶然性，如果在某次划分中，训练集里全是容易学习的数据，测试集里全是复杂的数据，这样的就会导致最终的结果不尽人意。&lt;br /&gt;
&lt;strong&gt;Standard Cross Validation&lt;/strong&gt;&lt;br /&gt;
  针对上面通过train_test_split划分，从而进行模型评估方式存在的弊端，提出Cross Validation交叉验证。&lt;br /&gt;
  Cross Validation：进行多次train_test_split划分；每次划分时，在不同的数据集上进行训练，测试评估，从而得到一个评价结果；如果是5折交叉验证，意思就是在原始数据集上，进行五次划分，每次划分进行一次训练，评估，最后得到5次划分后的评估结果，一般在这几次评估结果上取平均得到最后的评分，k-folf cross-validation ，其中K一般取5或10。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518012.png&quot; /&gt; &lt;/div&gt;

&lt;p&gt;  代码：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import  warnings
 
warnings.filterwarnings('ignore')
 
cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
    cancer.data , cancer.target, random_state=0
)
 
logreg = LogisticRegression()
# CV 默认是3折交叉验证，可以修改cv=5，变为5折交叉验证
scores = cross_val_score(logreg,cancer.data , cancer.target)
 
print(&quot;Cross validation scores:{}&quot;.format(scores))
print(&quot;Mean cross validation score:{:2f}&quot;.format(scores.mean()))   
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  结果：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Cross validation scores:[0.93684211 0.96842105 0.94179894]
Mean cross validation score:0.949021   
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;交叉验证的优点：&lt;br /&gt;
  原始采用的train_test_split方法，数据划分具有偶然性；交叉验证通过多次划分，大大降低了这种由一次随机划分带来的偶然性，同时通过多次划分，多次训练，模型也能遇到各种各样的数据，从而提高其泛化能力
与原始的train_test_split相比，对数据的使用效率更高，train_test_split，默认训练集，测试集比例为3:1，而对交叉验证来说，如果是5折交叉验证，训练集比测试集为4:1；10折交叉验证训练集比测试集为9:1.数据量越大，模型准确率越高！&lt;/li&gt;
  &lt;li&gt;交叉验证的缺点：&lt;br /&gt;
  这种简答的交叉验证方式，从上面的图片可以看出来，每次划分时对数据进行均分，设想一下，会不会存在一种情况：数据集有5类，抽取出来的也正好是按照类别划分的5类，也就是说第一折全是0类，第二折全是1类，等等；这样的结果就会导致，模型训练时。没有学习到测试集中数据的特点，从而导致模型得分很低，甚至为0，为避免这种情况，又出现了其他的各种交叉验证方式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Stratifid k-fold cross validation&lt;/strong&gt;&lt;br /&gt;
  分层交叉验证（Stratified k-fold cross validation）：首先它属于交叉验证类型，分层的意思是说在每一折中都保持着原始数据中各个类别的比例关系，比如说：原始数据有3类，比例为1:2:1，采用3折分层交叉验证，那么划分的3折中，每一折中的数据类别保持着1:2:1的比例，这样的验证结果更加可信。
  通常情况下，可以设置cv参数来控制几折，但是我们希望对其划分等加以控制，所以出现了KFold，KFold控制划分折，可以控制划分折的数目，是否打乱顺序等，可以赋值给cv，用来控制划分。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518013.png&quot; /&gt; &lt;/div&gt;

&lt;p&gt;  代码：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.datasets import load_iris
from sklearn.model_selection import StratifiedKFold ,cross_val_score
from sklearn.linear_model import LogisticRegression
import warnings
 
warnings.filterwarnings('ignore')
 
iris_data = load_iris()
logreg = LogisticRegression()
strKFold = StratifiedKFold(n_splits=3,shuffle=False,random_state=0)
scores = cross_val_score(logreg,iris_data.data,iris_data.target,cv=strKFold)
print(&quot;straitified cross validation scores:{}&quot;.format(scores))
print(&quot;Mean score of straitified cross validation:{:.2f}&quot;.format(scores.mean()))   
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  结果：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;straitified cross validation scores:[0.96078431 0.92156863 0.95833333]
Mean score of straitified cross validation:0.95  
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Leave-one-out Cross-validation 留一法&lt;/strong&gt;&lt;br /&gt;
  留一法Leave-one-out Cross-validation：是一种特殊的交叉验证方式。顾名思义，如果样本容量为n，则k=n，进行n折交叉验证，每次留下一个样本进行验证。主要针对小样本数据。&lt;br /&gt;
  代码：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.datasets import load_iris
from sklearn.model_selection import LeaveOneOut , cross_val_score
from sklearn.linear_model import LogisticRegression
import  warnings
 
warnings.filterwarnings('ignore')
 
iris = load_iris()
logreg = LogisticRegression()
loout = LeaveOneOut()
scores = cross_val_score(logreg,iris.data,iris.target,cv=loout)
print(&quot;leave-one-out cross validation scores:{}&quot;.format(scores))
print(&quot;Mean score of leave-one-out cross validation:{:.2f}&quot;.format(scores.mean()))   
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  结果：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;leave-one-out cross validation scores:[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1.]
Mean score of leave-one-out cross validation:0.95   
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Shuffle-split cross-validation&lt;/strong&gt;&lt;br /&gt;
  控制更加灵活，可以控制划分迭代次数，每次划分测试集和训练集的比例（也就说：可以存在机不再训练集也不再测试集的情况）&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518014.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;  代码：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.datasets import load_iris
from sklearn.model_selection import ShuffleSplit,cross_val_score
from sklearn.linear_model import LogisticRegression
import warnings
 
warnings.filterwarnings('ignore')
 
iris = load_iris()
# 迭代八次
shufsp1 = ShuffleSplit(train_size=0.5,test_size=0.4,n_splits=8)
logreg = LogisticRegression()
scores = cross_val_score(logreg,iris.data,iris.target,cv=shufsp1)
 
print(&quot;shuffle split cross validation scores:\n{}&quot;.format(scores))
print(&quot;Mean score of shuffle split cross validation:{:.2f}&quot;.format(scores.mean()))   
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  结果：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;shuffle split cross validation scores:
[0.95       1.         0.86666667 0.95       0.88333333 0.88333333
 0.85       0.9       ]
Mean score of shuffle split cross validation:0.91   
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;9sklearn中一些函数的用法&quot;&gt;9，sklearn中一些函数的用法&lt;/h3&gt;
&lt;h4 id=&quot;91--from-sklearnutils-import-shuffle-解析&quot;&gt;9.1  from sklearn.utils import shuffle 解析&lt;/h4&gt;
&lt;p&gt;  在进行机器学习时，经常需要打乱样本，这种时候Python中第三方库提供了这个功能——sklearn.utils.shuffle。&lt;/p&gt;

&lt;p&gt;1，Parameters&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518015.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;2，Returns&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0518016.png&quot; /&gt; &lt;/div&gt;

&lt;p&gt;参考文献：
http://www.cnblogs.com/lianyingteng/p/7811126.html&lt;/p&gt;

&lt;p&gt;https://www.cnblogs.com/magle/p/5638409.html&lt;/p&gt;

&lt;p&gt;https://blog.csdn.net/u014248127/article/details/78885180&lt;/p&gt;

&lt;p&gt;https://www.cnblogs.com/ysugyl/p/8707887.html&lt;/p&gt;

&lt;p&gt;https://www.cnblogs.com/wj-1314/p/10179741.html&lt;/p&gt;

</description>
        <pubDate>Mon, 18 May 2020 08:10:00 +0800</pubDate>
        <link>http://localhost:4000/2020/05/18/sklearn%E5%BA%93%E7%9A%84%E5%AD%A6%E4%B9%A0/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/18/sklearn%E5%BA%93%E7%9A%84%E5%AD%A6%E4%B9%A0/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>ML5--模型预测结果的评估</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;‘没有测量，就没有科学’这是科学家门捷列夫的名言。在计算机科学特别是机器学习领域中，对模型的评估同样至关重要，只有选择与问题相匹配的评估方法，才能快速地发现模型选择或训练过程中出现的问题，迭代地对模型进行优化。模型评估主要分为离线评估和在线评估两个阶段。针对分类、排序、回归、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。知道每种评估指标的精确定义、有针对性地选择合适的评估指标、更具评估指标的反馈进行模型调整，这些都是模型评估阶段的关键问题&lt;/p&gt;
  &lt;h1 id=&quot;评估指标&quot;&gt;评估指标&lt;/h1&gt;
  &lt;h2 id=&quot;1-accuracy_score&quot;&gt;1. accuracy_score&lt;/h2&gt;
  &lt;p&gt;  分类准确率分数是指所有分类正确的百分比。分类准确率这一衡量分类器的标准比较容易理解，但是它不能告诉你响应值的潜在分布，并且它也不能告诉你分类器犯错的类型。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;  形式:
&lt;strong&gt;sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;  normalize：默认值为True，返回正确分类的比例；如果为False，返回正确分类的样本数&lt;/p&gt;

&lt;p&gt;  示例:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt;import numpy as np  
&amp;gt;&amp;gt;&amp;gt;from sklearn.metrics import accuracy_score  
&amp;gt;&amp;gt;&amp;gt;y_pred = [0, 2, 1, 3]  
&amp;gt;&amp;gt;&amp;gt;y_true = [0, 1, 2, 3]  
&amp;gt;&amp;gt;&amp;gt;accuracy_score(y_true, y_pred)  
0.5  
&amp;gt;&amp;gt;&amp;gt;accuracy_score(y_true, y_pred, normalize=False)  
2  
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-recall_score&quot;&gt;2. recall_score&lt;/h2&gt;
&lt;p&gt;  召回率 =提取出的正确信息条数 /样本中的信息条数。通俗地说，就是所有准确的条目有多少被检索出来了。&lt;/p&gt;

&lt;p&gt;  形式：
&lt;strong&gt;klearn.metrics.recall_score(y_true, y_pred, labels=None, pos_label=1,average=’binary’, sample_weight=None)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;  参数average : string, [None, ‘micro’, ‘macro’(default), ‘samples’, ‘weighted’]&lt;/p&gt;

&lt;p&gt;  将一个二分类matrics拓展到多分类或多标签问题时，我们可以将数据看成多个二分类问题的集合，每个类都是一个二分类。接着，我们可以通过跨多个分类计算每个二分类metrics得分的均值，这在一些情况下很有用。你可以使用average参数来指定。&lt;/p&gt;

&lt;p&gt;  macro：计算二分类metrics的均值，为每个类给出相同权重的分值。当小类很重要时会出问题，因为该macro-averging方法是对性能的平均。另一方面，该方法假设所有分类都是一样重要的，因此macro-averaging方法会对小类的性能影响很大。&lt;/p&gt;

&lt;p&gt;  weighted:对于不均衡数量的类来说，计算二分类metrics的平均，通过在每个类的score上进行加权实现。&lt;/p&gt;

&lt;p&gt;  micro：给出了每个样本类以及它对整个metrics的贡献的pair（sample-weight），而非对整个类的metrics求和，它会每个类的metrics上的权重及因子进行求和，来计算整个份额。Micro-averaging方法在多标签（multilabel）问题中设置，包含多分类，此时，大类将被忽略。&lt;/p&gt;

&lt;p&gt;  samples：应用在multilabel问题上。它不会计算每个类，相反，它会在评估数据中，通过计算真实类和预测类的差异的metrics，来求平均（sample_weight-weighted）&lt;/p&gt;

&lt;p&gt;  average：average=None将返回一个数组，它包含了每个类的得分.
示例:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt;from sklearn.metrics import recall_score  
&amp;gt;&amp;gt;&amp;gt;y_true = [0, 1, 2, 0, 1, 2]  
&amp;gt;&amp;gt;&amp;gt;y_pred = [0, 2, 1, 0, 0, 1]  
&amp;gt;&amp;gt;&amp;gt;recall_score(y_true, y_pred, average='macro')   
0.33...  
&amp;gt;&amp;gt;&amp;gt;recall_score(y_true, y_pred, average='micro')   
0.33...  
&amp;gt;&amp;gt;&amp;gt;recall_score(y_true, y_pred, average='weighted')   
0.33...  
&amp;gt;&amp;gt;&amp;gt;recall_score(y_true, y_pred, average=None)  
array([1.,  0., 0.])  
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;3-f1_score&quot;&gt;3. f1_score&lt;/h2&gt;
&lt;p&gt;  F1分数是精度和召回率的谐波平均值，正常的平均值平等对待所有的值，而谐波平均值回给予较低的值更高的权重，因此，只有当召回率和精度都很高时，分类器才能得到较高的F1分数。
  其中，P代表Precision，R代表Recall。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0514001.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;  F1-Score指标综合了Precision与Recall的产出的结果。F1-Score的取值范围从0到1的，1代表模型的输出最好，0代表模型的输出结果最差。
  示例:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt;from sklearn.metrics import recall_score  
&amp;gt;&amp;gt;&amp;gt;y_true = [0, 1, 2, 0, 1, 2]  
&amp;gt;&amp;gt;&amp;gt;y_pred = [0, 2, 1, 0, 0, 1]  
&amp;gt;&amp;gt;&amp;gt;f1_score(y_true, y_pred, average='macro')   
0.57...
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;4-roc曲线&quot;&gt;4. roc曲线&lt;/h2&gt;
&lt;p&gt;  二值分类器是机器学习领域中最常见也是应用最广泛的分类器。评价二值分类器的指标很多，比如precision,recall,F1 score,P-R曲线等，但发现这些指标或多或少只能反映模型在某一方面的性能，相比而言，ROC曲线则有很多优点，经常作为评估二值分类器最重要的指标之一&lt;/p&gt;

&lt;p&gt;  ROC曲线是Receiver Operating Characteristic Curve的简称，中文名为’受试者工作特征曲线’&lt;/p&gt;

&lt;p&gt;  ROC曲线的横坐标为假阳性率(FPR),纵坐标为真阳性率(TPR)，FPR和TPR的计算方法分别为:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;𝐹𝑃𝑅=𝐹𝑃𝑁&lt;/strong&gt;
&lt;strong&gt;𝑇𝑃𝑅=𝑇𝑃𝑃&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;  P是真实的正样本数量，N是真实的负样本数量，TP是P个正样本中被分类器预测为正样本的个数，FP为N个负样本中被预测为正样本的个数。
  示例：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt;import numpy as np  
&amp;gt;&amp;gt;&amp;gt;from sklearn import metrics  
&amp;gt;&amp;gt;&amp;gt;y = np.array([1, 1, 2, 2])  
&amp;gt;&amp;gt;&amp;gt;scores = np.array([0.1, 0.4, 0.35, 0.8])  
&amp;gt;&amp;gt;&amp;gt;fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)  
&amp;gt;&amp;gt;&amp;gt;fpr  
array([0. ,  0.5,  0.5, 1. ])  
&amp;gt;&amp;gt;&amp;gt;tpr  
array([0.5,  0.5,  1. , 1. ])  
&amp;gt;&amp;gt;&amp;gt;thresholds  
array([0.8 ,  0.4 ,  0.35, 0.1 ])  
&amp;gt;&amp;gt;&amp;gt;from sklearn.metrics import auc   
&amp;gt;&amp;gt;&amp;gt;metrics.auc(fpr, tpr)   
0.75 
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;roc曲线绘制&quot;&gt;ROC曲线绘制&lt;/h4&gt;
&lt;h5 id=&quot;创建数据集&quot;&gt;创建数据集&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import pandas as pd

column_name = ['真实标签','模型输出概率']
datasets = [['p',0.9],['p',0.8],['n',0.7],['p',0.6],
           ['p',0.55],['p',0.54],['n',0.53],['n',0.52],
           ['p',0.51],['n',0.505],['p',0.4],['p',0.39],
           ['p',0.38],['n',0.37],['n',0.36],['n',0.35],
           ['p',0.34],['n',0.33],['p',0.30],['n',0.1]]

data = pd.DataFrame(datasets,index = [i for i in range(1,21,1)],columns=column_name)
print(data)
   真实标签  模型输出概率
1     p   0.900
2     p   0.800
3     n   0.700
4     p   0.600
5     p   0.550
6     p   0.540
7     n   0.530
8     n   0.520
9     p   0.510
10    n   0.505
11    p   0.400
12    p   0.390
13    p   0.380
14    n   0.370
15    n   0.360
16    n   0.350
17    p   0.340
18    n   0.330
19    p   0.300
20    n   0.100
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h5 id=&quot;绘制roc曲线&quot;&gt;绘制ROC曲线&lt;/h5&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 计算各种概率情况下对应的(假阳率，真阳率)
points = {0.1:[1,1],0.3:[0.9,1],0.33:[0.9,0.9],0.34:[0.8,0.9],0.35:[0.8,0.8],
        0.36:[0.7,0.8],0.37:[0.6,0.8],0.38:[0.5,0.8],0.39:[0.5,0.7],0.40:[0.4,0.7],
        0.505:[0.4,0.6],0.51:[0.3,0.6],0.52:[0.3,0.5],0.53:[0.2,0.5],0.54:[0.1,0.5],
        0.55:[0.1,0.4],0.6:[0.1,0.3],0.7:[0.1,0.2],0.8:[0,0.2],0.9:[0,0.1]}
X = []
Y = []
for value in points.values():
        X.append(value[0])
        Y.append(value[1])
        
import matplotlib.pyplot as plt

plt.scatter(X,Y,c = 'r',marker = 'o')
plt.plot(X,Y)

plt.xlim(0,1)
plt.ylim(0,1)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.show()
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0514002.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;  AUC指ROC曲线下的面积大小，该值能够量化地反映基于ROC曲线衡量出的模型性能,AUC越大说明分类器越可能把真正的正样本排在前面，分类性能越好&lt;/p&gt;

&lt;p&gt;  ROC曲线相比P-R曲线，当正负样本的分布发生变化时，ROC曲线的形状能够保存基本不变，而P-R曲线的形状一般会发生激烈的变化，这个特点让ROC曲线能够尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能&lt;/p&gt;
&lt;h2 id=&quot;5-auc&quot;&gt;5. Auc&lt;/h2&gt;
&lt;p&gt;  计算AUC值，其中x,y分别为数组形式，根据(xi,yi)在坐标上的点，生成的曲线，然后计算AUC值；&lt;/p&gt;

&lt;p&gt;  形式：
&lt;strong&gt;sklearn.metrics.auc(x, y, reorder=False)&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-roc_auc_score&quot;&gt;6. roc_auc_score&lt;/h2&gt;
&lt;p&gt;  直接根据真实值（必须是二值）、预测值（可以是0/1,也可以是proba值）计算出auc值，中间过程的roc计算省略。&lt;/p&gt;

&lt;p&gt;  形式：
&lt;strong&gt;sklearn.metrics.roc_auc_score(y_true, y_score, average=’macro’, sample_weight=None)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;  average : string, [None, ‘micro’, ‘macro’(default), ‘samples’, ‘weighted’]&lt;/p&gt;

&lt;p&gt;  示例：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt;import numpy as np  
&amp;gt;&amp;gt;&amp;gt;from sklearn.metrics import roc_auc_score  
&amp;gt;&amp;gt;&amp;gt;y_true = np.array([0, 0, 1, 1])  
&amp;gt;&amp;gt;&amp;gt;y_scores = np.array([0.1, 0.4, 0.35, 0.8])  
&amp;gt;&amp;gt;&amp;gt;roc_auc_score(y_true, y_scores)  
0.75  
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;7-confusion_matrix&quot;&gt;7. confusion_matrix&lt;/h2&gt;
&lt;p&gt;  用一个例子来理解混淆矩阵：
  假设有一个用来对猫（cats）、狗（dogs）、兔子（rabbits）进行分类的系统，混淆矩阵就是为了进一步分析性能而对该算法测试结果做出的总结。假设总共有 27 只动物：8只猫， 6条狗， 13只兔子。结果的混淆矩阵如下图：&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0514003.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;  在这个混淆矩阵中，实际有 8只猫，但是系统将其中3只预测成了狗；对于 6条狗，其中有 1条被预测成了兔子，2条被预测成了猫。从混淆矩阵中我们可以看出系统对于区分猫和狗存在一些问题，但是区分兔子和其他动物的效果还是不错的。所有正确的预测结果都在对角线上，所以从混淆矩阵中可以很方便直观的看出哪里有错误，因为他们呈现在对角线外面。&lt;/p&gt;

&lt;h1 id=&quot;评估方法&quot;&gt;评估方法&lt;/h1&gt;
&lt;h2 id=&quot;1-使用numpy计算&quot;&gt;1. 使用numpy计算&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import numpy as np
 
y_true = np.array([0, 1, 1, 0, 1, 0])
y_pred = np.array([1, 1, 1, 0, 0, 1])
 
# true positive
TP = np.sum(np.multiply(y_true, y_pred))
print(TP)
 
# false positive
FP = np.sum(np.logical_and(np.equal(y_true, 0), np.equal(y_pred, 1)))
print(FP)
 
# false negative
FN = np.sum(np.logical_and(np.equal(y_true, 1), np.equal(y_pred, 0)))
print(FN)
 
# true negative
TN = np.sum(np.logical_and(np.equal(y_true, 0), np.equal(y_pred, 0)))
print(TN)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;输出结果：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;2
2
1
1
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;2-使用tensorflow计算&quot;&gt;2. 使用tensorflow计算&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import tensorflow as tf
 
sess = tf.Session()
 
y_true = tf.constant([0, 1, 1, 0, 1, 0])
y_pred = tf.constant([1, 1, 1, 0, 0, 1])
 
# true positive
TP = tf.reduce_sum(tf.multiply(y_true, y_pred))
print(sess.run(TP))
 
# false positive
FP = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred, 1)), tf.int32))
print(sess.run(FP))
 
# false negative
FN = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 0)), tf.int32))
print(sess.run(FN))
 
# true negative
TN = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred, 0)), tf.int32))
print(sess.run(TN))
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;输出结果：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;2
2
1
1
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;3-使用sklearn的metrics模块计算&quot;&gt;3. 使用sklearn的metrics模块计算&lt;/h2&gt;
&lt;h3 id=&quot;31-数据是list类型&quot;&gt;3.1 数据是list类型&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.metrics import precision_score, recall_score, f1_score
 
y_true = [0, 1, 1, 0, 1, 0]
y_pred = [1, 1, 1, 0, 0, 1]
 
p = precision_score(y_true, y_pred, average='binary')
r = recall_score(y_true, y_pred, average='binary')
f1score = f1_score(y_true, y_pred, average='binary')
 
print(p)
print(r)
print(f1score)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;输出结果：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;0.5
0.666666666667
0.571428571429
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;32-数据是ndarray类型&quot;&gt;3.2 数据是ndarray类型&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np
 
y_true = np.array([[0, 1, 1], 
                   [0, 1, 0]])
y_pred = np.array([[1, 1, 1], 
                   [0, 0, 1]])
 
y_true = np.reshape(y_true, [-1])
y_pred = np.reshape(y_pred, [-1])
 
p = precision_score(y_true, y_pred, average='binary')
r = recall_score(y_true, y_pred, average='binary')
f1score = f1_score(y_true, y_pred, average='binary')
 
print(p)
print(r)
print(f1score)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;输出结果：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;0.5
0.666666666667
0.571428571429
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Thu, 14 May 2020 08:10:00 +0800</pubDate>
        <link>http://localhost:4000/2020/05/14/%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E7%9A%84%E8%AF%84%E4%BC%B0/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/14/%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E7%9A%84%E8%AF%84%E4%BC%B0/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>ML4--matplotlib绘图学习1</title>
        <description>&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;  matplotlib是受MATLAB的启发构建的。MATLAB是数据绘图领域广泛使用的语言和工具。MATLAB语言是面向过程的。利用函数的调用，MATLAB中可以轻松的利用一行命令来绘制直线，然后再用一系列的函数调整结果。&lt;/p&gt;

&lt;p&gt;  matplotlib有一套完全仿照MATLAB的函数形式的绘图接口，在matplotlib.pyplot模块中。这套函数接口方便MATLAB用户过度到matplotlib包&lt;/p&gt;

&lt;p&gt;官网：http://matplotlib.org/&lt;/p&gt;

&lt;p&gt;学习方式：从官网examples入门学习&lt;/p&gt;

&lt;p&gt;http://matplotlib.org/examples/index.html
http://matplotlib.org/gallery.html&lt;/p&gt;

&lt;h1 id=&quot;原理&quot;&gt;原理&lt;/h1&gt;
&lt;p&gt;  使用matplotlib绘图的原理，主要就是理解figure(画布)、axes(坐标系)、axis(坐标轴)三者之间的关系。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513002.png&quot; /&gt; &lt;/div&gt;

&lt;p&gt;  一个figure(画布)上，可以有多个区域axes(坐标系)，我们在每个坐标系上绘图，也就是说每个axes(坐标系)中，都有一个axis(坐标轴)。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513003.png&quot; /&gt; &lt;/div&gt;

&lt;p&gt;  在matplotlib中，figure画布和axes坐标轴并不能显示的看见，我们能够看到的就是一个axis坐标轴的各种图形。&lt;/p&gt;

&lt;p&gt;  在使用matplotlib画图时需要设置的参数。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513004.png&quot; /&gt; &lt;/div&gt;
&lt;h1 id=&quot;绘制&quot;&gt;绘制&lt;/h1&gt;
&lt;p&gt;  在绘图结构中，figure创建窗口，subplot创建子图。所有的绘画只能在子图上进行。plt表示当前子图，若没有就创建一个子图。所有你会看到一些教程中使用plt进行设置，一些教程使用子图属性进行设置。他们往往存在对应功能函数。&lt;/p&gt;

&lt;p&gt;  Figure：面板(图)，matplotlib中的所有图像都是位于figure对象中，一个图像只能有一个figure对象。&lt;/p&gt;

&lt;p&gt;  Subplot：子图，figure对象下创建一个或多个subplot对象(即axes)用于绘制图像。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513005.png&quot; /&gt; &lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;plt.figure() 画布&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513006.png&quot; /&gt; &lt;/div&gt;

&lt;p&gt;plt.subplot() 划分子图&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513007.png&quot; /&gt; &lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;plt.plot()
plt.rcdefaults()
plt.rcParams[&quot;font.sans-serif&quot;] = &quot;KaiTi&quot;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513008.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;plt.title() 子图标题&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513009.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;plt.suptitle() 总标题&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513010.png&quot; /&gt; &lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;plt.tight_layout() 自动调整、填充、消除子图之间的重叠
plt.show()
plt.scatter() 有label图例文字参数，通过plt.legend()指定图例位置等
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513011.png&quot; /&gt; &lt;/div&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513012.png&quot; /&gt; &lt;/div&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513013.png&quot; /&gt; &lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;plt.text() 设置图内文本
plt.xlabel() 设置坐标轴标签
plt.ylabel()
plt.xlim() 设置坐标取值范围
plt.ylim()
plt.imshow()
plt.axis(&quot;off&quot;)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513014.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;折线图和柱形图 plt.plot(), plt.bar()&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513015.png&quot; /&gt; &lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;图像处理
from PIL import Image
import matplotlib.pyplot as plt
f = Image.open()
f.format, f.size, f.mode
f.convert()
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0513016.png&quot; /&gt; &lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;f.split() 颜色通道分离
Image.merge() 合并
f.resize([720,1280])
f.thumbnail([720,1280]) 直接对原图相进行缩放
f.transpose(Image.FLIP_TOP_BOTTOM)
f.crop([0,0,360,640]) 裁剪
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Wed, 13 May 2020 08:10:00 +0800</pubDate>
        <link>http://localhost:4000/2020/05/13/matplotlib%E7%BB%98%E5%9B%BE%E5%AD%A6%E4%B9%A01/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/13/matplotlib%E7%BB%98%E5%9B%BE%E5%AD%A6%E4%B9%A01/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>ML3--主成分分析（PCA）</title>
        <description>&lt;h1 id=&quot;pca介绍&quot;&gt;PCA介绍&lt;/h1&gt;
&lt;p&gt;PCA是无监督数据降维方式，目的是将一个高维数据集转换为一个低维数据集。如今我们的数据集往往有成百上千维的特征，但并不是所有的特征都很重要，通过降维，去除那些不重要的特征。数据维度的降低了，同时计算机的运算效率也得到了提升。在人工智能技术刚起步的时候，人们关注的焦点在于算法的准确性，通过不断优化算法中的计算参数，来提高运算结果的准确率。今天，随着存储与通信技术的发展，数据规模变得空前的庞大，所以运算效率变成了我们不得不考虑的问题。
整个过程我们采用的是Python的Numpy库（线性代数中的矩阵计算）来进行的，整个过程如下：
1.数据的标准化处理 - 去均值
2.计算协方差矩阵
3.计算特征向量与特征值
4.根据特征值的大小，选择前k个特征向量组成一个新的特征矩阵
5.原始数据与新的特征矩阵相乘&lt;/p&gt;

&lt;h1 id=&quot;scikit-learn-pca类介绍&quot;&gt;scikit-learn PCA类介绍&lt;/h1&gt;
&lt;p&gt;  在scikit-learn中，与PCA相关的类都在sklearn.decomposition包中。最常用的PCA类就是sklearn.decomposition.PCA，我们下面主要也会讲解基于这个类的使用的方法。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;sklearn.decomposition.PCA(n_components=None, 
copy=True, whiten=False, svd_solver=’auto’,
 tol=0.0, iterated_power=’auto’, random_state=None)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;  除了PCA类以外，最常用的PCA相关类还有KernelPCA类，在原理篇我们也讲到了，它主要用于非线性数据的降维，需要用到核技巧。因此在使用的时候需要选择合适的核函数并对核函数的参数进行调参。&lt;/p&gt;

&lt;p&gt;  另外一个常用的PCA相关类是IncrementalPCA类，它主要是为了解决单机内存限制的。有时候我们的样本量可能是上百万+，维度可能也是上千，直接去拟合数据可能会让内存爆掉， 此时我们可以用IncrementalPCA类来解决这个问题。IncrementalPCA先将数据分成多个batch，然后对每个batch依次递增调用partial_fit函数，这样一步步的得到最终的样本最优降维。&lt;/p&gt;

&lt;p&gt;  此外还有SparsePCA和MiniBatchSparsePCA。他们和上面讲到的PCA类的区别主要是使用了L1的正则化，这样可以将很多非主要成分的影响度降为0，这样在PCA降维的时候我们仅仅需要对那些相对比较主要的成分进行PCA降维，避免了一些噪声之类的因素对我们PCA降维的影响。SparsePCA和MiniBatchSparsePCA之间的区别则是MiniBatchSparsePCA通过使用一部分样本特征和给定的迭代次数来进行PCA降维，以解决在大样本时特征分解过慢的问题，当然，代价就是PCA降维的精确度可能会降低。使用SparsePCA和MiniBatchSparsePCA需要对L1正则化参数进行调参。&lt;/p&gt;

&lt;h1 id=&quot;sklearndecompositionpca参数介绍&quot;&gt;sklearn.decomposition.PCA参数介绍&lt;/h1&gt;
&lt;p&gt;  下面我们主要基于sklearn.decomposition.PCA来讲解如何使用scikit-learn进行PCA降维。PCA类基本不需要调参，一般来说，我们只需要指定我们需要降维到的维度，或者我们希望降维后的主成分的方差和占原始维度所有特征方差和的比例阈值就可以了。&lt;/p&gt;

&lt;p&gt;  现在我们对sklearn.decomposition.PCA的主要参数做一个介绍：&lt;/p&gt;

&lt;p&gt;  1）n_components：这个参数可以帮我们指定希望PCA降维后的特征维度数目。最常用的做法是直接指定降维到的维度数目，此时n_components是一个大于等于1的整数。当然，我们也可以指定主成分的方差和所占的最小比例阈值，让PCA类自己去根据样本特征方差来决定降维到的维度数，此时n_components是一个（0，1]之间的数。当然，我们还可以将参数设置为”mle”, 此时PCA类会用MLE算法根据特征的方差分布情况自己去选择一定数量的主成分特征来降维。我们也可以用默认值，即不输入n_components，此时n_components=min(样本数，特征数)。&lt;/p&gt;

&lt;p&gt;  2）whiten ：判断是否进行白化。所谓白化，就是对降维后的数据的每个特征进行归一化，让方差都为1.对于PCA降维本身来说，一般不需要白化。如果你PCA降维后有后续的数据处理动作，可以考虑白化。默认值是False，即不进行白化。&lt;/p&gt;

&lt;p&gt;  3）svd_solver：即指定奇异值分解SVD的方法，由于特征分解是奇异值分解SVD的一个特例，一般的PCA库都是基于SVD实现的。有4个可以选择的值：{‘auto’, ‘full’, ‘arpack’, ‘randomized’}。randomized一般适用于数据量大，数据维度多同时主成分数目比例又较低的PCA降维，它使用了一些加快SVD的随机算法。 full则是传统意义上的SVD，使用了scipy库对应的实现。arpack和randomized的适用场景类似，区别是randomized使用的是scikit-learn自己的SVD实现，而arpack直接使用了scipy库的sparse SVD实现。默认是auto，即PCA类会自己去在前面讲到的三种算法里面去权衡，选择一个合适的SVD算法来降维。一般来说，使用默认值就够了。&lt;/p&gt;

&lt;p&gt;  除了这些输入参数外，有两个PCA类的成员值得关注。第一个是explained_variance_，它代表降维后的各主成分的方差值。方差值越大，则说明越是重要的主成分。第二个是explained_variance_ratio_，它代表降维后的各主成分的方差值占总方差值的比例，这个比例越大，则越是重要的主成分。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;components_：特征变换空间（特征矩阵），根据我们指定的n_components = k的值，选择方差最大的k个值所对应的的特征向量组成的特征矩阵。&lt;/li&gt;
  &lt;li&gt;explained_variance_：n_components所对应的的方差。&lt;/li&gt;
  &lt;li&gt;explained_variance_ratio_：不同特征方差的占比。&lt;/li&gt;
  &lt;li&gt;singular_values_ :特征值，与前面的特征向量conponents_是一一对应的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;pca实例1&quot;&gt;PCA实例1&lt;/h1&gt;
&lt;p&gt;  下面我们用一个实例来学习下scikit-learn中的PCA类使用。为了方便的可视化让大家有一个直观的认识，我们这里使用了三维的数据来降维。&lt;/p&gt;

&lt;p&gt;完整代码参见我的github: https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/pca.ipynb&lt;/p&gt;

&lt;p&gt;  首先我们生成随机数据并可视化，代码如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
%matplotlib inline
from sklearn.datasets.samples_generator import make_blobs
# X为样本特征，Y为样本簇类别， 共1000个样本，每个样本3个特征，共4个簇
X, y = make_blobs(n_samples=10000, n_features=3, centers=[[3,3, 3], [0,0,0], [1,1,1], [2,2,2]], cluster_std=[0.2, 0.1, 0.2, 0.2], random_state =9)
fig = plt.figure()
ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=30, azim=20)
plt.scatter(X[:, 0], X[:, 1], X[:, 2],marker='o')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;  三维数据的分布图如下：&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0512002.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  我们先不降维，只对数据进行投影，看看投影后的三个维度的方差分布，代码如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.decomposition import PCA
pca = PCA(n_components=3)
pca.fit(X)
print pca.explained_variance_ratio_
print pca.explained_variance_
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;输出如下：&lt;/p&gt;

&lt;p&gt;[ 0.98318212  0.00850037  0.00831751]
[ 3.78483785  0.03272285  0.03201892]&lt;/p&gt;

&lt;p&gt;  可以看出投影后三个特征维度的方差比例大约为98.3%：0.8%：0.8%。投影后第一个特征占了绝大多数的主成分比例。&lt;/p&gt;

&lt;p&gt;  现在我们来进行降维，从三维降到2维，代码如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;pca = PCA(n_components=2)
pca.fit(X)
print pca.explained_variance_ratio_
print pca.explained_variance_
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;　
输出如下：&lt;/p&gt;

&lt;p&gt;[ 0.98318212  0.00850037]
[ 3.78483785  0.03272285]&lt;/p&gt;

&lt;p&gt;  这个结果其实可以预料，因为上面三个投影后的特征维度的方差分别为：[ 3.78483785  0.03272285  0.03201892]，投影到二维后选择的肯定是前两个特征，而抛弃第三个特征。&lt;/p&gt;

&lt;p&gt;  为了有个直观的认识，我们看看此时转化后的数据分布，代码如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;X_new = pca.transform(X)
plt.scatter(X_new[:, 0], X_new[:, 1],marker='o')
plt.show()
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;输出的图如下：&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0512003.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
  可见降维后的数据依然可以很清楚的看到我们之前三维图中的4个簇。&lt;/p&gt;

&lt;p&gt;  现在我们看看不直接指定降维的维度，而指定降维后的主成分方差和比例。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;pca = PCA(n_components=0.95)
pca.fit(X)
print pca.explained_variance_ratio_
print pca.explained_variance_
print pca.n_components_
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  我们指定了主成分至少占95%，输出如下：&lt;/p&gt;

&lt;p&gt;[ 0.98318212]
[ 3.78483785]
1
  可见只有第一个投影特征被保留。这也很好理解，我们的第一个主成分占投影特征的方差比例高达98%。只选择这一个特征维度便可以满足95%的阈值。我们现在选择阈值99%看看，代码如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;pca = PCA(n_components=0.99)
pca.fit(X)
print pca.explained_variance_ratio_
print pca.explained_variance_
print pca.n_components_
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;此时的输出如下：&lt;/p&gt;

&lt;p&gt;[ 0.98318212  0.00850037]
[ 3.78483785  0.03272285]
2
  这个结果也很好理解，因为我们第一个主成分占了98.3%的方差比例，第二个主成分占了0.8%的方差比例，两者一起可以满足我们的阈值。&lt;/p&gt;

&lt;p&gt;  最后我们看看让MLE算法自己选择降维维度的效果，代码如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;pca = PCA(n_components='mle')
pca.fit(X)
print pca.explained_variance_ratio_
print pca.explained_variance_
print pca.n_components_
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;输出结果如下：&lt;/p&gt;

&lt;p&gt;[ 0.98318212]
[ 3.78483785]
1&lt;/p&gt;

&lt;p&gt;  可见由于我们的数据的第一个投影特征的方差占比高达98.3%，MLE算法只保留了我们的第一个特征。&lt;/p&gt;

&lt;h1 id=&quot;pca实例2&quot;&gt;PCA实例2&lt;/h1&gt;
&lt;p&gt;  我们使用sklearn自带的数据集boston（波士顿地区房价数据集），该数据集有506个样本，13个特征，像房屋面积，区位，卧室数量等等以及价格（标签值）。pca是一种无监督降维算法，所以我们不使用价格数据。下面是pca的代码实现过程：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn import datasets

boston_house_price = datasets.load_boston()#导入boston房价数据集
X = boston_house_price .data#获取特征数据
#第一步，对数据进行标准化处理
X_std = StandardScaler().fit_transform(X)
#实例化PCA
pca = PCA(n_components = 3)
#训练数据
pca.fit(X_std)

#使用PCA的属性查看特征值
pca.singular_values_
array([55.6793095 , 26.93022859, 25.07516773])
#使用PCA的属性查看特征值对应的特征向量
pca.components_
array([[ 0.2509514 , -0.25631454,  0.34667207,  0.00504243,  0.34285231,
        -0.18924257,  0.3136706 , -0.32154387,  0.31979277,  0.33846915,
         0.20494226, -0.20297261,  0.30975984],
       [-0.31525237, -0.3233129 ,  0.11249291,  0.45482914,  0.21911553,
         0.14933154,  0.31197778, -0.34907   , -0.27152094, -0.23945365,
        -0.30589695,  0.23855944, -0.07432203],
       [ 0.24656649,  0.29585782, -0.01594592,  0.28978082,  0.12096411,
         0.59396117, -0.01767481, -0.04973627,  0.28725483,  0.22074447,
        -0.32344627, -0.3001459 , -0.26700025]])
#对原始的数据集进行转换
new_data = X.dot(pca.components_.T)
print(new_data[:10])#打印出转换后的前十行数据做一个观察
array([[ 38.89018107,  32.93532391, -51.87396066],
       [ 33.02343232,  54.79866941, -71.20799688],
       [ 26.53873512,  48.76840918, -67.85363879],
       [ 12.75698667,  47.78351826, -72.33882223],
       [ 15.65240562,  50.77871883, -73.70920814],
       [ 17.71686561,  51.4336294 , -73.35783472],
       [ 51.22331968,  29.63835929, -51.11003359],
       [ 62.1527616 ,  38.52240664, -53.72636333],
       [ 68.87661773,  36.34017288, -53.90249412],
       [ 60.21849172,  32.80458593, -50.06565433]])
#此时的数据，我们就不知道它具体代表的什么含义了
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h1 id=&quot;图形化pca降维前后的数据对比&quot;&gt;图形化PCA降维前后的数据对比&lt;/h1&gt;
&lt;p&gt;  我们用sklearn中iris花数据集来举例，该数据集有四个特征，花萼长度、花萼宽度、花瓣长度、花瓣宽度，下面是画图过程的代码实现。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0512004.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets
from sklearn.decomposition import PCA

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features.
y = iris.target

x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5

plt.figure(2, figsize=(8, 6))
plt.clf()

# Plot the training points
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,
            edgecolor='k')
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())

# To getter a better understanding of interaction of the dimensions
# plot the first three PCA dimensions
fig = plt.figure(1, figsize=(8, 6))
ax = Axes3D(fig, elev=-150, azim=110)
X_reduced = PCA(n_components=3).fit_transform(iris.data)
ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,
           cmap=plt.cm.Set1, edgecolor='k', s=40)
ax.set_title(&quot;First three PCA directions&quot;)
ax.set_xlabel(&quot;1st eigenvector&quot;)
ax.w_xaxis.set_ticklabels([])
ax.set_ylabel(&quot;2nd eigenvector&quot;)
ax.w_yaxis.set_ticklabels([])
ax.set_zlabel(&quot;3rd eigenvector&quot;)
ax.w_zaxis.set_ticklabels([])

plt.show()
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  降维前，我们选取了iris数据集中的两个特征sepal length（花萼长度）和sepal width（花萼宽度）来绘制数据分布，由图可以看出，数据集中有三种花，但是三者相互混杂，难以区分清楚。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0512005.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
  iris数据集本有4个特征，这时我们采用pca算法，将4维数据变化为3维。从图中的结果可以看出，经过变换后，三种类别区分的更加清楚了。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/202005/0512006.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;pca降维小结&quot;&gt;PCA降维小结&lt;/h1&gt;
&lt;p&gt;  1.实现过程有两种：Python的Numpy库；SKlearn的PCA模块，两者的计算结果是相同的。
  2.数据降维的结果不一定都是好的，所以在解决实际问题，要同时计算降维前与降维后的结果，进行比较。
  3.降维后的数据是不可解释的，但不影响最终的计算结果。&lt;/p&gt;
</description>
        <pubDate>Tue, 12 May 2020 08:18:00 +0800</pubDate>
        <link>http://localhost:4000/2020/05/12/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90PCA/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/12/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90PCA/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>ML2--混淆矩阵及绘制</title>
        <description>&lt;h1 id=&quot;混淆矩阵confusion-matrix&quot;&gt;混淆矩阵（confusion matrix）&lt;/h1&gt;
&lt;p&gt;  混淆矩阵也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。具体评价指标有总体精度、制图精度、用户精度等，这些精度指标从不同的侧面反映了图像分类的精度。在人工智能中，混淆矩阵（confusion matrix）是可视化工具，特别用于监督学习，在无监督学习一般叫做匹配矩阵。矩阵的每一行表示预测类中的实例，而每一列表示实际类中的实例（反之亦然）。 这个名字源于这样一个事实，即很容易看出系统是否混淆了两个类。&lt;/p&gt;

&lt;p&gt;  一句话解释版本：&lt;br /&gt;
  &lt;strong&gt;混淆矩阵就是分别统计分类模型归错类，归对类的观测值个数，然后把结果放在一个表里展示出来。这个表就是混淆矩阵。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;  下图是混淆矩阵的一个例子&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_20200512001.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  其中灰色部分是真实分类和预测分类结果相一致的，绿色部分是真实分类和预测分类不一致的，即分类错误的。&lt;/p&gt;
&lt;h1 id=&quot;二分类用例&quot;&gt;二分类用例&lt;/h1&gt;
&lt;p&gt;  以分类模型中最简单的二分类为例，对于这种问题，我们的模型最终需要判断样本的结果是0还是1，或者说是positive还是negative。&lt;/p&gt;

&lt;p&gt;  我们通过样本的采集，能够直接知道真实情况下，哪些数据结果是positive，哪些结果是negative。同时，我们通过用样本数据跑出分类型模型的结果，也可以知道模型认为这些数据哪些是positive，哪些是negative。&lt;/p&gt;

&lt;p&gt;  因此，我们就能得到这样四个基础指标，我称他们是一级指标（最底层的）：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;真实值是positive，模型认为是positive的数量（True Positive=TP）
真实值是positive，模型认为是negative的数量（False Negative=FN）：这就是统计学上的第二类错误（Type II Error）
真实值是negative，模型认为是positive的数量（False Positive=FP）：这就是统计学上的第一类错误（Type I Error）
真实值是negative，模型认为是negative的数量（True Negative=TN）
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  将这四个指标一起呈现在表格中，就能得到如下这样一个矩阵，我们称它为混淆矩阵（Confusion Matrix）:&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_20200512002.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;混淆矩阵的指标&quot;&gt;混淆矩阵的指标&lt;/h1&gt;
&lt;p&gt;  预测性分类模型，肯定是希望越准越好。那么，对应到混淆矩阵中，那肯定是希望&lt;strong&gt;TP与TN的数量大，而FP与FN的数量小&lt;/strong&gt;。所以当我们得到了模型的混淆矩阵后，就需要去看有多少观测值在第二、四象限对应的位置，这里的数值越多越好；反之，在第一、三象限对应位置出现的观测值肯定是越少越好。&lt;/p&gt;
&lt;h2 id=&quot;二级指标&quot;&gt;二级指标&lt;/h2&gt;
&lt;p&gt;  但是，混淆矩阵里面统计的是个数，有时候面对大量的数据，光凭算个数，很难衡量模型的优劣。因此混淆矩阵在基本的统计结果上又延伸了如下4个指标，我称他们是二级指标（通过最底层指标加减乘除得到的）：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;准确率（Accuracy）针对整个模型
精确率（Precision）
灵敏度（Sensitivity）:就是召回率（Recall）
特异度（Specificity）
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  用表格的方式将这四种指标的定义、计算、理解进行了汇总：&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_202005120013.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
  通过上面的四个二级指标，可以将混淆矩阵中数量的结果转化为0-1之间的比率。便于进行标准化的衡量。
  在这四个指标的基础上在进行拓展，会产令另外一个三级指标。&lt;/p&gt;

&lt;h2 id=&quot;三级指标&quot;&gt;三级指标&lt;/h2&gt;
&lt;p&gt;  这个指标叫做F1 Score。他的计算公式是：&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_20200512004.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  其中，P代表Precision，R代表Recall。&lt;/p&gt;

&lt;p&gt;  F1-Score指标综合了Precision与Recall的产出的结果。F1-Score的取值范围从0到1的，1代表模型的输出最好，0代表模型的输出结果最差。&lt;/p&gt;
&lt;h1 id=&quot;混淆矩阵的实例&quot;&gt;混淆矩阵的实例&lt;/h1&gt;
&lt;p&gt;  当分类问题是二分问题是，混淆矩阵可以用上面的方法计算。当分类的结果多于两种的时候，混淆矩阵同时适用。&lt;/p&gt;

&lt;p&gt;  以下面的混淆矩阵为例，我们的模型目的是为了预测样本是什么动物，这是我们的结果：&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_20200512005.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  通过混淆矩阵，我们可以得到如下结论：&lt;/p&gt;

&lt;h3 id=&quot;accuracy&quot;&gt;Accuracy&lt;/h3&gt;

&lt;p&gt;  在总共66个动物中，我们一共预测对了10 + 15 + 20=45个样本，所以准确率（Accuracy）=45/66 = 68.2%。&lt;/p&gt;

&lt;p&gt;以猫为例，我们可以将上面的图合并为二分问题：&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_20200512006.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;precision&quot;&gt;Precision&lt;/h3&gt;

&lt;p&gt;  所以，以猫为例，模型的结果告诉我们，66只动物里有13只是猫，但是其实这13只猫只有10只预测对了。模型认为是猫的13只动物里，有1条狗，两只猪。所以，Precision（猫）= 10/13 = 76.9%&lt;/p&gt;

&lt;h3 id=&quot;recall&quot;&gt;Recall&lt;/h3&gt;

&lt;p&gt;  以猫为例，在总共18只真猫中，我们的模型认为里面只有10只是猫，剩下的3只是狗，5只都是猪。这5只八成是橘猫，能理解。所以，Recall（猫）= 10/18 = 55.6%&lt;/p&gt;

&lt;h3 id=&quot;specificity&quot;&gt;Specificity&lt;/h3&gt;

&lt;p&gt;  以猫为例，在总共48只不是猫的动物中，模型认为有45只不是猫。所以，Specificity（猫）= 45/48 = 93.8%。&lt;/p&gt;

&lt;p&gt;  虽然在45只动物里，模型依然认为错判了6只狗与4只猫，但是从猫的角度而言，模型的判断是没有错的。（这里是参见了Wikipedia，Confusion Matrix的解释,https://en.wikipedia.org/wiki/Confusion_matrix）&lt;/p&gt;

&lt;h3 id=&quot;f1-score&quot;&gt;F1-Score&lt;/h3&gt;

&lt;p&gt;  通过公式，可以计算出，对猫而言，F1-Score=（2 * 0.769 *  0.556）/（ 0.769 +  0.556） = 64.54%&lt;/p&gt;

&lt;p&gt;  同样，我们也可以分别计算猪与狗各自的二级指标与三级指标值。&lt;/p&gt;
&lt;h1 id=&quot;方法调用&quot;&gt;方法调用&lt;/h1&gt;
&lt;p&gt;  官方文档中给出的用法是&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)
y_true: 是样本真实分类结果，
y_pred: 是样本预测分类结果
labels：是所给出的类别，通过这个可对类别进行选择
sample_weight : 样本权重
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;实现例子：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from sklearn.metrics import confusion_matrix

y_true=[2,1,0,1,2,0]
y_pred=[2,0,0,1,2,1]

C=confusion_matrix(y_true, y_pred)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;运行结果:&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_20200512007.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  这儿没有标注类别：下图是标注类别以后，更加好理解&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_20200512008.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;绘制混淆矩阵&quot;&gt;绘制混淆矩阵&lt;/h1&gt;
&lt;h2 id=&quot;混淆矩阵的计算&quot;&gt;混淆矩阵的计算&lt;/h2&gt;
&lt;p&gt;  混淆矩阵就是我们会计算最后分类错误的个数, 如计算将class1分为class2的个数，以此类推。&lt;/p&gt;

&lt;p&gt;  我们可以使用下面的方式来进行混淆矩阵的计算。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# 绘制混淆矩阵
def confusion_matrix(preds, labels, conf_matrix):
    preds = torch.argmax(preds, 1)
    for p, t in zip(preds, labels):
        conf_matrix[p, t] += 1
    return conf_matrix
conf_matrix = torch.zeros(10, 10)
for data, target in test_loader:
    output = fullModel(data.to(device))
    conf_matrix = confusion_matrix(output, target, conf_matrix)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  最后得到的conf_matrix就是混淆矩阵的值。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_20200512009.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;混淆矩阵的绘制plot-a-confusion-matrix&quot;&gt;混淆矩阵的绘制(Plot a confusion matrix)&lt;/h2&gt;
&lt;p&gt;  有了上面的混淆矩阵中具体的值，下面就是进行可视化的步骤。可视化我们使用seaborn来进行完成。因为我这里conf_matrix的值是tensor, 所以需要先转换为Numpy.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import seaborn as sn
df_cm = pd.DataFrame(conf_matrix.numpy(),
                     index = [i for i in list(Attack2Index.keys())],
                     columns = [i for i in list(Attack2Index.keys())])
plt.figure(figsize = (10,7))
sn.heatmap(df_cm, annot=True, cmap=&quot;BuPu&quot;)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  最终的混淆矩阵的图如下所示：&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_202005120010.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;混淆矩阵的可视化进行美化&quot;&gt;混淆矩阵的可视化(进行美化)&lt;/h3&gt;
&lt;p&gt;  我们还可以对混淆矩阵做更多的处理, 使得显示的时候能更加好看一些. 下面的绘制混淆矩阵的函数我是在下面的链接里看到的, 最终的效果很是不错。&lt;/p&gt;

&lt;p&gt;参考链接 : AE_RL_NSL_KDD&lt;/p&gt;

&lt;p&gt;这里简单贴一下代码，可以方便直接进行使用。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;import itertools
# 绘制混淆矩阵
def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    &quot;&quot;&quot;
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    Input
    - cm : 计算出的混淆矩阵的值
    - classes : 混淆矩阵中每一行每一列对应的列
    - normalize : True:显示百分比, False:显示个数
    &quot;&quot;&quot;
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print(&quot;Normalized confusion matrix&quot;)
    else:
        print('Confusion matrix, without normalization')
    print(cm)
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment=&quot;center&quot;,
                 color=&quot;white&quot; if cm[i, j] &amp;gt; thresh else &quot;black&quot;)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  测试数据如下所示：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cnf_matrix = np.array([[8707, 64, 731, 164, 45],
                      [1821, 5530, 79, 0, 28],
                      [266, 167, 1982, 4, 2],
                      [691, 0, 107, 1930, 26],
                      [30, 0, 111, 17, 42]])
attack_types = ['Normal', 'DoS', 'Probe', 'R2L', 'U2R']
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  我们分别测试normalize=True/False的效果。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;plot_confusion_matrix(cnf_matrix, classes=attack_types, normalize=True, title='Normalized confusion matrix')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_202005120011.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;plot_confusion_matrix(cnf_matrix, classes=attack_types, normalize=False, title='Normalized confusion matrix')
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_202005120012.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 12 May 2020 08:10:00 +0800</pubDate>
        <link>http://localhost:4000/2020/05/12/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5%E5%8F%8A%E7%BB%98%E5%88%B6/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/12/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5%E5%8F%8A%E7%BB%98%E5%88%B6/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>ML1--Scikit-Learn简介</title>
        <description>&lt;h1 id=&quot;一简介&quot;&gt;一、简介&lt;/h1&gt;

&lt;p&gt;  对Python语言有所了解的科研人员可能都知道SciPy——一个开源的基于Python的科学计算工具包。基于SciPy，目前开发者们针对不同的应用领域已经发展出了为数众多的分支版本，它们被统一称为Scikits，即SciPy工具包的意思。而在这些分支版本中，最有名，也是专门面向机器学习的一个就是Scikit-learn。&lt;/p&gt;

&lt;p&gt;  Scikit-learn项目最早由数据科学家David Cournapeau 在2007 年发起，需要NumPy和SciPy等其他包的支持，是Python语言中专门针对机器学习应用而发展起来的一款开源框架。它的维护也主要依靠开源社区。&lt;/p&gt;

&lt;h1 id=&quot;二特点&quot;&gt;二、特点&lt;/h1&gt;

&lt;p&gt;  作为专门面向机器学习的Python开源框架，Scikit-learn可以在一定范围内为开发者提供非常好的帮助。它内部实现了各种各样成熟的算法，容易安装和使用，样例丰富，而且教程和文档也非常详细。&lt;/p&gt;

&lt;p&gt;  另一方面，Scikit-learn也有缺点。例如它不支持深度学习和强化学习，这在今天已经是应用非常广泛的技术。此外，它也不支持图模型和序列预测，不支持Python之外的语言，不支持PyPy，也不支持GPU加速。&lt;/p&gt;

&lt;p&gt;  看到这里可能会有人担心Scikit-learn的性能表现，这里需要指出的是：如果不考虑多层神经网络的相关应用，Scikit-learn的性能表现是非常不错的。究其原因，一方面是因为其内部算法的实现十分高效，另一方面或许可以归功于Cython编译器；通过Cython在Scikit-learn框架内部生成C语言代码的运行方式，Scikit-learn消除了大部分的性能瓶颈。&lt;/p&gt;

&lt;h1 id=&quot;三主要类或用过的类&quot;&gt;三、主要类或用过的类&lt;/h1&gt;

&lt;p&gt;  Scikit-learn的基本功能主要被分为六大部分：分类，回归，聚类，数据降维，模型选择和数据预处理。&lt;/p&gt;

&lt;h2 id=&quot;1preprocessing-预处理&quot;&gt;（1）Preprocessing 预处理&lt;/h2&gt;

&lt;p&gt;· 应用：转换输入数据，规范化、编码化&lt;/p&gt;

&lt;p&gt;· 模块：preprocessing，feature_extraction，transformer（转换器）&lt;/p&gt;

&lt;h2 id=&quot;2dimensionality-reduction-降维&quot;&gt;（2）Dimensionality reduction 降维&lt;/h2&gt;

&lt;p&gt;· 应用：Visualization（可视化），Increased efficiency（提高效率）&lt;/p&gt;

&lt;p&gt;· 算法：主成分分析(PCA)、非负矩阵分解（NMF），feature_selection(特征选择)等&lt;/p&gt;

&lt;h2 id=&quot;3classification-分类&quot;&gt;（3）Classification 分类&lt;/h2&gt;

&lt;p&gt;· 应用：二元分类问题、多分类问题、Image recognition 图像识别等&lt;/p&gt;

&lt;p&gt;· 算法：逻辑回归、SVM，最近邻，随机森林，Naïve Bayes，神经网络等&lt;/p&gt;

&lt;h2 id=&quot;4regression-回归&quot;&gt;（4）Regression 回归&lt;/h2&gt;

&lt;p&gt;· 应用：Drug response 药物反应，Stock prices 股票价格&lt;/p&gt;

&lt;p&gt;· 算法：线性回归、SVR，ridge regression，Lasso，最小角回归（LARS）等&lt;/p&gt;

&lt;h2 id=&quot;5clustering-聚类&quot;&gt;（5）Clustering 聚类&lt;/h2&gt;

&lt;p&gt;· 应用：客户细分，分组实验结果&lt;/p&gt;

&lt;p&gt;· 算法：k-Means，spectral clustering(谱聚类)，mean-shift（均值漂移）&lt;/p&gt;

&lt;h2 id=&quot;6model-selection-模型选择&quot;&gt;（6）Model selection 模型选择&lt;/h2&gt;

&lt;p&gt;· 目标：通过参数调整提高精度&lt;/p&gt;

&lt;p&gt;· 模块：pipeline(流水线)，grid_search（网格搜索），cross_validation( 交叉验证)，metrics（度量），learning_curve（学习曲线）&lt;/p&gt;

&lt;h2 id=&quot;7模型融合&quot;&gt;（7）模型融合&lt;/h2&gt;

&lt;p&gt;· 模块：ensemble(集成学习)、&lt;/p&gt;

&lt;h2 id=&quot;8辅助工具&quot;&gt;（8）辅助工具&lt;/h2&gt;

&lt;p&gt;· 模块：exceptions(异常和警告)、dataset（自带数据集）、utils、sklearn.base&lt;/p&gt;

&lt;h1 id=&quot;四选择模型流程&quot;&gt;四、选择模型流程&lt;/h1&gt;
&lt;p&gt;  学习 Sklearn 时，不要直接去用，先了解一下都有什么模型方法，然后选择适当的方法，来达到你的目标。&lt;/p&gt;

&lt;p&gt;  Sklearn 官网提供了一个流程图，蓝色圆圈内是判断条件，绿色方框内是可以选择的算法：&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_2020051001.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
  从 START 开始，首先看数据的样本是否 &amp;gt;50，小于则需要收集更多的数据。&lt;/p&gt;

&lt;p&gt;  由图中，可以看到算法有四类，&lt;strong&gt;分类，回归，聚类，降维&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;  其中分类和回归是监督式学习，即每个数据对应一个label。
聚类是非监督式学习，即没有label。
  另外一类是降维，当数据集有很多很多属性的时候，可以通过降维 算法把属性归纳起来。例如 20 个属性只变成 2 个，注意，这不是挑出 2 个，而是压缩成为 2 个，它们集合了 20 个属性的所有特征，相当于把重要的信息提取的更好，不重要的信息就不要了。&lt;/p&gt;

&lt;p&gt;  然后看问题属于哪一类问题，是分类还是回归，还是聚类，就选择相应的算法。当然还要考虑数据的大小，例如 100K 是一个阈值。&lt;/p&gt;

&lt;p&gt;  可以发现有些方法是既可以作为分类，也可以作为回归，例如 SGD。&lt;/p&gt;

&lt;h1 id=&quot;五应用模型&quot;&gt;五、应用模型&lt;/h1&gt;
&lt;p&gt;  Sklearn 把所有机器学习的模式整合统一起来了，学会了一个模式就可以通吃其他不同类型的学习模式。
  例如，分类器，
  Sklearn 本身就有很多数据库，可以用来练习。我们用其中 Iris 的数据为例，这种花有四个属性，花瓣的长宽，茎的长宽，根据这些属性把花分为三类。
  我们要用分类器 去把四种类型的花分开。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/img/image_20200510002.png&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
  今天用 KNN classifier，就是选择几个临近点，综合它们做个平均来作为预测值。&lt;/p&gt;

&lt;p&gt;使用模型的步骤：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;导入模块&lt;/li&gt;
  &lt;li&gt;创建数据&lt;/li&gt;
  &lt;li&gt;建立模型－训练－预测
    &lt;h2 id=&quot;1-导入模块&quot;&gt;1. 导入模块&lt;/h2&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from __future__ import print_function
from sklearn import datasets   
from sklearn.cross_validation import train_test_split   
from sklearn.neighbors import KNeighborsClassifier
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;h2 id=&quot;2-创建数据&quot;&gt;2. 创建数据&lt;/h2&gt;
    &lt;p&gt;  加载 iris 的数据，把属性存在 X，类别标签存在 y：&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;iris = datasets.load_iris()
iris_X = iris.data
iris_y = iris.target
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;  观察一下数据集，X 有四个属性，y 有 0，1，2 三类：&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;print(iris_X[:2, :])
print(iris_y)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&quot;&quot;&quot;
[[ 5.1  3.5  1.4  0.2]
 [ 4.9  3.   1.4  0.2]]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
 &quot;&quot;&quot;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;  把数据集分为训练集和测试集，其中 test_size=0.3，即测试集占总数据的 30%：&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;X_train, X_test, y_train, y_test = train_test_split(
 iris_X, iris_y, test_size=0.3)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;  可以看到分开后的数据集，顺序也被打乱，这样更有利于学习模型：&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;print(y_train)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&quot;&quot;&quot;
[2 1 0 1 0 0 1 1 1 1 0 0 1 2 1 1 1 0 2 2 1 1 1 1 0 2 2 0 2 2 2 2 2 0 1 2 2
 2 2 2 2 0 1 2 2 1 1 1 0 0 1 2 0 1 0 1 0 1 2 2 0 1 2 2 2 1 1 1 1 2 2 2 1 0
 1 1 0 0 0 2 0 1 0 0 1 2 0 2 2 0 0 2 2 2 1 2 0 0 2 1 2 0 0 1 2]
 &quot;&quot;&quot;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;h2 id=&quot;3-定义模型训练模型预测&quot;&gt;3. 定义模型－训练模型－预测&lt;/h2&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;  定义模块方式 KNeighborsClassifier()，用fit 来训练 training data，这一步就完成了训练的所有步骤，后面的 knn 就已经是训练好的模型，可以直接用来 predict 测试集的数据，对比用模型预测的值与真实的值，可以看到大概模拟出了数据，但是有误差，是不会完完全全预测正确的。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
print(knn.predict(X_test))
print(y_test)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&quot;&quot;&quot;
[2 0 0 1 2 2 0 0 0 1 2 2 1 1 2 1 2 1 0 0 0 2 1 2 0 0 0 0 1 0 2 0 0 2 1 0 1
 0 0 1 0 1 2 0 1]
[2 0 0 1 2 1 0 0 0 1 2 2 1 1 2 1 2 1 0 0 0 2 1 2 0 0 0 0 1 0 2 0 0 2 1 0 1
 0 0 1 0 1 2 0 1]
 &quot;&quot;&quot;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Sun, 10 May 2020 08:10:00 +0800</pubDate>
        <link>http://localhost:4000/2020/05/10/Scikit-Learn%E7%AE%80%E4%BB%8B/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/10/Scikit-Learn%E7%AE%80%E4%BB%8B/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>木渎古镇</title>
        <description>&lt;p&gt;  木渎古镇，别名渎川，胥江，雅称香溪，位于江苏省东南部，苏州古城西部，地处太湖流域，是江南著名的风景名胜区，素有“吴中第一镇”、“秀绝冠江南”之誉。木渎古镇是与苏州城同龄的汉族水乡文化古镇，已有2500多年历史。2008年08月，评为国家级AAAA景区，也是太湖风景区十三个景区之一。&lt;br /&gt;
  木渎是汉族传统手工艺品之乡。历史上有泥塑名家袁遇昌、银器高手朱碧山、琢玉名家陆子冈、绣圣沈寿等名家高手，他们的作品都是国家级收藏的珍品。有严家花园、虹饮山房、古松园、灵岩山、天平山等著名文物景点。
&lt;img src=&quot;/img/image_2020050701.jpg&quot; /&gt;
&lt;img src=&quot;/img/image_20200507004.jpg&quot; /&gt;
&lt;img src=&quot;/img/image_20200507001.jpg&quot; /&gt;
&lt;img src=&quot;/img/image_20200507005.jpg&quot; /&gt;
&lt;img src=&quot;/img/image_20200507002.jpg&quot; /&gt;
&lt;img src=&quot;/img/image_20200507006.jpg&quot; /&gt;
&lt;img src=&quot;/img/image_20200507003.jpg&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 07 May 2020 18:10:00 +0800</pubDate>
        <link>http://localhost:4000/2020/05/07/%E6%9C%A8%E6%B8%8E%E5%8F%A4%E9%95%87/</link>
        <guid isPermaLink="true">http://localhost:4000/2020/05/07/%E6%9C%A8%E6%B8%8E%E5%8F%A4%E9%95%87/</guid>
        
        <category>苏州</category>
        
        
      </item>
    
      <item>
        <title>秋实</title>
        <description>&lt;p&gt;&lt;strong&gt;日期&lt;/strong&gt;&lt;br /&gt;
  2019年09月09日下午&lt;br /&gt;
&lt;strong&gt;目的地&lt;/strong&gt;&lt;br /&gt;
  北京&lt;br /&gt;
&lt;strong&gt;行程&lt;/strong&gt;&lt;br /&gt;
  北京的秋天。
&lt;img src=&quot;/img/bg_2019110701.jpg&quot; /&gt;
&lt;img src=&quot;/img/20191109002.jpeg&quot; /&gt;
&lt;img src=&quot;/img/20191109003.jpeg&quot; /&gt;
&lt;img src=&quot;/img/20191109004.jpeg&quot; /&gt;
&lt;img src=&quot;/img/20191109005.jpeg&quot; /&gt;
&lt;img src=&quot;/img/20191109006.jpeg&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Nov 2019 20:10:00 +0800</pubDate>
        <link>http://localhost:4000/2019/11/09/%E7%A7%8B%E5%AE%9E/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/11/09/%E7%A7%8B%E5%AE%9E/</guid>
        
        <category>秋天</category>
        
        
      </item>
    
      <item>
        <title>网络模型及协议相关（应用层）</title>
        <description>&lt;!-- GFM-TOC --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#域名系统&quot;&gt;域名系统&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#文件传送协议&quot;&gt;文件传送协议&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#动态主机配置协议&quot;&gt;动态主机配置协议&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#远程登录协议&quot;&gt;远程登录协议&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#电子邮件协议&quot;&gt;电子邮件协议&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1-smtp&quot;&gt;1. SMTP&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-pop3&quot;&gt;2. POP3&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3-imap&quot;&gt;3. IMAP&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#常用端口&quot;&gt;常用端口&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#web-页面请求过程&quot;&gt;Web 页面请求过程&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1-dhcp-配置主机信息&quot;&gt;1. DHCP 配置主机信息&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-arp-解析-mac-地址&quot;&gt;2. ARP 解析 MAC 地址&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3-dns-解析域名&quot;&gt;3. DNS 解析域名&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4-http-请求页面&quot;&gt;4. HTTP 请求页面&lt;/a&gt;
&lt;!-- GFM-TOC --&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;域名系统&quot;&gt;域名系统&lt;/h1&gt;

&lt;p&gt;DNS 是一个分布式数据库，提供了主机名和 IP 地址之间相互转换的服务。这里的分布式数据库是指，每个站点只保留它自己的那部分数据。&lt;/p&gt;

&lt;p&gt;域名具有层次结构，从上到下依次为：根域名、顶级域名、二级域名。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/pics/b54eeb16-0b0e-484c-be62-306f57c40d77.jpg&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;DNS 可以使用 UDP 或者 TCP 进行传输，使用的端口号都为 53。大多数情况下 DNS 使用 UDP 进行传输，这就要求域名解析器和域名服务器都必须自己处理超时和重传从而保证可靠性。在两种情况下会使用 TCP 进行传输：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果返回的响应超过的 512 字节（UDP 最大只支持 512 字节的数据）。&lt;/li&gt;
  &lt;li&gt;区域传送（区域传送是主域名服务器向辅助域名服务器传送变化的那部分数据）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;文件传送协议&quot;&gt;文件传送协议&lt;/h1&gt;

&lt;p&gt;FTP 使用 TCP 进行连接，它需要两个连接来传送一个文件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;控制连接：服务器打开端口号 21 等待客户端的连接，客户端主动建立连接后，使用这个连接将客户端的命令传送给服务器，并传回服务器的应答。&lt;/li&gt;
  &lt;li&gt;数据连接：用来传送一个文件数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据数据连接是否是服务器端主动建立，FTP 有主动和被动两种模式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;主动模式：服务器端主动建立数据连接，其中服务器端的端口号为 20，客户端的端口号随机，但是必须大于 1024，因为 0~1023 是熟知端口号。&lt;/li&gt;
&lt;/ul&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/pics/03f47940-3843-4b51-9e42-5dcaff44858b.jpg&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;被动模式：客户端主动建立数据连接，其中客户端的端口号由客户端自己指定，服务器端的端口号随机。&lt;/li&gt;
&lt;/ul&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/pics/be5c2c61-86d2-4dba-a289-b48ea23219de.jpg&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;主动模式要求客户端开放端口号给服务器端，需要去配置客户端的防火墙。被动模式只需要服务器端开放端口号即可，无需客户端配置防火墙。但是被动模式会导致服务器端的安全性减弱，因为开放了过多的端口号。&lt;/p&gt;

&lt;h1 id=&quot;动态主机配置协议&quot;&gt;动态主机配置协议&lt;/h1&gt;

&lt;p&gt;DHCP (Dynamic Host Configuration Protocol) 提供了即插即用的连网方式，用户不再需要手动配置 IP 地址等信息。&lt;/p&gt;

&lt;p&gt;DHCP 配置的内容不仅是 IP 地址，还包括子网掩码、网关 IP 地址。&lt;/p&gt;

&lt;p&gt;DHCP 工作过程如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;客户端发送 Discover 报文，该报文的目的地址为 255.255.255.255:67，源地址为 0.0.0.0:68，被放入 UDP 中，该报文被广播到同一个子网的所有主机上。如果客户端和 DHCP 服务器不在同一个子网，就需要使用中继代理。&lt;/li&gt;
  &lt;li&gt;DHCP 服务器收到 Discover 报文之后，发送 Offer 报文给客户端，该报文包含了客户端所需要的信息。因为客户端可能收到多个 DHCP 服务器提供的信息，因此客户端需要进行选择。&lt;/li&gt;
  &lt;li&gt;如果客户端选择了某个 DHCP 服务器提供的信息，那么就发送 Request 报文给该 DHCP 服务器。&lt;/li&gt;
  &lt;li&gt;DHCP 服务器发送 Ack 报文，表示客户端此时可以使用提供给它的信息。&lt;/li&gt;
&lt;/ol&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/pics/23219e4c-9fc0-4051-b33a-2bd95bf054ab.jpg&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;远程登录协议&quot;&gt;远程登录协议&lt;/h1&gt;

&lt;p&gt;TELNET 用于登录到远程主机上，并且远程主机上的输出也会返回。&lt;/p&gt;

&lt;p&gt;TELNET 可以适应许多计算机和操作系统的差异，例如不同操作系统系统的换行符定义。&lt;/p&gt;

&lt;h1 id=&quot;电子邮件协议&quot;&gt;电子邮件协议&lt;/h1&gt;

&lt;p&gt;一个电子邮件系统由三部分组成：用户代理、邮件服务器以及邮件协议。&lt;/p&gt;

&lt;p&gt;邮件协议包含发送协议和读取协议，发送协议常用 SMTP，读取协议常用 POP3 和 IMAP。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/pics/7b3efa99-d306-4982-8cfb-e7153c33aab4.png&quot; width=&quot;700&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-smtp&quot;&gt;1. SMTP&lt;/h2&gt;

&lt;p&gt;SMTP 只能发送 ASCII 码，而互联网邮件扩充 MIME 可以发送二进制文件。MIME 并没有改动或者取代 SMTP，而是增加邮件主体的结构，定义了非 ASCII 码的编码规则。&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;/pics/ed5522bb-3a60-481c-8654-43e7195a48fe.png&quot; width=&quot;&quot; /&gt; &lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-pop3&quot;&gt;2. POP3&lt;/h2&gt;

&lt;p&gt;POP3 的特点是只要用户从服务器上读取了邮件，就把该邮件删除。但最新版本的 POP3 可以不删除邮件。&lt;/p&gt;

&lt;h2 id=&quot;3-imap&quot;&gt;3. IMAP&lt;/h2&gt;

&lt;p&gt;IMAP 协议中客户端和服务器上的邮件保持同步，如果不手动删除邮件，那么服务器上的邮件也不会被删除。IMAP 这种做法可以让用户随时随地去访问服务器上的邮件。&lt;/p&gt;

&lt;h1 id=&quot;常用端口&quot;&gt;常用端口&lt;/h1&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;应用&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;应用层协议&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;端口号&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;传输层协议&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;备注&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;域名解析&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;DNS&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;53&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;UDP/TCP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;长度超过 512 字节时使用 TCP&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;动态主机配置协议&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;DHCP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;67/68&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;UDP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;简单网络管理协议&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SNMP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;161/162&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;UDP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;文件传送协议&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FTP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20/21&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TCP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;控制连接 21，数据连接 20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;远程终端协议&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TELNET&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;23&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TCP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;超文本传送协议&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;HTTP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TCP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;简单邮件传送协议&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SMTP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;25&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TCP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;邮件读取协议&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POP3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;110&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TCP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;网际报文存取协议&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;IMAP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;143&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TCP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;web-页面请求过程&quot;&gt;Web 页面请求过程&lt;/h1&gt;

&lt;h2 id=&quot;1-dhcp-配置主机信息&quot;&gt;1. DHCP 配置主机信息&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;假设主机最开始没有 IP 地址以及其它信息，那么就需要先使用 DHCP 来获取。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;主机生成一个 DHCP 请求报文，并将这个报文放入具有目的端口 67 和源端口 68 的 UDP 报文段中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;该报文段则被放入在一个具有广播 IP 目的地址(255.255.255.255) 和源 IP 地址（0.0.0.0）的 IP 数据报中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;该数据报则被放置在 MAC 帧中，该帧具有目的地址 FF:&lt;zero-width space=&quot;&quot;&gt;FF:&lt;zero-width space=&quot;&quot;&gt;FF:&lt;zero-width space=&quot;&quot;&gt;FF:&lt;zero-width space=&quot;&quot;&gt;FF:FF，将广播到与交换机连接的所有设备。&lt;/zero-width&gt;&lt;/zero-width&gt;&lt;/zero-width&gt;&lt;/zero-width&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;连接在交换机的 DHCP 服务器收到广播帧之后，不断地向上分解得到 IP 数据报、UDP 报文段、DHCP 请求报文，之后生成 DHCP ACK 报文，该报文包含以下信息：IP 地址、DNS 服务器的 IP 地址、默认网关路由器的 IP 地址和子网掩码。该报文被放入 UDP 报文段中，UDP 报文段有被放入 IP 数据报中，最后放入 MAC 帧中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;该帧的目的地址是请求主机的 MAC 地址，因为交换机具有自学习能力，之前主机发送了广播帧之后就记录了 MAC 地址到其转发接口的交换表项，因此现在交换机就可以直接知道应该向哪个接口发送该帧。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;主机收到该帧后，不断分解得到 DHCP 报文。之后就配置它的 IP 地址、子网掩码和 DNS 服务器的 IP 地址，并在其 IP 转发表中安装默认网关。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-arp-解析-mac-地址&quot;&gt;2. ARP 解析 MAC 地址&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;主机通过浏览器生成一个 TCP 套接字，套接字向 HTTP 服务器发送 HTTP 请求。为了生成该套接字，主机需要知道网站的域名对应的 IP 地址。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;主机生成一个 DNS 查询报文，该报文具有 53 号端口，因为 DNS 服务器的端口号是 53。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;该 DNS 查询报文被放入目的地址为 DNS 服务器 IP 地址的 IP 数据报中。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;该 IP 数据报被放入一个以太网帧中，该帧将发送到网关路由器。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;DHCP 过程只知道网关路由器的 IP 地址，为了获取网关路由器的 MAC 地址，需要使用 ARP 协议。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;主机生成一个包含目的地址为网关路由器 IP 地址的 ARP 查询报文，将该 ARP 查询报文放入一个具有广播目的地址（FF:&lt;zero-width space=&quot;&quot;&gt;FF:&lt;zero-width space=&quot;&quot;&gt;FF:&lt;zero-width space=&quot;&quot;&gt;FF:&lt;zero-width space=&quot;&quot;&gt;FF:FF）的以太网帧中，并向交换机发送该以太网帧，交换机将该帧转发给所有的连接设备，包括网关路由器。&lt;/zero-width&gt;&lt;/zero-width&gt;&lt;/zero-width&gt;&lt;/zero-width&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;网关路由器接收到该帧后，不断向上分解得到 ARP 报文，发现其中的 IP 地址与其接口的 IP 地址匹配，因此就发送一个 ARP 回答报文，包含了它的 MAC 地址，发回给主机。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-dns-解析域名&quot;&gt;3. DNS 解析域名&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;知道了网关路由器的 MAC 地址之后，就可以继续 DNS 的解析过程了。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;网关路由器接收到包含 DNS 查询报文的以太网帧后，抽取出 IP 数据报，并根据转发表决定该 IP 数据报应该转发的路由器。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;因为路由器具有内部网关协议（RIP、OSPF）和外部网关协议（BGP）这两种路由选择协议，因此路由表中已经配置了网关路由器到达 DNS 服务器的路由表项。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;到达 DNS 服务器之后，DNS 服务器抽取出 DNS 查询报文，并在 DNS 数据库中查找待解析的域名。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;找到 DNS 记录之后，发送 DNS 回答报文，将该回答报文放入 UDP 报文段中，然后放入 IP 数据报中，通过路由器反向转发回网关路由器，并经过以太网交换机到达主机。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-http-请求页面&quot;&gt;4. HTTP 请求页面&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;有了 HTTP 服务器的 IP 地址之后，主机就能够生成 TCP 套接字，该套接字将用于向 Web 服务器发送 HTTP GET 报文。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在生成 TCP 套接字之前，必须先与 HTTP 服务器进行三次握手来建立连接。生成一个具有目的端口 80 的 TCP SYN 报文段，并向 HTTP 服务器发送该报文段。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;HTTP 服务器收到该报文段之后，生成 TCP SYN ACK 报文段，发回给主机。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;连接建立之后，浏览器生成 HTTP GET 报文，并交付给 HTTP 服务器。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;HTTP 服务器从 TCP 套接字读取 HTTP GET 报文，生成一个 HTTP 响应报文，将 Web 页面内容放入报文主体中，发回给主机。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;浏览器收到 HTTP 响应报文后，抽取出 Web 页面内容，之后进行渲染，显示 Web 页面。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 10 Sep 2019 05:10:00 +0800</pubDate>
        <link>http://localhost:4000/2019/09/10/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%8D%8F%E8%AE%AE%E7%9B%B8%E5%85%B3-%E5%BA%94%E7%94%A8%E5%B1%82/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/09/10/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%8D%8F%E8%AE%AE%E7%9B%B8%E5%85%B3-%E5%BA%94%E7%94%A8%E5%B1%82/</guid>
        
        <category>学习</category>
        
        <category>网络</category>
        
        
      </item>
    
  </channel>
</rss>
