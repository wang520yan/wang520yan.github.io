<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
    <meta name="description" content="代码编织梦想，程序创造未来">
    <meta name="keywords"  content="彦，程序员">
    <meta name="theme-color" content="#000000">

    <!-- Open Graph -->
    <meta property="og:title" content="ML6--sklearn库的学习 - Yan Blog">
    
    <meta property="og:type" content="article">
    <meta property="og:description" content="介绍
  自2007年发布以来，scikit-learn已经成为Python重要的机器学习库了，scikit-learn简称sklearn，支持包括分类，回归，降维和聚类四大机器学习算法。还包括了特征提取，数据处理和模型评估者三大模块。

">
    
    <meta property="article:published_time" content="2020-05-18T08:10:00Z">
    
    
    <meta property="article:author" content="yan">
    
    
    <meta property="article:tag" content="机器学习">
    
    
    <meta property="og:image" content="http://localhost:4000/img/avatar-yan1.jpg">
    <meta property="og:url" content="http://localhost:4000/2020/05/18/sklearn%E5%BA%93%E7%9A%84%E5%AD%A6%E4%B9%A0/">
    <meta property="og:site_name" content="Yan Blog">

    <title>ML6--sklearn库的学习 - Yan Blog</title>

    

    <!-- Web App Manifest -->
    <link rel="manifest" href="/pwa/manifest.json">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">

    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:4000/2020/05/18/sklearn%E5%BA%93%E7%9A%84%E5%AD%A6%E4%B9%A0/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">
    <link rel="stylesheet" href="/portfolio/css/timeline.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->

<nav class="navbar navbar-default navbar-custom navbar-fixed-top">

    <div class="container-fluid">
      <!-- 打算设置logo，暂时放弃 -->
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <!-- <img src="img/logo.png" alt="" style="position: 'absolute'; width : 500px;"> -->
            <a class="navbar-brand" href="/" style="color: #08d0f7fc">Yan</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/"><h4 style="color: #08d0f7fc">主页</h4></a>
                    </li>
                    
                    
                    
                    
                    <li>
                        
                        
                        
                        <a href="/about/"><h4 style="color: #08d0f7fc">关于</h4></a>
                        
                    </li>
                    
                    
                    
                    <li>
                        
                        <a href="/archive/"><h4 style="color: #08d0f7fc">文章</h4></a>
                        
                        
                        
                    </li>
                    
                    
                    
                    
                    
                    <li>
                        
                        
                        
                    </li>
                    
                    
                    
                    <li>
                        
                        
                        
                    </li>
                    
                    
                    
                    <li>
                        
                        
                        <a href="/portfolio/"><h4 style="color: #08d0f7fc">游记</h4></a>
                        
                        
                    </li>
                    
                    
                    
                    <li>
                        
                        
                        
                    </li>
                    
                    
                    
                    
                    
                    <li>
                        
                        
                        
                    </li>
                    
                    
                    
                    <li>
                        
                        
                        
                    </li>
                    
                    
                    
                    <li>
                        
                        
                        
                    </li>
                    
                    
                    
                    <li>
                        
                        
                        
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    var __HuxNav__ = {
        close: function(){
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        },
        open: function(){
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }




    // Bind Event
    // $toggle.addEventListener('click', function(e){
    //     if ($navbar.className.indexOf('in') > 0) {
    //         __HuxNav__.close()
    //     }else{
    //         __HuxNav__.open()
    //     }
    // })

    /**
     * Since Fastclick is used to delegate 'touchstart' globally
     * to hack 300ms delay in iOS by performing a fake 'click',
     * Using 'e.stopPropagation' to stop 'touchstart' event from
     * $toggle/$collapse will break global delegation.
     *
     * Instead, we use a 'e.target' filter to prevent handler
     * added to document close HuxNav.
     *
     * Also, we use 'click' instead of 'touchstart' as compromise
     */
    document.addEventListener('click', function(e){
        if(e.target == $toggle) return;
        if(e.target.className == 'icon-bar') return;
        __HuxNav__.close();
    })

    // place1.addEventListener('mouseenter', function () {
    //   var ob = document.getElementById("palce3");
    //   ob.setAttribute("class","bak");
    // })

    // var top_div1 = document.getElementById("place1");
    // top_div1.onMouseOver=mouseAlert1;
    //
    // function mouseAlert1(){
    //   alert("123");
    //   // $("h4").css("background-color","yellow");
    // }
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/image_back_05.jpg" width="0" height="0"> -->

<!-- Post Header -->


<!-- 博客文章样式 -->

<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/image_back_05.jpg');
        background: ;
    }

    
</style>

<header class="intro-header" >

    <div class="header-mask"></div>
    
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/archive/?tag=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" title="机器学习">机器学习</a>
                        
                    </div>
                    <!-- 
                    
                     -->
                    <h1>ML6--sklearn库的学习</h1>
                    
                    <h2 class="subheading">机器学习第六课--机器学习全流程</h2>
                    <span class="meta">Posted by yan on May 18, 2020</span>
                </div>
            </div>
        </div>
    </div>
</header>





<!-- 首页的样式 -->



<!-- 照片页的样式 -->



<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <!-- Multi-Lingual -->
                

				<h1 id="介绍">介绍</h1>
<p>  自2007年发布以来，scikit-learn已经成为Python重要的机器学习库了，scikit-learn简称sklearn，支持包括分类，回归，降维和聚类四大机器学习算法。还包括了特征提取，数据处理和模型评估者三大模块。</p>

<p>  sklearn是Scipy的扩展，建立在Numpy和matplolib库的基础上。利用这几大模块的优势，可以大大的提高机器学习的效率。</p>

<p>  sklearn拥有着完善的文档，上手容易，具有着丰富的API，在学术界颇受欢迎。sklearn已经封装了大量的机器学习算法，包括LIBSVM和LIBINEAR。同时sklearn内置了大量数据集，节省了获取和整理数据集的时间。</p>

<p>  官网：https://scikit-learn.org/stable/</p>
<h1 id="官方文档解读">官方文档解读</h1>
<h2 id="sklearn官方文档的内容">sklearn官方文档的内容</h2>
<p>  定义：针对经验E和一系列的任务T和一定表现的衡量P，如果随着经验E的积累，针对定义好的任务T可以提高表现P，就说明机器具有学习能力。</p>

<div align="center"> <img src="/img/202005/0518001.png" /> </div>
<h2 id="sklearn官方文档结构">sklearn官方文档结构</h2>
<div align="center"> <img src="/img/202005/0518002.png" /> </div>

<p>  由图中，可以看到库的算法主要有四类：分类，回归，聚类，降维。其中：</p>

<p>  常用的回归：线性、决策树、SVM、KNN ；集成回归：随机森林、Adaboost、GradientBoosting、Bagging、ExtraTrees</p>

<p>  常用的分类：线性、决策树、SVM、KNN，朴素贝叶斯；集成分类：随机森林、Adaboost、GradientBoosting、Bagging、ExtraTrees</p>

<p>  常用聚类：k均值（K-means）、层次聚类（Hierarchical clustering）、DBSCAN</p>

<p>  常用降维：LinearDiscriminantAnalysis、PCA</p>

<p>  这个流程图代表：蓝色圆圈是判断条件，绿色方框是可以选择的算法，我们可以根据自己的数据特征和任务目标去找一条自己的操作路线。</p>

<p>  sklearn中包含众多数据预处理和特征工程相关的模块，虽然刚接触sklearn时，大家都会为其中包含的各种算法的广度深度所震惊，但其实sklearn六大板块中有两块都是关于数据预处理和特征工程的，两个板块互相交互，为建模之前的全部工程打下基础。</p>

<div align="center"> <img src="/img/202005/0518003.png" /> </div>

<ul>
  <li>模块preprocessing：几乎包含数据预处理的所有内容</li>
  <li>模块Impute：填补缺失值专用</li>
  <li>模块feature_selection：包含特征选择的各种方法的实践</li>
  <li>模块decomposition：包含降维算法
    <h1 id="sklearn的快速使用">sklearn的快速使用</h1>
    <p>  传统的机器学习任务从开始到建模的一般流程就是：<strong>获取数据——&gt;数据预处理——&gt;训练模型——&gt;模型评估——&gt;预测，分类</strong>。本次我们将根据传统机器学习的流程，看看在每一步流程中都有哪些常用的函数以及他们的用法是怎么样的。那么首先先看一个简单的例子：</p>
  </li>
</ul>

<p>  鸢尾花识别是一个经典的机器学习分类问题，它的数据样本中包括了4个特征变量，1个类别变量，样本总数为150。</p>

<p>  它的目标是为了根据花萼长度（sepal length）、花萼宽度（sepal width）、花瓣长度（petal length）、花瓣宽度（petal width）这四个特征来识别出鸢尾花属于山鸢尾（iris-setosa）、变色鸢尾（iris-versicolor）和维吉尼亚鸢尾（iris-virginica）中的哪一种。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
</pre></td><td class="rouge-code"><pre># 引入数据集，sklearn包含众多数据集
from sklearn import datasets
# 将数据分为测试集和训练集
from sklearn.model_selection import train_test_split
# 利用邻近点方式训练数据
from sklearn.neighbors import KNeighborsClassifier
 
# 引入数据,本次导入鸢尾花数据，iris数据包含4个特征变量
iris = datasets.load_iris()
# 特征变量
iris_X = iris.data
# print(iris_X)
print('特征变量的长度',len(iris_X))
# 目标值
iris_y = iris.target
print('鸢尾花的目标值',iris_y)
# 利用train_test_split进行训练集和测试机进行分开，test_size占30%
X_train,X_test,y_train,y_test=train_test_split(iris_X,iris_y,test_size=0.3)
# 我们看到训练数据的特征值分为3类
# print(y_train)
'''
[1 1 0 2 0 0 0 2 2 2 1 0 2 0 2 1 0 1 0 2 0 1 0 0 2 1 2 0 0 1 0 0 1 0 0 0 0
 2 2 2 1 1 1 2 0 2 0 1 1 1 1 2 2 1 2 2 2 0 2 2 2 0 1 0 1 0 0 1 2 2 2 1 1 1
 2 0 0 1 0 2 1 2 0 1 2 2 2 1 2 1 0 0 1 0 0 1 1 1 0 2 1 1 0 2 2]
 '''
# 训练数据
# 引入训练方法
knn = KNeighborsClassifier()
# 进行填充测试数据进行训练
knn.fit(X_train,y_train)
 
params = knn.get_params()
print(params)
'''
{'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski',
 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5,
 'p': 2, 'weights': 'uniform'}
 
'''
 
score = knn.score(X_test,y_test)
print("预测得分为：%s"%score)
'''
预测得分为：0.9555555555555556
[1 2 1 1 2 2 1 0 0 0 0 1 2 0 1 0 2 0 0 0 2 2 0 2 2 2 2 1 2 2 2 1 2 2 1 2 0
 2 1 2 1 1 0 2 1]
[1 2 1 1 2 2 1 0 0 0 0 1 2 0 1 0 2 0 0 0 1 2 0 2 2 2 2 1 1 2 2 1 2 2 1 2 0
 2 1 2 1 1 0 2 1]
'''
 
# 预测数据，预测特征值
print(knn.predict(X_test))
'''
[0 2 2 2 2 0 0 0 0 2 2 0 2 0 2 1 2 0 2 1 0 2 1 0 1 2 2 0 2 1 0 2 1 1 2 0 2
 1 2 0 2 1 0 1 2]
'''
# 打印真实特征值
print(y_test)
'''
[1 2 2 2 2 1 1 1 1 2 1 1 1 1 2 1 1 0 2 1 1 1 0 2 0 2 0 0 2 0 2 0 2 0 2 2 0
 2 2 0 1 0 2 0 0]
 
'''
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  下面，我们开始一步步介绍</p>
<h3 id="1-获取数据">1. 获取数据</h3>
<h4 id="11-导入sklearn数据集">1.1 导入sklearn数据集</h4>
<p>  sklearn中包含了大量的优质的数据集，在我们学习机器学习的过程中，我们可以使用这些数据集实现出不同的模型，从而提高我们的动手实践能力，同时这个过程也可以加深对理论知识的理解和把握。除了引入数据之外，我们还可以通过load_sample_images()来引入图片。</p>

<p>  首先，要使用sklearn中的数据集，必须导入datasets模块。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>from sklearn import datasets
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  下面两个图中包含了大部分sklearn中的数据集，调用方式也图中给出。</p>
<div align="center"> <img src="/img/202005/0518004.png" /> </div>

<div align="center"> <img src="/img/202005/0518005.png" /> </div>

<div align="center"> <img src="/img/202005/0518006.png" /> </div>

<p>  这里我们使用iris的数据来举个例子，表示导出数据集：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre>iris = datasets.load_iris() # 导入数据集
X = iris.data # 获得其特征向量
y = iris.target # 获得样本label
</pre></td></tr></tbody></table></code></pre></div></div>
<h3 id="2数据预处理">2，数据预处理</h3>
<p>  数据预处理阶段是机器学习中不可缺少的一环，它会使得数据更加有效的被模型或者评估器识别。下面我们来看一下sklearn中有哪些平时我们常用的函数：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>from sklearn import preprocessing
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  为了使得训练数据的标准化规则与测试数据的标准化规则同步，preprocessing中提供了很多的Scaler：</p>

<ul>
  <li>StandardScaler</li>
  <li>MaxAbsScaler</li>
  <li>MinMaxScaler</li>
  <li>RobustScaler</li>
  <li>Normalizer
等其他预处理操作
  对应的有直接的函数使用：scale()，maxabs_scale()，minmax_scale()，robust_scale()，normaizer（）
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>sklearn.preprocessing.scale(X)
</pre></td></tr></tbody></table></code></pre></div>    </div>
    <h4 id="21-数据标准化">2.1 数据标准化</h4>
    <p>  标准化：在机器学习中，我们可能要处理不同种类的资料，例如，音讯和图片上的像素值，这些资料可能是高纬度的，资料标准化后会使得每个特征中的数值平均变为0（将每个特征的值都减掉原始资料中该特征的平均），标准差变为1，这个方法被广泛的使用在许多机器学习算法中（例如：支持向量机，逻辑回归和类神经网络）。</p>
  </li>
</ul>

<p>  StandardScaler计算训练集的平均值和标准差，以便测试数据及使用相同的变换。</p>

<p>  变换后各维特征有0均值，单位方差，也叫z-score规范化（零均值规范化），计算方式是将特征值减去均值，除以标准差。</p>

<p>  <strong>fit</strong> :用于计算训练数据的均值和方差，后面就会用均值和方差来转换训练数据</p>

<p>  <strong>fit_transform</strong> :不仅计算训练数据的均值和方差，还会基于计算出来的均值和方差来转换训练数据，从而把数据转化成标准的正态分布。</p>

<p>  <strong>transform</strong> :很显然，它只是进行转换，只是把训练数据转换成标准的正态分布。（一般会把train和test集放在一起做标准化，或者在train集上做标准化后，用同样的标准化器去标准化test集，此时可以使用scaler)。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre>data = [[0, 0], [0, 0], [1, 1], [1, 1]]
# 1. 基于mean和std的标准化
scaler = preprocessing.StandardScaler().fit(train_data)
scaler.transform(train_data)
scaler.transform(test_data)
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  一般来说先使用fit：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>scaler = preocessing.StandardScaler().fit(X)
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  这一步可以计算得到scaler，scaler里面存的有计算出来的均值和方差。
  再使用transform</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>scaler.transform(X)
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  这一步再用scaler中的均值和方差来转换X，使X标准化。</p>

<p>  最后，在预测的时候，也要对数据做同样的标准化处理，即也要用上面的scaler中的均值和方差来对预测时候的特征进行标准化。</p>

<p>  注意：测试数据和预测数据的标准化的方式要和训练数据标准化的方式一样，必须使用同一个scaler来进行transform</p>

<h4 id="22-最小-最大规范化">2.2 最小-最大规范化</h4>
<p>  最小最大规范化对原始数据进行线性变换，变换到[0,1]区间（也可以是其他固定最小最大值的区间）。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre># 2. 将每个特征值归一化到一个固定范围
scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(train_data)
scaler.transform(train_data)
scaler.transform(test_data)
#feature_range: 定义归一化范围，注用（）括起来
</pre></td></tr></tbody></table></code></pre></div></div>
<h4 id="23-正则化normalize">2.3 正则化（normalize）</h4>
<p>  当你想要计算两个样本的相似度时必不可少的一个操作，就是正则化。其思想是：首先求出样本的p范数，然后该样本的所有元素都要除以该范数，这样最终使得每个样本的范数都是1。规范化（Normalization）是将不同变化范围的值映射到相同的固定范围，常见的是[0,1]，也成为归一化。</p>

<p>  如下例子，将每个样本变换成unit norm。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre>&gt;&gt;&gt; X = [[ 1., -1.,  2.],
...      [ 2.,  0.,  0.],
...      [ 0.,  1., -1.]]
&gt;&gt;&gt; X_normalized = preprocessing.normalize(X, norm='l2')
 
&gt;&gt;&gt; X_normalized                                     
array([[ 0.40..., -0.40...,  0.81...],
       [ 1.  ...,  0.  ...,  0.  ...],
       [ 0.  ...,  0.70..., -0.70...]])
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  我们可以发现对于每一个样本都有0.4^2+0.4^2+0.81^2=1。这就是L2 norm，变换后每个样本的各维特征的平方和为1.类似的，L1 norm则是变换后每个样本的各维特征的绝对值之和为1.还有max norm，则是将每个样本的各维特征除以该样本各维特征的最大值，</p>

<p>  在度量样本之间相似性时，如果使用的是二次型kernel，则需要做Normalization。</p>
<h3 id="3-数据集拆分">3. 数据集拆分</h3>
<p>  在得到训练数据集时，通常我们经常会把训练数据进一步拆分成训练集和验证集，这样有助于我们模型参数的选取。</p>

<p>  train_test_split是交叉验证中常用的函数，功能是从样本中随机的按比例选取train data和testdata，形式为：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre>X_train,X_test, y_train, y_test =
 
cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0)
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  注意：train_test_split 不再 cross_validation中，已经移到 model_selection 中。</p>

<p>  参数解释</p>
<ul>
  <li>train_data：所要划分的样本特征集</li>
  <li>train_target：所要划分的样本结果</li>
  <li>test_size：样本占比，如果是整数的话就是样本的数量</li>
  <li>random_state：是随机数的种子。</li>
  <li>随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。</li>
</ul>

<p>  随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则：<br />
   1. 种子不同，产生不同的随机数</p>

<p>   2. 种子相同，即使实例不同也产生相同的随机数</p>

<p>参数说明</p>
<div align="center"> <img src="/img/202005/0518007.png" /> </div>
<p>示例</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td><td class="rouge-code"><pre># 作用：将数据集划分为 训练集和测试集
# 格式：train_test_split(*arrays, **options)
from sklearn.mode_selection import train_test_split
 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
"""
参数
---
arrays：样本数组，包含特征向量和标签
 
test_size：
　　float-获得多大比重的测试样本 （默认：0.25）
　　int - 获得多少个测试样本
 
train_size: 同test_size
 
random_state:
　　int - 随机种子（种子固定，实验可复现）
　　
shuffle - 是否在分割之前对数据进行洗牌（默认True）
 
返回
---
分割后的列表，长度=2*len(arrays),
　　(train-test split)
"""
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  拆分参数遇到的问题及其解决方法
  导入模块</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>from sklearn.cross_validation import cross_val_score
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  则会报错，代码如下：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>    from sklearn.cross_validation import cross_val_score
ModuleNotFoundError: No module named 'sklearn.cross_validation'
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  解决方法：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>from sklearn.model_selection import cross_val_score
</pre></td></tr></tbody></table></code></pre></div></div>
<h3 id="4-定义模型">4. 定义模型</h3>
<p>  在这一步我们首先要分析自己数据的类型，明白自己要用什么模型来做，然后我们就可以在sklearn中定义模型了，sklearn为所有模型提供了非常相似的接口，这样使得我们可以更加快速的熟悉所有模型的用法，在这之前，我们先来看看模型的常用属性和功能。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre># 拟合模型
model.fit(X_train, y_train)
# 模型预测
model.predict(X_test)
 
# 获得这个模型的参数
model.get_params()
# 为模型进行打分
model.score(data_X, data_y) # 线性回归：R square； 分类问题： acc
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="41-线性回归">4.1 线性回归</h4>
<div align="center"> <img src="/img/202005/0518008.png" /> </div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="k">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">linear_model</span> <span class="n">import</span> <span class="n">LinearRegression</span>
<span class="p">#</span> <span class="err">定义线性回归模型</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="p">=</span><span class="nb">True</span><span class="p">,</span> <span class="n">normalize</span><span class="p">=</span><span class="nb">False</span><span class="p">,</span>
    <span class="n">copy_X</span><span class="p">=</span><span class="nb">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="p">=</span><span class="m">1</span><span class="p">)</span>
 
<span class="err">参数</span>
    <span class="n">fit_intercept</span><span class="err">：是否计算截距。</span><span class="nb">False</span><span class="p">-</span><span class="err">模型没有截距</span>
    <span class="n">normalize</span><span class="err">：</span> <span class="err">当</span><span class="n">fit_intercept</span><span class="err">设置为</span><span class="n">False</span><span class="err">时，该参数将被忽略。</span> <span class="err">如果为真，</span>
<span class="err">则回归前的回归系数</span><span class="n">X</span><span class="err">将通过减去平均值并除以</span><span class="n">l2</span><span class="p">-</span><span class="err">范数而归一化。</span>
     <span class="n">n_jobs</span><span class="err">：指定线程数</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="42-逻辑回归lr">4.2 逻辑回归LR</h4>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre><span class="k">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">linear_model</span> <span class="n">import</span> <span class="n">LogisticRegression</span>
<span class="p">#</span> <span class="err">定义逻辑回归模型</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="p">=</span><span class="err">’</span><span class="n">l2</span><span class="err">’</span><span class="p">,</span> <span class="n">dual</span><span class="p">=</span><span class="nb">False</span><span class="p">,</span> <span class="n">tol</span><span class="p">=</span><span class="m">0.0001</span><span class="p">,</span> <span class="n">C</span><span class="p">=</span><span class="m">1.0</span><span class="p">,</span>
    <span class="n">fit_intercept</span><span class="p">=</span><span class="nb">True</span><span class="p">,</span> <span class="n">intercept_scaling</span><span class="p">=</span><span class="m">1</span><span class="p">,</span> <span class="n">class_weight</span><span class="p">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">random_state</span><span class="p">=</span><span class="n">None</span><span class="p">,</span> <span class="n">solver</span><span class="p">=</span><span class="err">’</span><span class="n">liblinear</span><span class="err">’</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">=</span><span class="m">100</span><span class="p">,</span> <span class="n">multi_class</span><span class="p">=</span><span class="err">’</span><span class="n">ovr</span><span class="err">’</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">=</span><span class="m">0</span><span class="p">,</span> <span class="n">warm_start</span><span class="p">=</span><span class="nb">False</span><span class="p">,</span> <span class="n">n_jobs</span><span class="p">=</span><span class="m">1</span><span class="p">)</span>
 
<span class="err">参数</span>
    <span class="n">penalty</span><span class="err">：使用指定正则化项（默认：</span><span class="n">l2</span><span class="err">）</span>
    <span class="n">dual</span><span class="p">:</span> <span class="n">n_samples</span> <span class="p">&gt;</span> <span class="n">n_features</span><span class="err">取</span><span class="nb">False</span><span class="err">（默认）</span>
    <span class="n">C</span><span class="err">：正则化强度的反，值越小正则化强度越大</span>
    <span class="n">n_jobs</span><span class="p">:</span> <span class="err">指定线程数</span>
    <span class="n">random_state</span><span class="err">：随机数生成器</span>
    <span class="n">fit_intercept</span><span class="p">:</span> <span class="err">是否需要常量</span>  

</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="43-朴素贝叶斯算法nbnaive-bayes">4.3 朴素贝叶斯算法NB（Naive Bayes）</h4>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="k">from</span> <span class="n">sklearn</span> <span class="n">import</span> <span class="n">naive_bayes</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">naive_bayes</span><span class="p">.</span><span class="n">GaussianNB</span><span class="p">()</span> <span class="p">#</span> <span class="err">高斯贝叶斯</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">naive_bayes</span><span class="p">.</span><span class="n">MultinomialNB</span><span class="p">(</span><span class="n">alpha</span><span class="p">=</span><span class="m">1.0</span><span class="p">,</span> <span class="n">fit_prior</span><span class="p">=</span><span class="nb">True</span><span class="p">,</span> <span class="n">class_prior</span><span class="p">=</span><span class="n">None</span><span class="p">)</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">naive_bayes</span><span class="p">.</span><span class="n">BernoulliNB</span><span class="p">(</span><span class="n">alpha</span><span class="p">=</span><span class="m">1.0</span><span class="p">,</span> <span class="n">binarize</span><span class="p">=</span><span class="m">0.0</span><span class="p">,</span> <span class="n">fit_prior</span><span class="p">=</span><span class="nb">True</span><span class="p">,</span> <span class="n">class_prior</span><span class="p">=</span><span class="n">None</span><span class="p">)</span>

<span class="err">文本分类问题常用</span><span class="n">MultinomialNB</span>
<span class="err">参数</span>
    <span class="n">alpha</span><span class="err">：平滑参数</span>
    <span class="n">fit_prior</span><span class="err">：是否要学习类的先验概率；</span><span class="nb">false</span><span class="p">-</span><span class="err">使用统一的先验概率</span>
    <span class="n">class_prior</span><span class="p">:</span> <span class="err">是否指定类的先验概率；若指定则不能根据参数调整</span>
    <span class="n">binarize</span><span class="p">:</span> <span class="err">二值化的阈值，若为</span><span class="n">None</span><span class="err">，则假设输入由二进制向量组成</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="44-决策树dt">4.4 决策树DT</h4>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="k">from</span> <span class="n">sklearn</span> <span class="n">import</span> <span class="n">tree</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">tree</span><span class="p">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="p">=</span><span class="err">’</span><span class="n">gini</span><span class="err">’</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">min_samples_split</span><span class="p">=</span><span class="m">2</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="p">=</span><span class="m">1</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="p">=</span><span class="m">0.0</span><span class="p">,</span>
    <span class="n">max_features</span><span class="p">=</span><span class="n">None</span><span class="p">,</span> <span class="n">random_state</span><span class="p">=</span><span class="n">None</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="p">=</span><span class="n">None</span><span class="p">,</span>
    <span class="n">min_impurity_decrease</span><span class="p">=</span><span class="m">0.0</span><span class="p">,</span> <span class="n">min_impurity_split</span><span class="p">=</span><span class="n">None</span><span class="p">,</span>
     <span class="n">class_weight</span><span class="p">=</span><span class="n">None</span><span class="p">,</span> <span class="n">presort</span><span class="p">=</span><span class="nb">False</span><span class="p">)</span>
<span class="err">参数</span>
    <span class="n">criterion</span> <span class="err">：特征选择准则</span><span class="n">gini</span><span class="p">/</span><span class="n">entropy</span>
    <span class="n">max_depth</span><span class="err">：树的最大深度，</span><span class="n">None</span><span class="p">-</span><span class="err">尽量下分</span>
    <span class="n">min_samples_split</span><span class="err">：分裂内部节点，所需要的最小样本树</span>
    <span class="n">min_samples_leaf</span><span class="err">：叶子节点所需要的最小样本数</span>
    <span class="n">max_features</span><span class="p">:</span> <span class="err">寻找最优分割点时的最大特征数</span>
    <span class="n">max_leaf_nodes</span><span class="err">：优先增长到最大叶子节点数</span>
    <span class="n">min_impurity_decrease</span><span class="err">：如果这种分离导致杂质的减少大于或等于这个值，则节点将被拆分。</span>   
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="45-支持向量机svm">4.5 支持向量机SVM</h4>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="k">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">svm</span> <span class="n">import</span> <span class="n">SVC</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="p">=</span><span class="m">1.0</span><span class="p">,</span> <span class="n">kernel</span><span class="p">=</span><span class="err">’</span><span class="n">rbf</span><span class="err">’</span><span class="p">,</span> <span class="n">gamma</span><span class="p">=</span><span class="err">’</span><span class="n">auto</span><span class="err">’</span><span class="p">)</span>
<span class="err">参数</span>
    <span class="n">C</span><span class="err">：误差项的惩罚参数</span><span class="n">C</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="err">核相关系数。浮点数，</span><span class="k">If</span> <span class="n">gamma</span> <span class="n">is</span> <span class="err">‘</span><span class="n">auto</span><span class="err">’</span> <span class="k">then</span> <span class="m">1</span><span class="p">/</span><span class="n">n_features</span> <span class="n">will</span> <span class="n">be</span> <span class="n">used</span> <span class="n">instead</span><span class="p">.</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="46-k近邻算法knn">4.6 k近邻算法KNN</h4>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="k">from</span> <span class="n">sklearn</span> <span class="n">import</span> <span class="n">neighbors</span>
<span class="p">#</span><span class="err">定义</span><span class="n">kNN</span><span class="err">分类模型</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">neighbors</span><span class="p">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">=</span><span class="m">5</span><span class="p">,</span> <span class="n">n_jobs</span><span class="p">=</span><span class="m">1</span><span class="p">)</span> <span class="p">#</span> <span class="err">分类</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">neighbors</span><span class="p">.</span><span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">=</span><span class="m">5</span><span class="p">,</span> <span class="n">n_jobs</span><span class="p">=</span><span class="m">1</span><span class="p">)</span> <span class="p">#</span> <span class="err">回归</span>
<span class="err">参数</span>
    <span class="n">n_neighbors</span><span class="err">：</span> <span class="err">使用邻居的数目</span>
    <span class="n">n_jobs</span><span class="err">：并行任务数</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="47-多层感知器神经网络">4.7 多层感知器（神经网络）</h4>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="k">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">neural_network</span> <span class="n">import</span> <span class="n">MLPClassifier</span>
<span class="p">#</span> <span class="err">定义多层感知机分类算法</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">activation</span><span class="p">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">solver</span><span class="p">=</span><span class="s1">'adam'</span><span class="p">,</span> <span class="n">alpha</span><span class="p">=</span><span class="m">0.0001</span><span class="p">)</span>
<span class="s2">"""参数
---
    hidden_layer_sizes: 元祖
    activation：激活函数
    solver ：优化算法{‘lbfgs’, ‘sgd’, ‘adam’}
    alpha：L2惩罚(正则化项)参数。
"""</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<h3 id="5-模型评估与选择">5. 模型评估与选择</h3>
<p>  评价指标针对不同的机器学习任务有不同的指标，同一任务也有不同侧重点的评价指标。以下方法，sklearn中都在sklearn.metrics类下，务必记住那些指标适合分类，那些适合回归。</p>

<p>  机器学习常用的评估指标请参考博文：Python机器学习笔记：常用评估指标的前世今生</p>

<h4 id="51-交叉验证">5.1 交叉验证</h4>
<p>交叉验证cross_val_score的scoring参数<br />
  分类：accuracy(准确率)、f1、f1_micro、f1_macro（这两个用于多分类的f1_score）、precision(精确度)、recall(召回率)、roc_auc<br />
  回归：neg_mean_squared_error（MSE、均方误差）、r2<br />
  聚类：adjusted_rand_score、completeness_score等</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>from sklearn.model_selection import cross_val_score
cross_val_score(model, X, y=None, scoring=None, cv=None, n_jobs=1)
"""参数
---
    model：拟合数据的模型
    cv ： k-fold
    scoring: 打分参数-‘accuracy’、‘f1’、‘precision’、‘recall’ 、‘roc_auc’、'neg_log_loss'等等
"""
</pre></td></tr></tbody></table></code></pre></div></div>

<p>交叉验证的学习<br />
  1，导入k折交叉验证模块<br />
  注意cross_val_score 是根据模型进行计算，计算交叉验证的结果，可以简单的认为cross_val_score中调用了KFold 进行数据集划分。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>from sklearn.model_selection import cross_val_score
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  2，交叉验证的思想<br />
  把某种意义下将原始数据（dataset）进行分组，一部分作为训练集（train set），另一部分作为验证集（validation set or test set），首先用训练集对分类器进行训练，再利用验证集来测试训练得到的模型（model），以此来作为评价分类器的性能指标。</p>

<p>  3，为什么使用交叉验证法<br />
  交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程序熵减少过拟合。
  交叉验证还可以从有限的数据中获取尽可能多的有效信息
  4，model_selection.KFold 和 model_selection.cross_val_score的区别
  我们直接看官网：KFold：https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold</p>

<p>  cross_val_score：https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score</p>

<div align="center"> <img src="/img/202005/0518009.png" /> </div>
<p>  KFold 就是对数据集划分为 训练集/测试集，然后将训练数据集划分为K折，每个折进行一次验证，而剩下的K-1 折进行训练，依次循环，直到用完所有的折。</p>
<div align="center"> <img src="/img/202005/0518010.png" /> </div>
<p>  而 cross_val_score 就是通过交叉验证评估得分。</p>

<p>  下面看看K折交叉验证函数KFold函数：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre>KFold（n_split, shuffle, random_state）
 
　　参数：n_split:要划分的折数
 
　　　　　shuffle: 每次都进行shuffle，测试集中折数的总和就是训练集的个数
 
　　　　　random_state:随机状态
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  其使用如下：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="rouge-code"><pre>from sklearn.model_selection import KFold
from sklearn.datasets import load_iris
from sklearn.linear_model import LinearRegression
import numpy as np
import pandas as pd
 
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4])
kf = KFold(n_splits=2)
# get_n_splits: Returns the number of splitting iterations in the cross-validator
print(kf.get_n_splits(X))  # 2
 
KF = KFold(n_splits=5)
X, Y = load_iris().data, load_iris().target
alg = LinearRegression()
# 这里想强行使用DataFrame的数据格式，因为以后大家读取数据使用都是csv格式
# 所以必不可免要用 iloc
X, Y = pd.DataFrame(X), pd.DataFrame(Y)
# split()：Generate indices to split data into training and test set.
for train_index, test_index in KF.split(X):
    print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]
    alg.fit(X_train, y_train)
    # 后面就自由发挥即可
</pre></td></tr></tbody></table></code></pre></div></div>
<h3 id="5主要有哪些方法">5，主要有哪些方法</h3>
<h4 id="1留出法holdout-cross-validation">1，留出法（holdout cross validation）</h4>

<p>  在机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：训练集，验证集和测试集。</p>

<p>  训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。</p>

<div align="center"> <img src="/img/202005/0518011.png" /> </div>

<p>  这个方法操作简单，只需要随机将原始数据分为三组即可。</p>

<p>  不过如果只做一次分割，它对训练集，验证集和测试机的样本比例，还有分割后数据的分布是否和原始数据集的分布相同等因素比较敏感，不同的划分会得到不同的最优模型，，而且分成三个集合后，用于训练的数据更少了。于是又了2.k折交叉验证（k-fold cross validation）.</p>

<p>  下面例子，一共有150条数据：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.model_selection import train_test_split
&gt;&gt;&gt; from sklearn import datasets
&gt;&gt;&gt; from sklearn import svm
 
&gt;&gt;&gt; iris = datasets.load_iris()
&gt;&gt;&gt; iris.data.shape, iris.target.shape
((150, 4), (150,))
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  用train_test_split来随机划分数据集，其中40%用于测试集，有60条数据，60%为训练集，有90条数据：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre>&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(
...     iris.data, iris.target, test_size=0.4, random_state=0)
 
&gt;&gt;&gt; X_train.shape, y_train.shape
((90, 4), (90,))
&gt;&gt;&gt; X_test.shape, y_test.shape
((60, 4), (60,))
　　用train来训练，用test来评价模型的分数。

&gt;&gt;&gt; clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
&gt;&gt;&gt; clf.score(X_test, y_test)                          
0.96...
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="2-k-折交叉验证k-fold-cross-validation">2. k 折交叉验证（k-fold cross validation）</h4>

<p>  K折交叉验证通过对k个不同分组训练的结果进行平均来减少方差，因此模型的性能对数据的划分就不那么敏感。</p>

<ul>
  <li>第一步，不重复抽样将原始数据随机分为 k 份。</li>
  <li>第二步，每一次挑选其中 1 份作为测试集，剩余 k-1 份作为训练集用于模型训练。</li>
  <li>第三步，重复第二步 k 次，这样每个子集都有一次机会作为测试集，其余机会作为训练集。</li>
  <li>在每个训练集上训练后得到一个模型，</li>
  <li>用这个模型在相应的测试集上测试，计算并保存模型的评估指标，
第四步，计算 k 组测试结果的平均值作为模型精度的估计，并作为当前 k 折交叉验证下模型的性能指标。
  K一般取10，数据量小的是，k可以设大一点，这样训练集占整体比例就比较大，不过同时训练的模型个数也增多。数据量大的时候，k可以设置小一点。当k=m的时候，即样本总数，出现了留一法。</li>
</ul>

<p>  举例，这里直接调用了cross_val_score，这里用了5折交叉验证</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre>&gt;&gt;&gt; from sklearn.model_selection import cross_val_score
&gt;&gt;&gt; clf = svm.SVC(kernel='linear', C=1)
&gt;&gt;&gt; scores = cross_val_score(clf, iris.data, iris.target, cv=5)
&gt;&gt;&gt; scores                                             
array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])
　　得到最后平均分数为0.98，以及它的95%置信区间：

&gt;&gt;&gt; print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
Accuracy: 0.98 (+/- 0.03)
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  我们可以直接看一下K-Fold是怎么样划分数据的：X有四个数据，把它分成2折，结构中最后一个集合是测试集，前面的是训练集，每一行为1折：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.model_selection import KFold
 
&gt;&gt;&gt; X = ["a", "b", "c", "d"]
&gt;&gt;&gt; kf = KFold(n_splits=2)
&gt;&gt;&gt; for train, test in kf.split(X):
...     print("%s %s" % (train, test))
[2 3] [0 1]
[0 1] [2 3]
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  同样的数据X，我们来看LeaveOneOut后是什么样子，那就是把它分成4折，结果中最后一个集合是测试集，只有一个元素，前面的是训练集，每一行为1折：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre>&gt;&gt;&gt; from sklearn.model_selection import LeaveOneOut
 
&gt;&gt;&gt; X = [1, 2, 3, 4]
&gt;&gt;&gt; loo = LeaveOneOut()
&gt;&gt;&gt; for train, test in loo.split(X):
...     print("%s %s" % (train, test))
[1 2 3] [0]
[0 2 3] [1]
[0 1 3] [2]
[0 1 2] [3]
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="3留一法leave-one-out-cross-validation">3，留一法（Leave one out cross validation）</h4>

<p>  每次的测试集都只有一个样本，要进行m次训练和预测，这个方法用于训练的数据只比整体数据集少一个样本，因此最接近原始样本的分布。但是训练复杂度增加了，因为模型的数量与原始数据样本数量相同。一般在数据缺少时使用。<br />
  此外： <br />
  多次 k 折交叉验证再求均值，例如：10 次 10 折交叉验证，以求更精确一点。<br />
  划分时有多种方法，例如对非平衡数据可以用分层采样，就是在每一份子集中都保持和原始数据集相同的类别比例。<br />
  模型训练过程的所有步骤，包括模型选择，特征选择等都是在单个折叠 fold 中独立执行的。</p>
<h4 id="4bootstrapping">4，Bootstrapping</h4>

<p>  通过自助采样法，即在含有 m 个样本的数据集中，每次随机挑选一个样本，再放回到数据集中，再随机挑选一个样本，这样有放回地进行抽样 m 次，组成了新的数据集作为训练集。<br />
  这里会有重复多次的样本，也会有一次都没有出现的样本，原数据集中大概有 36.8% 的样本不会出现在新组数据集中。<br />
  优点是训练集的样本总数和原数据集一样都是 m，并且仍有约 1/3 的数据不被训练而可以作为测试集。 <br />
  缺点是这样产生的训练集的数据分布和原数据集的不一样了，会引入估计偏差。</p>

<h4 id="52-检验曲线">5.2 检验曲线</h4>
<p>  使用检验曲线，我们可以更加方便的改变模型参数，获取模型表现。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre>from sklearn.model_selection import validation_curve
train_score, test_score = validation_curve(model, X, y, param_name, param_range, cv=None, scoring=None, n_jobs=1)
"""参数
---
    model:用于fit和predict的对象
    X, y: 训练集的特征和标签
    param_name：将被改变的参数的名字
    param_range： 参数的改变范围
    cv：k-fold
    
返回值
---
   train_score: 训练集得分（array）
    test_score: 验证集得分（array）
"""
</pre></td></tr></tbody></table></code></pre></div></div>
<p>　　</p>
<h4 id="53-分类模型">5.3 分类模型</h4>
<p>  accuracy_score（准确率得分）是模型分类正确的数据除以样本总数 【模型的score方法算的也是准确率】</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>accuracy_score(y_test,y_pre)
# 或者 model.score(x_test,y_test)，大多模型都是有score方法的
</pre></td></tr></tbody></table></code></pre></div></div>

<p>  classification_report中的各项得分的avg/total 是每一分类占总数的比例加权算出来的</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>print(classification_report(y_test,y_log_pre))
 
             precision    recall  f1-score   support
 
          0       0.87      0.94      0.90       105
          1       0.91      0.79      0.85        73
 
avg / total       0.88      0.88      0.88       178
</pre></td></tr></tbody></table></code></pre></div></div>

<p>  confusion_matrix（混淆矩阵），用来评估分类的准确性</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>&gt;&gt;&gt; from sklearn.metrics import confusion_matrix
&gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]
&gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]
&gt;&gt;&gt; confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])

</pre></td></tr></tbody></table></code></pre></div></div>
<p>  precision_score(精确度)、recall_score(召回率)、f1_score（后者由前两个推导出的）
  这三个不仅适合二分类，也适合多分类。只需要指出参数average=‘micro’/‘macro’/’weighted’</p>

<p>  macro：计算二分类metrics的均值，为每个类给出相同权重的分值。
当小类很重要时会出问题，因为该macro-averging方法是对性能的平均。
另一方面，该方法假设所有分类都是一样重要的，因此macro-averaging
方法会对小类的性能影响很大</p>

<p>  micro： 给出了每个样本类以及它对整个metrics的贡献的pair（sample-
weight），而非对整个类的metrics求和，它会每个类的metrics上的权重及
因子进行求和，来计算整个份额。Micro-averaging方法在多标签（multilabel）
问题中设置，包含多分类，此时，大类将被忽略</p>

<p>  weighted: 对于不均衡数量的类来说，计算二分类metrics的平均，
通过在每个类的score上进行加权实现
roc_curve（ROC曲线，用于二分类）</p>

<h3 id="6-保存模型">6 保存模型</h3>
<p>  最后，我们可以将我们训练好的model保存到本地，或者放到线上供用户使用，那么如何保存训练好的model呢？主要有下面两种方式：</p>

<h4 id="61-保存为pickle文件">6.1 保存为pickle文件</h4>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="n">import</span> <span class="n">pickle</span>
 
<span class="p">#</span> <span class="err">保存模型</span>
<span class="k">with</span> <span class="n">open</span><span class="p">(</span><span class="s1">'model.pickle'</span><span class="p">,</span> <span class="s1">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="k">model</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
 
<span class="p">#</span> <span class="err">读取模型</span>
<span class="k">with</span> <span class="n">open</span><span class="p">(</span><span class="s1">'model.pickle'</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">model</span> <span class="p">=</span> <span class="n">pickle</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="k">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<h4 id="62-sklearn自带方法joblib">6.2 sklearn自带方法joblib</h4>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="k">from</span> <span class="n">sklearn</span><span class="p">.</span><span class="n">externals</span> <span class="n">import</span> <span class="n">joblib</span>
 
<span class="p">#</span> <span class="err">保存模型</span>
<span class="n">joblib</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="k">model</span><span class="p">,</span> <span class="s1">'model.pickle'</span><span class="p">)</span>
 
<span class="p">#</span><span class="err">载入模型</span>
<span class="k">model</span> <span class="p">=</span> <span class="n">joblib</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="s1">'model.pickle'</span><span class="p">)</span>   
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="7模型评分">7，模型评分</h3>
<p>  1，模型的score方法：最简单的模型评估方法就是调用模型自己的方法：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre># 预测
y_predict = knnClf.predict(x_test)
print("score on the testdata:",knnClf.score(x_test,y_test))
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  2，sklearn的指标函数：库提供的一些计算方法，常用的有classification_report方法</p>

<p>  3，sklearn也支持自己开发评价方法。</p>

<h3 id="8几种交叉验证cross-validation方式的比较">8，几种交叉验证（cross validation）方式的比较</h3>
<p>  模型评价的目的：通过模型评价，我们知道当前训练模型的好坏，泛化能力如何？从而知道是否可以应用在解决问题上，如果不行，那又是那些出了问题？<br />
<strong>train_test_split</strong><br />
  在分类问题中，我们通常通过对训练集进行triain_test_split，划分出train 和test两部分，其中train用来训练模型，test用来评估模型，模型通过fit方法从train数据集中学习，然后调用score方法在test集上进行评估，打分；从分数上我们知道模型当前的训练水平如何。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre>from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import  matplotlib.pyplot as plt
 
cancer = load_breast_cancer()
X_train,X_test,y_train,y_test = train_test_split(cancer.data,cancer.target,random_state=0)
 
logreg = LogisticRegression().fit(X_train,y_train)
print("Test set score:{:.2f}".format(logreg.score(X_test,y_test)))   
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  结果：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>Test set score:0.96   
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  然而这这方式只进行了一次划分，数据结果具有偶然性，如果在某次划分中，训练集里全是容易学习的数据，测试集里全是复杂的数据，这样的就会导致最终的结果不尽人意。<br />
<strong>Standard Cross Validation</strong><br />
  针对上面通过train_test_split划分，从而进行模型评估方式存在的弊端，提出Cross Validation交叉验证。<br />
  Cross Validation：进行多次train_test_split划分；每次划分时，在不同的数据集上进行训练，测试评估，从而得到一个评价结果；如果是5折交叉验证，意思就是在原始数据集上，进行五次划分，每次划分进行一次训练，评估，最后得到5次划分后的评估结果，一般在这几次评估结果上取平均得到最后的评分，k-folf cross-validation ，其中K一般取5或10。</p>

<div align="center"> <img src="/img/202005/0518012.png" /> </div>

<p>  代码：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre>from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import  warnings
 
warnings.filterwarnings('ignore')
 
cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
    cancer.data , cancer.target, random_state=0
)
 
logreg = LogisticRegression()
# CV 默认是3折交叉验证，可以修改cv=5，变为5折交叉验证
scores = cross_val_score(logreg,cancer.data , cancer.target)
 
print("Cross validation scores:{}".format(scores))
print("Mean cross validation score:{:2f}".format(scores.mean()))   
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  结果：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>Cross validation scores:[0.93684211 0.96842105 0.94179894]
Mean cross validation score:0.949021   
</pre></td></tr></tbody></table></code></pre></div></div>
<ul>
  <li>交叉验证的优点：<br />
  原始采用的train_test_split方法，数据划分具有偶然性；交叉验证通过多次划分，大大降低了这种由一次随机划分带来的偶然性，同时通过多次划分，多次训练，模型也能遇到各种各样的数据，从而提高其泛化能力
与原始的train_test_split相比，对数据的使用效率更高，train_test_split，默认训练集，测试集比例为3:1，而对交叉验证来说，如果是5折交叉验证，训练集比测试集为4:1；10折交叉验证训练集比测试集为9:1.数据量越大，模型准确率越高！</li>
  <li>交叉验证的缺点：<br />
  这种简答的交叉验证方式，从上面的图片可以看出来，每次划分时对数据进行均分，设想一下，会不会存在一种情况：数据集有5类，抽取出来的也正好是按照类别划分的5类，也就是说第一折全是0类，第二折全是1类，等等；这样的结果就会导致，模型训练时。没有学习到测试集中数据的特点，从而导致模型得分很低，甚至为0，为避免这种情况，又出现了其他的各种交叉验证方式。</li>
</ul>

<p><strong>Stratifid k-fold cross validation</strong><br />
  分层交叉验证（Stratified k-fold cross validation）：首先它属于交叉验证类型，分层的意思是说在每一折中都保持着原始数据中各个类别的比例关系，比如说：原始数据有3类，比例为1:2:1，采用3折分层交叉验证，那么划分的3折中，每一折中的数据类别保持着1:2:1的比例，这样的验证结果更加可信。
  通常情况下，可以设置cv参数来控制几折，但是我们希望对其划分等加以控制，所以出现了KFold，KFold控制划分折，可以控制划分折的数目，是否打乱顺序等，可以赋值给cv，用来控制划分。</p>

<div align="center"> <img src="/img/202005/0518013.png" /> </div>

<p>  代码：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre>from sklearn.datasets import load_iris
from sklearn.model_selection import StratifiedKFold ,cross_val_score
from sklearn.linear_model import LogisticRegression
import warnings
 
warnings.filterwarnings('ignore')
 
iris_data = load_iris()
logreg = LogisticRegression()
strKFold = StratifiedKFold(n_splits=3,shuffle=False,random_state=0)
scores = cross_val_score(logreg,iris_data.data,iris_data.target,cv=strKFold)
print("straitified cross validation scores:{}".format(scores))
print("Mean score of straitified cross validation:{:.2f}".format(scores.mean()))   
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  结果：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>straitified cross validation scores:[0.96078431 0.92156863 0.95833333]
Mean score of straitified cross validation:0.95  
</pre></td></tr></tbody></table></code></pre></div></div>
<p><strong>Leave-one-out Cross-validation 留一法</strong><br />
  留一法Leave-one-out Cross-validation：是一种特殊的交叉验证方式。顾名思义，如果样本容量为n，则k=n，进行n折交叉验证，每次留下一个样本进行验证。主要针对小样本数据。<br />
  代码：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre>from sklearn.datasets import load_iris
from sklearn.model_selection import LeaveOneOut , cross_val_score
from sklearn.linear_model import LogisticRegression
import  warnings
 
warnings.filterwarnings('ignore')
 
iris = load_iris()
logreg = LogisticRegression()
loout = LeaveOneOut()
scores = cross_val_score(logreg,iris.data,iris.target,cv=loout)
print("leave-one-out cross validation scores:{}".format(scores))
print("Mean score of leave-one-out cross validation:{:.2f}".format(scores.mean()))   
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  结果：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre>leave-one-out cross validation scores:[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1.]
Mean score of leave-one-out cross validation:0.95   
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>Shuffle-split cross-validation</strong><br />
  控制更加灵活，可以控制划分迭代次数，每次划分测试集和训练集的比例（也就说：可以存在机不再训练集也不再测试集的情况）</p>

<div align="center"> <img src="/img/202005/0518014.png" /> </div>
<p>  代码：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre>from sklearn.datasets import load_iris
from sklearn.model_selection import ShuffleSplit,cross_val_score
from sklearn.linear_model import LogisticRegression
import warnings
 
warnings.filterwarnings('ignore')
 
iris = load_iris()
# 迭代八次
shufsp1 = ShuffleSplit(train_size=0.5,test_size=0.4,n_splits=8)
logreg = LogisticRegression()
scores = cross_val_score(logreg,iris.data,iris.target,cv=shufsp1)
 
print("shuffle split cross validation scores:\n{}".format(scores))
print("Mean score of shuffle split cross validation:{:.2f}".format(scores.mean()))   
</pre></td></tr></tbody></table></code></pre></div></div>
<p>  结果：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre>shuffle split cross validation scores:
[0.95       1.         0.86666667 0.95       0.88333333 0.88333333
 0.85       0.9       ]
Mean score of shuffle split cross validation:0.91   
</pre></td></tr></tbody></table></code></pre></div></div>
<h3 id="9sklearn中一些函数的用法">9，sklearn中一些函数的用法</h3>
<h4 id="91--from-sklearnutils-import-shuffle-解析">9.1  from sklearn.utils import shuffle 解析</h4>
<p>  在进行机器学习时，经常需要打乱样本，这种时候Python中第三方库提供了这个功能——sklearn.utils.shuffle。</p>

<p>1，Parameters</p>
<div align="center"> <img src="/img/202005/0518015.png" /> </div>
<p>2，Returns</p>
<div align="center"> <img src="/img/202005/0518016.png" /> </div>

<p>参考文献：
http://www.cnblogs.com/lianyingteng/p/7811126.html</p>

<p>https://www.cnblogs.com/magle/p/5638409.html</p>

<p>https://blog.csdn.net/u014248127/article/details/78885180</p>

<p>https://www.cnblogs.com/ysugyl/p/8707887.html</p>

<p>https://www.cnblogs.com/wj-1314/p/10179741.html</p>



                <hr style="visibility: hidden;">
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2020/05/14/%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E7%9A%84%E8%AF%84%E4%BC%B0/" data-toggle="tooltip" data-placement="top" title="ML5--模型预测结果的评估">
                        Previous<br>
                        <span>ML5--模型预测结果的评估</span>
                        </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2020/06/03/%E4%B8%80%E6%9B%B2%E9%95%BF%E6%81%A8%E4%B8%80%E6%AE%B5%E6%83%85/" data-toggle="tooltip" data-placement="top" title="一曲长恨一段情">
                        Next<br>
                        <span>一曲长恨一段情</span>
                        </a>
                    </li>
                    
                </ul>
                <hr style="visibility: hidden;">

                
                <!-- disqus 评论框 start -->
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                <!-- disqus 评论框 end -->
                

                
            </div>

    <!-- Side Catalog Container -->
        
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">目录</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                



<section>
    
        <hr class="hidden-sm hidden-xs">
    
    <h5><a href="/archive/">推荐标签</a></h5>
    <div class="tags">
        
        
        
        </a>
        
        
                <a data-sort="0015"
                    href="/archive/?tag=%E5%AD%A6%E4%B9%A0"
                    title="学习"
                    rel="26">学习</a>
        
                <a data-sort="0035"
                    href="/archive/?tag=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"
                    title="机器学习"
                    rel="6">机器学习</a>
        
                <a data-sort="0035"
                    href="/archive/?tag=python"
                    title="python"
                    rel="6">python</a>
        
                <a data-sort="0036"
                    href="/archive/?tag=%E7%BD%91%E7%BB%9C"
                    title="网络"
                    rel="5">网络</a>
        
                <a data-sort="0036"
                    href="/archive/?tag=centos7"
                    title="centos7"
                    rel="5">centos7</a>
        
                <a data-sort="0036"
                    href="/archive/?tag=docker"
                    title="docker"
                    rel="5">docker</a>
        
                <a data-sort="0037"
                    href="/archive/?tag=linux"
                    title="linux"
                    rel="4">linux</a>
        
                <a data-sort="0039"
                    href="/archive/?tag=%E8%B8%8F%E6%98%A5"
                    title="踏春"
                    rel="2">踏春
    </div>
</section>



                <!-- Friends Blog -->
                
<hr>
<h5>朋友</h5>
<ul class="list-inline">
  
  <li><a href="http://www.cnblogs.com/yanwanglol/">博客园-yan</a></li>
  
  <li><a href="https://www.zhihu.com/people/wang520yan/activities">知乎-yan</a></li>
  
</ul>


            </div>
        </div>
    </div>
</article>

<!-- add support for mathjax by voleking-->






<!-- disqus 公共JS代码 start (一个网页只需插入一次) -->
<!-- <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "yan";
    var disqus_identifier = "/2020/05/18/sklearn库的学习";
    var disqus_url = "http://localhost:4000/2020/05/18/sklearn%E5%BA%93%E7%9A%84%E5%AD%A6%E4%B9%A0/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script> -->
<!-- disqus 公共JS代码 end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    // async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
    //     anchors.options = {
    //       visible: 'always',
    //       placement: 'right',
    //       icon: '#'
    //     };
    //     anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    // })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <!-- SNS Link -->
                


<ul class="list-inline text-center">


  
  
  <li>
    <a href="https://twitter.com/wang520yan">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
  <li>
    <a target="_blank" href="https://www.zhihu.com/people/wang521yan">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa  fa-stack-1x fa-inverse">知</i>
      </span>
    </a>
  </li>
  
  
  <li>
    <a target="_blank" href="http://weibo.com/wang520yan">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
  <li>
    <a target="_blank" href="https://www.facebook.com/wang520yan">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
  <li>
    <a target="_blank" href="https://github.com/wang520yan">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
  <li>
    <a target="_blank" href="https://www.linkedin.com/in/wang520yan">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
</ul>

                <p class="copyright text-muted">
                    Copyright &copy; Yan Blog 2020
                    <br>
                    Powered by <a href="https://wang520yan.github.io/">Yan Blog</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=wang520yan&repo=wang520yan.github.io&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<!-- Currently, only navbar scroll-down effect at desktop still depends on this -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>

<!-- Service Worker -->

<script src="/js/snackbar.js "></script>
<script src="/js/sw-registration.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
     - https://github.com/jneen/rouge/wiki/list-of-supported-languages-and-lexers
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->





<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->



<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = '5cd3746f37e5658b23190f6f61f05fc0';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>



<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog (selector) {

        // interop with multilangual
        if ('' == 'true') {
            _containerSelector = 'div.post-container.active'
        } else {
            _containerSelector = 'div.post-container'
        }

        // init
        var P = $(_containerSelector),a,n,t,l,i,c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        // clean
        $(selector).html('')

        // appending
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#"+$(this).prop('id');
            t = $(this).text();
            c = $('<a href="'+i+'" rel="nofollow">'+t+'</a>');
            l = $('<li class="'+n+'_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function(e){
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>



<!-- Multi-Lingual -->




<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
